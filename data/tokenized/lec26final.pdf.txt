CPSC 420: Lecture 26
The Experts Problem and Multiplicative Weights
Joseph Poremba
March 22, 2022

Topics/Learning Objectives
Understand the Experts problem
The model, objective, etc.
Understand the main algorithm
Be able to run it, understand the analysis
(Hopefully) See some of the bigger picture
The Multiplicative Weights algorithm
Applications (Machine learning, linear programming)
(but not understand in detail)
No supplementary reading, but sources for the brave:
S. Arora, E. Hazan, and S. Kale, The Multiplicative Weights Update Method: A Meta-Algorithm and
Applications
N. Garg and J. Knemann, Faster and Simpler Algorithms for Multicommodity Flow and Other Fractional
Packing Problems
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
2 / 25

Part I
The Experts Problem
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
3 / 25

Predicting Stocks
Suppose there is some stock on the market.
Every day, the stock goes ↑ or ↓, we want to predict which
The true stock performance is revealed each day after our
prediction
Minimize mistakes
Day
1
2
3
4
5
ALG
Stock
mALG
Stock behaviour?
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
4 / 25

Adversarial
The stock performance is adversarial.
Not random.
Algorithm must do well in all cases.
Could be designed to be evil and thwart our algorithm
speciﬁcally.
Problem?
Not really predictable. We could be wrong every time! Doesn’t
seem fair...
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
5 / 25

Adding Experts
We get access to n “experts”, who make their prediction ﬁrst.
Day
1
2
3
4
5
e1
e2
e3
ALG
Stock
mALG
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
6 / 25

Adding Experts
The expert predictions are also adversarial!
They might not know what they are doing.
Could also hate us, conspire against us, etc.
We only need to do well relative to the experts
To make us do poorly, the input needs to make the experts do
poorly
Goal
Don’t make many more mistakes than the best expert.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
7 / 25

Example: Naive Approach
Strategy 1: Follow the majority prediction
Day
1
2
3
4
5
e1
e2
e3
ALG
Stock
m∗
mALG
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
8 / 25

Example: Thwarting the Naive Approach
Strategy 1: Follow the majority prediction
Day
1
2
3
4
5
e1
e2
e3
ALG
Stock
m∗
mALG
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
9 / 25

The Weighted Majority Algorithm
Have a trust weight wi(t) for expert i at the start of day t
Choose the decision (↑ or ↓) with the majority of total trust.
i.e. compare 󰁓
i voting ↑ wi(t) and 󰁓
i voting ↓ wi(t)
If an expert is wrong, penalize their weight, so we pay less
attention to them in the future.
Update Rule
Initially, wi(1) = 1 for all i. For every day after,
wi(t + 1) =
󰀫
wi(t),
if i is correct on day t
1
2wi(t),
if i is wrong on day t
Multiplicative penalty is key.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
10 / 25

The Weighted Majority Algorithm
Day
1
2
3
4
5
e1
w1
1
e2
w2
1
e3
w3
1
W ↑
W ↓
ALG
Stock
m∗
mALG
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
11 / 25

Analysis of the Weighted Majority Algorithm
Let:
wi(t) = weight of expert i at the start of day t
mi(t) = total mistakes of expert i at the end of day t
Goal: Discern some relationships. We want to relate the number of
mistakes of the algorithm to mi.
First: Relate mi to wi. Observe that wi(t) is halved for every
mistake i makes.
wi(t + 1) = 1
2 × 1
2 × · · · × 1
2
󰁿
󰁾󰁽
󰂀
mi(t) times
wi(1)
=
󰀕1
2
󰀖mi(t)
Now can we relate wi to the mistakes of the algorithm?
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
12 / 25

Analysis of the Weighted Majority Algorithm
Let W (t) = 󰁓
i wi(t) (the “potential”, or “total trust”)
How does the potential evolve over time?
W (t + 1) =
󰀣 󰁛
i correct
wi(t)
󰀤
󰁿
󰁾󰁽
󰂀
W +(t)
+
1
2
󰀳
󰁃 󰁛
i wrong
wi(t)
󰀴
󰁄
󰁿
󰁾󰁽
󰂀
W −(t)
= W +(t) + 1
2W −(t)
Also: W (t) = W +(t) + W −(t). Therefore...
W (t + 1) = W (t) − W −(t) + 1
2W −(t)
= W (t) − 1
2W −(t)
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
13 / 25

Last slide: W (t + 1) = W (t) − 1
2W −(t).
If the algorithm guesses wrong, W −(t) ≥ 1
2W (t). Then,
W (t + 1) ≤ W (t) − 1
2
󰀕1
2W (t)
󰀖
= 3
4W (t)
So every time the algorithm is wrong, the potential goes down by a
factor of at least 1/4.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
14 / 25

Analysis of the Weighted Majority Algorithm
Say we have ﬁnished day T, start of day T + 1.
Say our algorithm has made mALG(T) mistakes.
The total trust is at most
W (T + 1) ≤ 3
4 × 3
4 × · · · · ×3
4
󰁿
󰁾󰁽
󰂀
k times
W (1) =
󰀕3
4
󰀖mALG(T)
· n.
From before: wi(T + 1) ≥
󰀃 1
2
󰀄mi(T).
Now, an easy bound: wi(T + 1) ≤ W (T + 1) ≤
󰀃 3
4
󰀄mALG(T) · n.
󰀕1
2
󰀖mi(T)
≤
󰀕3
4
󰀖mALG(T)
n
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
15 / 25

Analysis of the Weighted Majority Algorithm
Just algebra from here.
󰀕1
2
󰀖mi(T)
≤
󰀕3
4
󰀖mALG(T)
n
mi(T) · log 1
2 ≤ mALG(T) · log 3
4 + log n
take logs
mALG(T) ≤ 2.41mi(T) + O(log n)
rearranging
Holds for any expert i, including the best one.
Theorem
The weighted majority algorithm with multiplicative penalty 1
2
makes at most 2.41m∗(T) + O(log n) mistakes in the worst case.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
16 / 25

How Good is This?
Our error: 2.41m∗(T) + O(log n)
Theorem
Every deterministic algorithm has some input that forces it to
make ≥ 2m∗(T) mistakes.
We can get very close to this by changing the 1
2 weight penalty to
a diﬀerent fraction.
Theorem
The weighted majority algorithm with multiplicative penalty
(1 − 󰂃), where 0 < 󰂃 ≤ 1
2, makes at most
2(1 + 󰂃)m∗(T) + 2 log n
󰂃
mistakes in the worst case.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
17 / 25

Randomized Variant
We can do even better (on average) with a randomized variant.
Select one expert to listen to randomly.
Use weights, penalize as before.
Select expert i with probability wi(t)/W (t).
Theorem
This randomized algorithm, with multiplicative penalty (1 − 󰂃),
where 0 < 󰂃 ≤ 1
2, makes at most
(1 + 󰂃)m∗(T) + 2 log n
󰂃
mistakes in expectation.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
18 / 25

Part II
The Bigger Picture
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
19 / 25

Flows??
Consider this completely unrelated problem.
Remember network ﬂow?
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
20 / 25

Two-Commodity Flows??
Consider two-commodity ﬂows:
Maximize k such that:
there is an s1t1 ﬂow f1 of size k
there is an s2t2 ﬂow f2 of size k
f1(e) + f2(e) ≤ c(e)
∀e ∈ E (shared bandwidth)
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
21 / 25

Multiplicative Weights Flow Algorithm
Can’t just adapt Ford-Fulkerson!
Consider the following “algorithm”.
f1, f2 ← the zero-ﬂow
w(e) ← δ for some semi-complicated δ, for every e ∈ E
while ...semi-complicated condition... do
for i ← 1, 2 do
P ← a shortest (using lengths w) siti path
γ ← mine∈P c(e)
fi ← fi+ push γ ﬂow on P
w(e) ←
󰀓
1 + 󰂃
γ
c(e)
󰀔
w(e) for e ∈ P
⊲ Penalize selected edges
end for
end while
Scale down f1, f2 until they are feasible
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
22 / 25

Multiplicative Weights Flow Algorithm
A special kind of approximation algorithm.
Theorem
This approach gives a polynomial-time approximation scheme
(PTAS). For any ﬁxed ω > 0, we can tune the parameters δ, 󰂃 to
make an algorithm that achieves:
(1 − ω)-approximation
running time O (poly(|V |, |E|)) (where ω is a hidden constant
in the Big O)
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
23 / 25

Multiplicative Weights
Experts, and this ﬂow algorithm, are special cases of the
Multiplicative Weights “meta-algorithm”.
Use weights to guide decision making.
Experts: weight = how much we care about their opinion.
Flows: weight = how much we care about violating the edge’s
capacity.
Penalize weights multiplicatively.
You can analyze both at the same time using one framework.
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
24 / 25

Applications of Multiplicative Weights
Online algorithms
Online decision making (e.g. experts)
Online convex optimization
Linear Programming
Network ﬂows
Any “packing LP”: constraints look like Ax ≤ b, x ≥ 0.
Machine Learning
Linear classiﬁcation
Boosting
Joseph Poremba
CPSC 420: Lecture 26 The Experts Problem and Multiplicative W
25 / 25