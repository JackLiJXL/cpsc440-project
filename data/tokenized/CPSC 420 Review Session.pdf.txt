CPSC 420 Review Session
▶ Final Exam: Tue Apr 18, 2023 08:30am LSK 200.
One 2-sided page of notes
Today’s Plan (Yikes)
▶ Convex Hull & its Algorithms
▶ Voronoi diagrams
▶ Linear Programming
▶ Network Flow
▶ Ford-Fulkerson algorithm
▶ Maximum matching in bipartite
graphs
▶ Linear programming duality
▶ Compression
▶ Huffman Coding & Lempel-Ziv
Compression
▶ P, NP, and NP-hardness
▶ Karp’s 21 Problems
▶ Approximation algorithms
▶ Hardness of approximation
▶ Online Algorithms
▶ Cuckoo Hashing
▶ RSA cryptosystem
▶ Quantum Computing
▶ Zero-knowledge Proofs
1 / 38

Convex Hull
A set S is convex if for all a, b ∈ S the segment ab is in S.
2 / 38

Convex Hull
A set S is convex if for all a, b ∈ S the segment ab is in S.
The convex hull of a set P of points is the intersection of all
convex sets that contain P, or equivalently the set of all convex
combinations of P (� λipi, � λi = 1).
2 / 38

Convex Hull
A set S is convex if for all a, b ∈ S the segment ab is in S.
The convex hull of a set P of points is the intersection of all
convex sets that contain P, or equivalently the set of all convex
combinations of P (� λipi, � λi = 1).
A point p ∈ P is on the boundary of
CH(P) iff there exists a line ℓ through
p with all P on one side of ℓ.
CH(P)
2 / 38

Convex Hull Algorithms
3 / 38

Convex Hull Algorithms
3 / 38

Convex Hull Algorithms
3 / 38

Reductions (the fast kind)
As a reminder: the concept of reductions is not only for NP! Here’s
a tiny refresher.
4 / 38

Reductions (the fast kind)
As a reminder: the concept of reductions is not only for NP! Here’s
a tiny refresher.
I can use convex hull to sort using the mapping i �→ (i, i2) in time
equal to O(Convex + n), so convex hull can’t be faster than
comparison sorting, and I can use a decision tree to show that’s
Ω(lg n).
4 / 38

Reductions (the fast kind)
As a reminder: the concept of reductions is not only for NP! Here’s
a tiny refresher.
I can use convex hull to sort using the mapping i �→ (i, i2) in time
equal to O(Convex + n), so convex hull can’t be faster than
comparison sorting, and I can use a decision tree to show that’s
Ω(lg n).
(Reminder) in essence the tree has n! leaves, which requires a
depth of Ω(n lg n).
4 / 38

Voronoi diagrams
Definition
A Voronoi diagram of a set of n sites (points) s1, . . . , sn is a set
of regions R1, . . . , Rn where Ri is the set of points x such that
d(x, si) ≤ d(x, sj) for all j.
5 / 38

Voronoi diagrams
Definition
A Voronoi diagram of a set of n sites (points) s1, . . . , sn is a set
of regions R1, . . . , Rn where Ri is the set of points x such that
d(x, si) ≤ d(x, sj) for all j.
A Voronoi edge is the border between two regions:
{x|x ∈ Ri and x ∈ Rj and i ̸= j}.
5 / 38

Voronoi diagrams
Definition
A Voronoi diagram of a set of n sites (points) s1, . . . , sn is a set
of regions R1, . . . , Rn where Ri is the set of points x such that
d(x, si) ≤ d(x, sj) for all j.
A Voronoi edge is the border between two regions:
{x|x ∈ Ri and x ∈ Rj and i ̸= j}.
A Voronoi vertex is the intersection of Voronoi edges:
{x|x in more than 2 Vor. Regions}.
5 / 38

Voronoi diagrams
Definition
A Voronoi diagram of a set of n sites (points) s1, . . . , sn is a set
of regions R1, . . . , Rn where Ri is the set of points x such that
d(x, si) ≤ d(x, sj) for all j.
A Voronoi edge is the border between two regions:
{x|x ∈ Ri and x ∈ Rj and i ̸= j}.
A Voronoi vertex is the intersection of Voronoi edges:
{x|x in more than 2 Vor. Regions}.
Problem Given S = s1, s2, . . . , sn, find Voronoi vertices and edges
5 / 38

Algorithm from Computational Geometry by Preparata & Shamos
6 / 38

Exercises
1. Of Graham and Jarvis, which is faster on a circle of points?
7 / 38

Exercises
1. Of Graham and Jarvis, which is faster on a circle of points?
Graham
2. Which is faster on a line?
7 / 38

Exercises
1. Of Graham and Jarvis, which is faster on a circle of points?
Graham
2. Which is faster on a line? Jarvis
3. How can you tell a vertex is on the convex hull using a
Voronoi diagram?
7 / 38

Exercises
1. Of Graham and Jarvis, which is faster on a circle of points?
Graham
2. Which is faster on a line? Jarvis
3. How can you tell a vertex is on the convex hull using a
Voronoi diagram? It has infinite area
4. Say I offered you a O(h log(log h)4) algorithm for Convex
Hull. Is that cool?
7 / 38

Exercises
1. Of Graham and Jarvis, which is faster on a circle of points?
Graham
2. Which is faster on a line? Jarvis
3. How can you tell a vertex is on the convex hull using a
Voronoi diagram? It has infinite area
4. Say I offered you a O(h log(log h)4) algorithm for Convex
Hull. Is that cool? No
7 / 38

Linear Programming
Definition
A Linear Program is a problem that can be arranged into the form
maximize
cT x
subject to
Ax≤ b
x ≥ 0
8 / 38

Linear Programming
Definition
A Linear Program is a problem that can be arranged into the form
maximize
cT x
subject to
Ax≤ b
x ≥ 0
A popular algorithm for solving LPs is Danzig’s Simplex
Algorithm:
1. Start at a vertex v of the feasible set
3.2. While there is a neighbor v′ of v with better objective value
v = v′
8 / 38

Network Flows
A flow network is a directed graph G = (V , E) in which each
edge (u, v) ∈ E has a positive capacity c(u, v) (non-edges have
capacity 0).
G contains a source vertex s and a sink vertex t.
9 / 38

Network Flows
A flow network is a directed graph G = (V , E) in which each
edge (u, v) ∈ E has a positive capacity c(u, v) (non-edges have
capacity 0).
G contains a source vertex s and a sink vertex t.
A flow is an assignment f of real numbers to edges of G:
1. Non-negativity: 0 ≤ fe
2. Capacity fe ≤ ce
3. Conservation v ̸= s, t : � fδ−(v) = � fδ+(v)
9 / 38

Network Flows
A flow network is a directed graph G = (V , E) in which each
edge (u, v) ∈ E has a positive capacity c(u, v) (non-edges have
capacity 0).
G contains a source vertex s and a sink vertex t.
A flow is an assignment f of real numbers to edges of G:
1. Non-negativity: 0 ≤ fe
2. Capacity fe ≤ ce
3. Conservation v ̸= s, t : � fδ−(v) = � fδ+(v)
The size (or value) of a flow is: size(f ) = �
v f (s, v) − f (v, s)
9 / 38

Network Flows
A flow network is a directed graph G = (V , E) in which each
edge (u, v) ∈ E has a positive capacity c(u, v) (non-edges have
capacity 0).
G contains a source vertex s and a sink vertex t.
A flow is an assignment f of real numbers to edges of G:
1. Non-negativity: 0 ≤ fe
2. Capacity fe ≤ ce
3. Conservation v ̸= s, t : � fδ−(v) = � fδ+(v)
The size (or value) of a flow is: size(f ) = �
v f (s, v) − f (v, s)
Goal: Find flow with maximum size.
s
a
b
c
d
e
t
3
3
4
10
2
1
1
5
1
2
5
9 / 38

Max Flow via Path Augmentation [Ford & Fulkerson 1962]
1. Start with zero flow (a feasible solution)
2. Repeat until impossible
▶ Choose an augmenting path from s to t
▶ Increase flow on this path as much as
possible
s
a
b
t
1
1
1
1
1
s
a
b
t
10 / 38

Max Flow via Path Augmentation [Ford & Fulkerson 1962]
1. Start with zero flow (a feasible solution)
2. Repeat until impossible
▶ Choose an augmenting path from s to t
▶ Increase flow on this path as much as
possible
s
a
b
t
1
1
1
1
1
s
a
b
t
The residual network of flow network G = (V , E) with flow f is
G f = (V , E f ) where
E f = {(u, v)|f (u, v) < c(u, v) or f (v, u) > 0}
The residual capacity of an edge (u, v) ∈ E f is
cf (u, v) =
�
c(u, v) − f (u, v)
if f (u, v) < c(u, v)
f (v, u)
if f (v, u) > 0
An augmenting path in G is an s ⇝ t path in G f
10 / 38

Residual Network Example
14
7
s
a
b
c
d
t
5
5
12
5
5
2
7
3
3 5 5
5
4
Residual network
3 5 5
=
a
d
8
5
a
d
3
5
7
3
5
s
a
b
c
d
t
Flow
3 10
7
17
10
s
a
b
c
d
t
Flow network
5
5
0
5
4
12
7
17
11 / 38

Ford & Fulkerson, cont.
More Terminology
A cut is a partition (S, T) of V such that s ∈ S and t ∈ T. (Cut
separates s from t.)
The capacity of cut (S, T) is c(S, T) =
�
u∈S,v∈T
c(u, v)
The flow across cut (S, T) is f (S, T) =
�
u∈S,v∈T
f (u, v) − f (v, u)
12 / 38

Ford & Fulkerson, cont.
More Terminology
A cut is a partition (S, T) of V such that s ∈ S and t ∈ T. (Cut
separates s from t.)
The capacity of cut (S, T) is c(S, T) =
�
u∈S,v∈T
c(u, v)
The flow across cut (S, T) is f (S, T) =
�
u∈S,v∈T
f (u, v) − f (v, u)
The Gem: Max-Flow Min-Cut
|f ∗| = c(S∗, T ∗)
Runtime
O(m|f ∗|) (pseudo-polynomial). Solution?
12 / 38

Ford & Fulkerson, cont.
More Terminology
A cut is a partition (S, T) of V such that s ∈ S and t ∈ T. (Cut
separates s from t.)
The capacity of cut (S, T) is c(S, T) =
�
u∈S,v∈T
c(u, v)
The flow across cut (S, T) is f (S, T) =
�
u∈S,v∈T
f (u, v) − f (v, u)
The Gem: Max-Flow Min-Cut
|f ∗| = c(S∗, T ∗)
Runtime
O(m|f ∗|) (pseudo-polynomial). Solution? Edmonds-Karp is
O(mn2) by using shortest path. There is even faster though!
12 / 38

Maximum Matching in Bipartite Graphs
A matching in a graph G is a subset M of its
edges with no vertex the endpoint of more than
one edge in M.
A maximum matching is a matching with the
maximum number of edges.
A maximal matching is a matching to which
another edge cannot be added to form a new
matching.
matching edge
A bipartite graph is a graph G = (V , E) where
V can be partitioned into A and B such that
∀(u, v) ∈ E, either u ∈ A and v ∈ B or u ∈ B
and v ∈ A.
13 / 38

Maximum Matching in Bipartite Graphs
A matching in a graph G is a subset M of its
edges with no vertex the endpoint of more than
one edge in M.
A maximum matching is a matching with the
maximum number of edges.
A maximal matching is a matching to which
another edge cannot be added to form a new
matching.
matching edge
A bipartite graph is a graph G = (V , E) where
V can be partitioned into A and B such that
∀(u, v) ∈ E, either u ∈ A and v ∈ B or u ∈ B
and v ∈ A.
B
A
Given bipartite graph G = (V , E) with partitions A and B
find maximum matching in G.
13 / 38

Exercises
Projects with Dependencies
We have n projects, each with value pi. pi > 0 is a profit, and
pi < 0 is a cost. Additionally, we have an acyclic dependency
graph G where the edge (i, j) means i depends on j.
Goal: find projects to do so as to maximize profit.
14 / 38

LP Duality
The dual program is constructed as follows:
▶ Variable ←→ Constraint
▶ Maximum ←→ Minimum
maximize
cTx
subject to
Ax≤ b
x≥ 0
minimize
bTy
subject to
ATy≥ c
y≥ 0
15 / 38

LP Duality
The dual program is constructed as follows:
▶ Variable ←→ Constraint
▶ Maximum ←→ Minimum
maximize
cTx
subject to
Ax≤ b
x≥ 0
minimize
bTy
subject to
ATy≥ c
y≥ 0
Weak Duality gives us says the dual solution is always an upper
bound on the primal solution. Strong Duality says that they have
the same optima if it exists.
15 / 38

Dynamic Programming
Dynamic Programming is an oft-overloaded term. The two key
things you need to call what you’re doing Dynamic Programming
are
16 / 38

Dynamic Programming
Dynamic Programming is an oft-overloaded term. The two key
things you need to call what you’re doing Dynamic Programming
are
1. The solution is a combination of sub-problem solutions.
16 / 38

Dynamic Programming
Dynamic Programming is an oft-overloaded term. The two key
things you need to call what you’re doing Dynamic Programming
are
1. The solution is a combination of sub-problem solutions.
2. There is a nice number of total unique subproblems, so we
solve them once
16 / 38

Dynamic Programming
Dynamic Programming is an oft-overloaded term. The two key
things you need to call what you’re doing Dynamic Programming
are
1. The solution is a combination of sub-problem solutions.
2. There is a nice number of total unique subproblems, so we
solve them once
Within this, there are 2 key ways to approach the solution:
▶ Recursive or Top-Down solutions solve the top level by calling
& memoizing sub-problems.
▶ Iterative or Bottom-Up solutions start with the sub-problems
with no dependencies, then build up until they reach the top
level.
16 / 38

Exercises: 0-1 Knapsack
We have n items, each with a profit pi and a weight wi. Given a
total profit P and total weight W , does there exists a set S ⊆ [n]
with � pS ≥ P, � pS ≤ W ?
17 / 38

Information Theory [Shannon 1948]
The information content / surprisal contained in a message i
that has probability pi of being sent is
log2
pi1
Entropy is the average information content of a message X:
H(X) =
�
i
pi log2
pi1
= −
�
i
pi log2 pi
18 / 38

Information Theory [Shannon 1948]
The information content / surprisal contained in a message i
that has probability pi of being sent is
log2
pi1
Entropy is the average information content of a message X:
H(X) =
�
i
pi log2
pi1
= −
�
i
pi log2 pi
Source Coding Theorem m i.i.d. random variables each with
entropy H(X) can be compressed into more than mH(X) bits with
negligible risk of information loss but using less than mH(X) bits
results almost certainly in information loss. [Wikipedia-ish]
18 / 38

Huffman Coding and LZ78
Huffman Coding
Given a set of characters a1, a2, . . . , aα and a probability pi for
each ai (�
i pi = 1), construct an encoding ci for each character ai
so that the expected length of an encoded message is minimized,
then it’s just lookup.
19 / 38

Huffman Coding and LZ78
Huffman Coding
Given a set of characters a1, a2, . . . , aα and a probability pi for
each ai (�
i pi = 1), construct an encoding ci for each character ai
so that the expected length of an encoded message is minimized,
then it’s just lookup.
LZ78
Parse input into distinct phrases reading from left to right.
Each phrase is the shortest string not already a phrase. The 0th
phrase is ∅.
Output ic for each phrase w, where c is the last character of w
and i is the index of phrase u where w = u ◦ c
Length is c(n)(lg c(n) + lg α) bits, as good as any finite state
compressor.
19 / 38

Exercises
1. Imagine I promise you 2 compression algorithms: the first
compresses to 70% on average, and at worst it compresses to
110%. The second compresses to 25% on average, and at
worst it compresses to 97%. Why would you ask for the first?
20 / 38

Exercises
1. Imagine I promise you 2 compression algorithms: the first
compresses to 70% on average, and at worst it compresses to
110%. The second compresses to 25% on average, and at
worst it compresses to 97%. Why would you ask for the first?
The second is impossible, so I am either wrong or lying.
2. T/F: A low probability message has high information
20 / 38

Exercises
1. Imagine I promise you 2 compression algorithms: the first
compresses to 70% on average, and at worst it compresses to
110%. The second compresses to 25% on average, and at
worst it compresses to 97%. Why would you ask for the first?
The second is impossible, so I am either wrong or lying.
2. T/F: A low probability message has high information True
3. Lempel-Ziv Compress the string AABAABBAA$.
20 / 38

Exercises
1. Imagine I promise you 2 compression algorithms: the first
compresses to 70% on average, and at worst it compresses to
110%. The second compresses to 25% on average, and at
worst it compresses to 97%. Why would you ask for the first?
The second is impossible, so I am either wrong or lying.
2. T/F: A low probability message has high information True
3. Lempel-Ziv Compress the string AABAABBAA$.
Dict:{,0A, 1B, 1A, 0B, 4A, 1$}, String: 0A1B1A0B4A1$
20 / 38

Complexity Classes
21 / 38

What is a Reduction?
There are loads of ways to conceptualize a hardness reduction from
A → B:
▶ Prove that if I can solve B, I can solve A
▶ I can turn an input to A into an input for B, then transform
the output back to the answer for A.
▶ If you hand me a B-machine, I can write a polyime algorithm
for A
22 / 38

Reduction Structure & Strategy
Rules
The strategy of reduction we typically use is often called a Karp
Reduction (guess why?). In this strategy, you cannot mess with
the oracle. All you get to do is transform an input, then
transform an output.
Proof Structure
With that in mind, a nice Completeness/Hardness answer for
problem A usually follows the following structure (as do our
rubrics):
23 / 38

Reduction Structure & Strategy
Rules
The strategy of reduction we typically use is often called a Karp
Reduction (guess why?). In this strategy, you cannot mess with
the oracle. All you get to do is transform an input, then
transform an output.
Proof Structure
With that in mind, a nice Completeness/Hardness answer for
problem A usually follows the following structure (as do our
rubrics):
1. Prove A in NP if we want Completeness
23 / 38

Reduction Structure & Strategy
Rules
The strategy of reduction we typically use is often called a Karp
Reduction (guess why?). In this strategy, you cannot mess with
the oracle. All you get to do is transform an input, then
transform an output.
Proof Structure
With that in mind, a nice Completeness/Hardness answer for
problem A usually follows the following structure (as do our
rubrics):
1. Prove A in NP if we want Completeness
2. State your input mapping and output mapping
23 / 38

Reduction Structure & Strategy
Rules
The strategy of reduction we typically use is often called a Karp
Reduction (guess why?). In this strategy, you cannot mess with
the oracle. All you get to do is transform an input, then
transform an output.
Proof Structure
With that in mind, a nice Completeness/Hardness answer for
problem A usually follows the following structure (as do our
rubrics):
1. Prove A in NP if we want Completeness
2. State your input mapping and output mapping
3. Prove A = T =⇒ B = T OR B = F =⇒ A = F
23 / 38

Reduction Structure & Strategy
Rules
The strategy of reduction we typically use is often called a Karp
Reduction (guess why?). In this strategy, you cannot mess with
the oracle. All you get to do is transform an input, then
transform an output.
Proof Structure
With that in mind, a nice Completeness/Hardness answer for
problem A usually follows the following structure (as do our
rubrics):
1. Prove A in NP if we want Completeness
2. State your input mapping and output mapping
3. Prove A = T =⇒ B = T OR B = F =⇒ A = F
4. Prove A = F =⇒ B = F OR B = T =⇒ A = T
If it seems really intuitive and you think it’s still rigourous enough,
you can combine the last two, but beware!
23 / 38

Karp’s 21 Problems
A quick bit of history
: In 1971 The Cook-Levin Theorem proves that SAT is NP-Hard
from scratch, then in 1972 Karp uses reductions to prove the same
for 21 other problems (a year after Edmonds-Karp). They receive a
Turing Award each, and get everyone excited.
24 / 38

Karp’s 21 Problems (Exercises)
Clique / Independent Set
▶ Set packing
▶ Vertex cover
▶ Set covering
▶ Feedback node set
▶ Feedback arc set
▶ Directed Hamilton cycle
▶ Undirected Hamilton
cycle
Oh, and 0–1 integer programming
(A sat-only variation)
3-SAT
▶ Chromatic number /
Coloring
▶ Clique cover
▶ Exact cover
▶ Hitting set
▶ Steiner tree
▶ 3-D matching
▶ Knapsack / Subset
Sum
▶ - Job sequencing
▶▶ - Partition
- Max Cut
25 / 38

Approximation Algorithms
Approximation Algorithms are useful for optimization problems,
not decision problems.
An f (n)-approximation algorithm A for a minimization problem has
A
OPT ≤ f (n)
An f (n)-approximation algorithm A for a maximization problem has
OPT
A
≤ f (n)
Sometimes, you get problems that are even hard to approximate!
26 / 38

Online Algorithms
Online Algorithms work on streams of input instead of fixed inputs.
An f (n)-competitive algorithm A for a minimization problem has
E(A)
OPT ≤ f (n)
An f (n)-competitive algorithm A for a maximization problem has
E(A)
OPT ≥ f (n)
OPT here is not an online algorithm. It gets the whole string up
front!
Wait, those equations look different.
27 / 38

Online Algorithms
Online Algorithms work on streams of input instead of fixed inputs.
An f (n)-competitive algorithm A for a minimization problem has
E(A)
OPT ≤ f (n)
An f (n)-competitive algorithm A for a maximization problem has
E(A)
OPT ≥ f (n)
OPT here is not an online algorithm. It gets the whole string up
front!
Wait, those equations look different. How does this make me feel?
27 / 38

Online Algorithms
Online Algorithms work on streams of input instead of fixed inputs.
An f (n)-competitive algorithm A for a minimization problem has
E(A)
OPT ≤ f (n)
An f (n)-competitive algorithm A for a maximization problem has
E(A)
OPT ≥ f (n)
OPT here is not an online algorithm. It gets the whole string up
front!
Wait, those equations look different. How does this make me feel? Confused.
27 / 38

Exercise
Knapsack 2-Approximation
We have n items, each with a profit pi and a weight wi. Given a
total weight W , maximize � pS ≥ P by picking S ⊆ [n] with
� pS ≤ W .
28 / 38

Randomized Marking Mouse
If Mouse follows a deterministic strategy, there is a sequence S of
Cat probes that causes
MouseCost(S) ≥ (m − 1)OPT(S)
Paging
m − 1 = cache size
m = different pages
Mouse = page not in cache
Cat probes = page requests
Must move = page fault
Randomized Marking Mouse (RMM)
• Start at random spot
• If Cat probes a spot, mark it
• If Cat probes Mouse’s spot,
Mouse moves to random unmarked spot
• If Mouse is at last unmarked spot, clear marks [phase ends]
29 / 38

Randomized Marking Mouse performance
Claim: E[RMMCost(S)] ≤ O(log m)OPT(S)
Proof: Initially, RMM is equally likely to be at any of the m spots.
1st probe finds Mouse with probability 1/m.
Whether Mouse is found or not, Mouse is at each of the m−1
unmarked spots with prob. 1/(m − 1).
2nd probe (to unmarked spot) finds Mouse with prob 1/(m − 1).
Mouse is at each of the m − 2 unmarked spots with prob.
1/(m − 2). Etc.
Let Xi =
�
1
if Mouse found on ith probe to unmarked spot
0
otherwise
E[#times found per phase] = E[X1 + X2 + · · · + Xm]
≤ 1
m +
1
m − 1 + · · · + 1
1 = O(log m)
OPT moves once per phase.
30 / 38

Is Totally Random Mouse (TRM) better?
TRM runs to a random spot if found.
Consider the Methodical Cat (MC):
• Probe spots 1, 2, 3, ... until Mouse found
• Repeat
What does the OPT mouse do?
31 / 38

Is Totally Random Mouse (TRM) better?
TRM runs to a random spot if found.
Consider the Methodical Cat (MC):
• Probe spots 1, 2, 3, ... until Mouse found
• Repeat
What does the OPT mouse do? Hide in spot m
E[#times RM found before MC probes m] =
E[#rolls of m-sided dice before m] = m
⇒ RM is m-competitive.
31 / 38

Random Marking Mouse is best
Claim: Any Mouse A has E[A(S)] ∈ Ω(log m)OPT(S)
Proof:
Idea: Show that a Cat exists that will cause E[A(S)] ∈ Ω(log m)
regardless of the Mouse.
Random Cat (RC) probes a random spot with each probe. RC
finds Mouse with prob.
1
m no matter what Mouse does.
⇒ E[A(S)] after t probes is
t
m.
How many RC probes until RC examines every spot?
Coupon Collector Problem ⇒ Θ(m log m)
So OPT Mouse (that knows RC’s probes) moves once in sequence
S of Ω(m log m) probes, while Mouse A moves
E[A(S)] ∈ Ω(m log m)
m
= Ω(log m) times.
32 / 38

Universal Families of Hash Functions
A family of hash functions H (that map U → {0, 1, . . . , m − 1}) is
universal if for all distinct keys x, y ∈ U
Pr
h∈H[h(x) = h(y)] ≤ 1
m.
Example
Let ha,b(x) = ((ax + b) mod p) mod m where p is a prime bigger
than any key.
H =
�
ha,b|a ∈ {1, 2, . . . , p − 1}, b ∈ {0, 1, . . . , p − 1}
�
33 / 38

Cuckoo Hashing
Time per operation
Find O(1) time worst case
Delete O(1) time worst case
Insert O(1) expected, amortized time
▶ Use two hash functions h1 and h2.
▶ Item x will be stored in slot h1(x) or h2(x) of hash table T.
▶ Each slot in the hash table can contain at most one item.
▶ n = maximum number of items stored at any time
▶ m = size of hash table T (m > n)
On an insert(x) collision, item x kicks the resident item y out.
Item y then goes to its alternate slot (kicking whoever’s there
out). Etc. Etc.
34 / 38

RSA public/private key cryptosystem [Rivest,Shamir,Adleman ’77]
Bob has two functions: secret SB() and public PB()
Properties:
1. SB(PB(M)) = M and PB(SB(M)) = M
2. Hard to find M given PB(M) without SB()
Alice sends PB(M) to Bob.
Bob decrypts: SB(PB(M)) = M
Good: Use again and again
Bad: No one knows if it’s secure.
factoring easy ⇒ RSA breakable.
factoring hard ⇒ RSA secure? (unknown)
Digital Signatures: Alice sends (M, σ = SA(M) to Bob
Bob can check that PA(σ) = M.
35 / 38

Quantum Computing
Qubits
A qubit is a combination |0⟩ and |1⟩:
|ψ⟩ = α|0⟩ + β|1⟩
α and β are both complex numbers here, but since it must be that
α2 + β2 = 1, we actually only have 3 degrees of freedom.
Gates
Quantum gates, at the end of the day, are unitary matrices. For
example, Hadamard is
1
√
2
�1
1
1
−1
�
.
36 / 38

Zero Knowledge
The key idea here, is prove your identity without giving away any
of your secrets.
If there’s time, a quick example: Colour-Blindness [Credits to
Wikipedia]
37 / 38

Closing Remarks
Any Questions?
38 / 38