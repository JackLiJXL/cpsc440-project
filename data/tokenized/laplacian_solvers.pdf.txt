Foundations and Trends R
⃝ in
Theoretical Computer Science
Vol. 8, Nos. 1–2 (2012) 1–141
c⃝ 2013 N. K. Vishnoi
DOI: 10.1561/0400000054
Lx = b
Laplacian Solvers and
Their Algorithmic Applications
By Nisheeth K. Vishnoi
Contents
Preface
2
Notation
6
I
Basics
8
1
Basic Linear Algebra
9
1.1
Spectral Decomposition of Symmetric Matrices
9
1.2
Min–Max Characterizations of Eigenvalues
12
2
The Graph Laplacian
14
2.1
The Graph Laplacian and Its Eigenvalues
14
2.2
The Second Eigenvalue and Connectivity
16
3
Laplacian Systems and Solvers
18
3.1
System of Linear Equations
18
3.2
Laplacian Systems
19
3.3
An Approximate, Linear-Time Laplacian Solver
19
3.4
Linearity of the Laplacian Solver
20

4
Graphs as Electrical Networks
22
4.1
Incidence Matrices and Electrical Networks
22
4.2
Eﬀective Resistance and the Π Matrix
24
4.3
Electrical Flows and Energy
25
4.4
Weighted Graphs
27
II
Applications
28
5
Graph Partitioning I The Normalized Laplacian
29
5.1
Graph Conductance
29
5.2
A Mathematical Program
31
5.3
The Normalized Laplacian and Its Second Eigenvalue
34
6
Graph Partitioning II
A Spectral Algorithm for Conductance
37
6.1
Sparse Cuts from ℓ1 Embeddings
37
6.2
An ℓ1 Embedding from an ℓ2
2 Embedding
40
7
Graph Partitioning III Balanced Cuts
44
7.1
The Balanced Edge-Separator Problem
44
7.2
The Algorithm and Its Analysis
46
8
Graph Partitioning IV
Computing the Second Eigenvector
49
8.1
The Power Method
49
8.2
The Second Eigenvector via Powering
50
9
The Matrix Exponential and Random Walks
54
9.1
The Matrix Exponential
54
9.2
Rational Approximations to the Exponential
56
9.3
Simulating Continuous-Time Random Walks
59

10 Graph Sparsiﬁcation I
Sparsiﬁcation via Eﬀective Resistances
62
10.1 Graph Sparsiﬁcation
62
10.2 Spectral Sparsiﬁcation Using Eﬀective Resistances
64
10.3 Crude Spectral Sparsﬁcation
67
11 Graph Sparsiﬁcation II
Computing Electrical Quantities
69
11.1 Computing Voltages and Currents
69
11.2 Computing Eﬀective Resistances
71
12 Cuts and Flows
75
12.1 Maximum Flows, Minimum Cuts
75
12.2 Combinatorial versus Electrical Flows
77
12.3 s,t-MaxFlow
78
12.4 s,t-Min Cut
83
III
Tools
86
13 Cholesky Decomposition Based Linear Solvers
87
13.1 Cholesky Decomposition
87
13.2 Fast Solvers for Tree Systems
89
14 Iterative Linear Solvers I
The Kaczmarz Method
92
14.1 A Randomized Kaczmarz Method
92
14.2 Convergence in Terms of Average Condition Number
94
14.3 Toward an ˜O(m)-Time Laplacian Solver
96
15 Iterative Linear Solvers II
The Gradient Method
99

15.1 Optimization View of Equation Solving
99
15.2 The Gradient Descent-Based Solver
100
16 Iterative Linear Solvers III
The Conjugate Gradient Method
103
16.1 Krylov Subspace and A-Orthonormality
103
16.2 Computing the A-Orthonormal Basis
105
16.3 Analysis via Polynomial Minimization
107
16.4 Chebyshev Polynomials — Why Conjugate
Gradient Works
110
16.5 The Chebyshev Iteration
111
16.6 Matrices with Clustered Eigenvalues
112
17 Preconditioning for Laplacian Systems
114
17.1 Preconditioning
114
17.2 Combinatorial Preconditioning via Trees
116
17.3 An ˜O(m
4/3)-Time Laplacian Solver
117
18 Solving a Laplacian System in ˜O(m) Time
119
18.1 Main Result and Overview
119
18.2 Eliminating Degree 1,2 Vertices
122
18.3 Crude Sparsiﬁcation Using Low-Stretch
Spanning Trees
123
18.4 Recursive Preconditioning — Proof of the
Main Theorem
125
18.5 Error Analysis and Linearity of the Inverse
127
19 Beyond Ax = b The Lanczos Method
129
19.1 From Scalars to Matrices
129
19.2 Working with Krylov Subspace
130
19.3 Computing a Basis for the Krylov Subspace
132
References
136

Foundations and Trends R
⃝ in
Theoretical Computer Science
Vol. 8, Nos. 1–2 (2012) 1–141
c⃝ 2013 N. K. Vishnoi
DOI: 10.1561/0400000054
Lx = b
Laplacian Solvers and
Their Algorithmic Applications
Nisheeth K. Vishnoi
Microsoft Research, India, nisheeth.vishnoi@gmail.com
Abstract
The ability to solve a system of linear equations lies at the heart of areas
such as optimization, scientiﬁc computing, and computer science, and
has traditionally been a central topic of research in the area of numer-
ical linear algebra. An important class of instances that arise in prac-
tice has the form Lx = b, where L is the Laplacian of an undirected
graph. After decades of sustained research and combining tools from
disparate areas, we now have Laplacian solvers that run in time nearly-
linear in the sparsity (that is, the number of edges in the associated
graph) of the system, which is a distant goal for general systems. Sur-
prisingly, and perhaps not the original motivation behind this line of
research, Laplacian solvers are impacting the theory of fast algorithms
for fundamental graph problems. In this monograph, the emerging
paradigm of employing Laplacian solvers to design novel fast algorithms
for graph problems is illustrated through a small but carefully chosen
set of examples. A part of this monograph is also dedicated to develop-
ing the ideas that go into the construction of near-linear-time Laplacian
solvers. An understanding of these methods, which marry techniques
from linear algebra and graph theory, will not only enrich the tool-set
of an algorithm designer but will also provide the ability to adapt these
methods to design fast algorithms for other fundamental problems.

Preface
The ability to solve a system of linear equations lies at the heart of areas
such as optimization, scientiﬁc computing, and computer science and,
traditionally, has been a central topic of research in numerical linear
algebra. Consider a system Ax = b with n equations in n variables.
Broadly, solvers for such a system of equations fall into two categories.
The ﬁrst is Gaussian elimination-based methods which, essentially, can
be made to run in the time it takes to multiply two n × n matrices,
(currently O(n2.3...) time). The second consists of iterative methods,
such as the conjugate gradient method. These reduce the problem
to computing n matrix–vector products, and thus make the running
time proportional to mn where m is the number of nonzero entries, or
sparsity, of A.1 While this bound of n in the number of iterations is
tight in the worst case, it can often be improved if A has additional
structure, thus, making iterative methods popular in practice.
An important class of such instances has the form Lx = b, where L
is the Laplacian of an undirected graph G with n vertices and m edges
1 Strictly speaking, this bound on the running time assumes that the numbers have bounded
precision.
2

Preface
3
with m (typically) much smaller than n2. Perhaps the simplest setting
in which such Laplacian systems arise is when one tries to compute cur-
rents and voltages in a resistive electrical network. Laplacian systems
are also important in practice, e.g., in areas such as scientiﬁc computing
and computer vision. The fact that the system of equations comes from
an underlying undirected graph made the problem of designing solvers
especially attractive to theoretical computer scientists who entered the
fray with tools developed in the context of graph algorithms and with
the goal of bringing the running time down to O(m). This eﬀort gained
serious momentum in the last 15 years, perhaps in light of an explosive
growth in instance sizes which means an algorithm that does not scale
near-linearly is likely to be impractical.
After decades of sustained research, we now have a solver for Lapla-
cian systems that runs in O(mlogn) time. While many researchers have
contributed to this line of work, Spielman and Teng spearheaded this
endeavor and were the ﬁrst to bring the running time down to ˜O(m)
by combining tools from graph partitioning, random walks, and low-
stretch spanning trees with numerical methods based on Gaussian elim-
ination and the conjugate gradient. Surprisingly, and not the original
motivation behind this line of research, Laplacian solvers are impacting
the theory of fast algorithms for fundamental graph problems; giving
back to an area that empowered this work in the ﬁrst place.
That is the story this monograph aims to tell in a comprehensive
manner to researchers and aspiring students who work in algorithms
or numerical linear algebra. The emerging paradigm of employing
Laplacian solvers to design novel fast algorithms for graph problems
is illustrated through a small but carefully chosen set of problems
such as graph partitioning, computing the matrix exponential, simulat-
ing random walks, graph sparsiﬁcation, and single-commodity ﬂows. A
signiﬁcant part of this monograph is also dedicated to developing the
algorithms and ideas that go into the proof of the Spielman–Teng Lapla-
cian solver. It is a belief of the author that an understanding of these
methods, which marry techniques from linear algebra and graph theory,
will not only enrich the tool-set of an algorithm designer, but will also
provide the ability to adapt these methods to design fast algorithms
for other fundamental problems.

4
Preface
How to use this monograph.
This monograph can be used as the
text for a graduate-level course or act as a supplement to a course on
spectral graph theory or algorithms. The writing style, which deliber-
ately emphasizes the presentation of key ideas over rigor, should even
be accessible to advanced undergraduates. If one desires to teach a
course based on this monograph, then the best order is to go through
the sections linearly. Essential are Sections 1 and 2 that contain the
basic linear algebra material necessary to follow this monograph and
Section 3 which contains the statement and a discussion of the main
theorem regarding Laplacian solvers. Parts of this monograph can also
be read independently. For instance, Sections 5–7 contain the Cheeger
inequality based spectral algorithm for graph partitioning. Sections 15
and 16 can be read in isolation to understand the conjugate gradient
method. Section 19 looks ahead into computing more general functions
than the inverse and presents the Lanczos method. A dependency dia-
gram between sections appears in Figure 1. For someone solely inter-
ested in a near-linear-time algorithm for solving Laplacian systems, the
quick path to Section 14, where the approach of a short and new proof
is presented, should suﬃce. However, the author recommends going all
1,2,3
5
8
9
4
13
15
6
11
12
10
16
14
7
17
19
18
Fig. 1 The dependency diagram among the sections in this monograph. A dotted line from
i to j means that the results of Section j use some results of Section i in a black-box manner
and a full understanding is not required.

Preface
5
the way to Section 18 where multiple techniques developed earlier in
the monograph come together to give an ˜O(m) Laplacian solver.
Acknowledgments.
This monograph is partly based on lectures
delivered by the author in a course at the Indian Institute of Sci-
ence, Bangalore. Thanks to the scribes: Deeparnab Chakrabarty,
Avishek Chatterjee, Jugal Garg, T. S. Jayaram, Swaprava Nath, and
Deepak R. Special thanks to Elisa Celis, Deeparnab Chakrabarty,
Lorenzo Orecchia, Nikhil Srivastava, and Sushant Sachdeva for read-
ing through various parts of this monograph and providing valuable
feedback. Finally, thanks to the reviewer(s) for several insightful com-
ments which helped improve the presentation of the material in this
monograph.
Bangalore
Nisheeth K. Vishnoi
15 January 2013
Microsoft Research India

Notation
• The set of real numbers is denoted by R, and R≥0 denotes
the set of nonnegative reals. We only consider real numbers
in this monograph.
• The set of integers is denoted by Z, and Z≥0 denotes the set
of nonnegative integers.
• Vectors are denoted by boldface, e.g., u,v. A vector v ∈ Rn
is a column vector but often written as v = (v1,...,vn). The
transpose of a vector v is denoted by v⊤.
• For vectors u,v, their inner product is denoted by ⟨u,v⟩ or
u⊤v.
• For a vector v, ∥v∥ denotes its ℓ2 or Euclidean norm where
∥v∥ def
=
�
⟨v,v⟩. We sometimes also refer to the ℓ1 or Man-
hattan distance norm ∥v∥1
def
= �n
i=1 |vi|.
• The outer product of a vector v with itself is denoted by
vv⊤.
• Matrices are denoted by capitals, e.g., A,L. The transpose
of A is denoted by A⊤.
• We use tA to denote the time it takes to multiply the matrix
A with a vector.
6

Notation
7
• The A-norm of a vector v is denoted by ∥v∥A
def
=
√
v⊤Av.
• For
a
real
symmetric
matrix
A,
its
real
eigenvalues
are ordered λ1(A) ≤ λ2(A) ≤ ··· ≤ λn(A). We let Λ(A) def
=
[λ1(A),λn(A)].
• A positive-semideﬁnite (PSD) matrix is denoted by A ⪰ 0
and a positive-deﬁnite matrix A ≻ 0.
• The norm of a symmetric matrix A is denoted by ∥A∥ def
=
max{|λ1(A)|,|λn(A)|}. For a symmetric PSD matrix A,
∥A∥ = λn(A).
• Thinking of a matrix A as a linear operator, we denote the
image of A by Im(A) and the rank of A by rank(A).
• A graph G has a vertex set V and an edge set E. All graphs
are assumed to be undirected unless stated otherwise. If the
graph is weighted, there is a weight function w : E �→ R≥0.
Typically, n is reserved for the number of vertices |V |, and
m for the number of edges |E|.
• EF[·] denotes the expectation and PF[·] denotes the proba-
bility over a distribution F. The subscript is dropped when
clear from context.
• The following acronyms are used liberally, with respect to
(w.r.t.), without loss of generality (w.l.o.g.), with high prob-
ability (w.h.p.), if and only if (iﬀ), right-hand side (r.h.s.),
left-hand side (l.h.s.), and such that (s.t.).
• Standard big-o notation is used to describe the limiting
behavior of a function.
˜O denotes potential logarithmic
factors which are ignored, i.e., f = ˜O(g) is equivalent to
f = O(glogk(g)) for some constant k.

Part I
Basics

1
Basic Linear Algebra
This section reviews basics from linear algebra, such as eigenvalues and
eigenvectors, that are relevant to this monograph. The spectral theorem
for symmetric matrices and min–max characterizations of eigenvalues
are derived.
1.1
Spectral Decomposition of Symmetric Matrices
One way to think of an m × n matrix A with real entries is as a linear
operator from Rn to Rm which maps a vector v ∈ Rn to Av ∈ Rm.
Let dim(S) be dimension of S, i.e., the maximum number of linearly
independent vectors in S. The rank of A is deﬁned to be the dimension
of the image of this linear transformation. Formally, the image of A
is deﬁned to be Im(A) def
= {u ∈ Rm : u = Av for some v ∈ Rn}, and the
rank is deﬁned to be rank(A) def
= dim(Im(A)) and is at most min{m,n}.
We are primarily interested in the case when A is square, i.e., m =
n, and symmetric, i.e., A⊤ = A. Of interest are vectors v such that
Av = λv for some λ. Such a vector is called an eigenvector of A with
respect to (w.r.t.) the eigenvalue λ. It is a basic result in linear algebra
that every real matrix has n eigenvalues, though some of them could
9

10
Basic Linear Algebra
be complex. If A is symmetric, then one can show that the eigenvalues
are real. For a complex number z = a + ib with a,b ∈ R, its conjugate
is deﬁned as ¯z = a − ib. For a vector v, its conjugate transpose v⋆ is
the transpose of the vector whose entries are conjugates of those in v.
Thus, v⋆v = ∥v∥2.
Lemma 1.1. If A is a real symmetric n × n matrix, then all of its
eigenvalues are real.
Proof. Let λ be an eigenvalue of A, possibly complex, and v be the
corresponding eigenvector. Then, Av = λv. Conjugating both sides we
obtain that v⋆A⊤ = λv⋆, where v⋆ is the conjugate transpose of v.
Hence, v⋆Av = λv⋆v, since A is symmetric. Thus, λ∥v∥2 = λ∥v∥2
which implies that λ = λ. Thus, λ ∈ R.
Let λ1 ≤ λ2 ≤ ··· ≤ λn be the n real eigenvalues, or the spectrum, of A
with corresponding eigenvectors u1,...,un. For a symmetric matrix, its
norm is
∥A∥ def
= max{|λ1(A)|,|λn(A)|}.
We now study eigenvectors that correspond to distinct eigenvalues.
Lemma 1.2. Let λi and λj be two eigenvalues of a symmetric matrix
A, and ui, uj be the corresponding eigenvectors. If λi ̸= λj, then
⟨ui,uj⟩ = 0.
Proof. Given Aui = λiui and Auj = λjuj, we have the following
sequence of equalities. Since A is symmetric, u⊤
i A⊤ = u⊤
i A. Thus,
u⊤
i Auj = λiu⊤
i uj on the one hand, and u⊤
i Auj = λju⊤
i uj on the
other. Therefore, λju⊤
i uj = λiu⊤
i uj. This implies that u⊤
i uj = 0 since
λi ̸= λj.
Hence, the eigenvectors corresponding to diﬀerent eigenvalues are
orthogonal. Moreover, if ui and uj correspond to the same eigen-
value λ, and are linearly independent, then any linear combination

1.1 Spectral Decomposition of Symmetric Matrices
11
is also an eigenvector corresponding to the same eigenvalue. The maxi-
mal eigenspace of an eigenvalue is the space spanned by all eigenvectors
corresponding to that eigenvalue. Hence, the above lemma implies that
one can decompose Rn into maximal eigenspaces Ui, each of which cor-
responds to an eigenvalue of A, and the eigenspaces corresponding to
distinct eigenvalues are orthogonal. Thus, if λ1 < λ2 < ··· < λk are the
set of distinct eigenvalues of a real symmetric matrix A, and Ui is the
eigenspace associated with λi, then, from the discussion above,
k
�
i=1
dim(Ui) = n.
Hence, given that we can pick an orthonormal basis for each Ui, we may
assume that the eigenvectors of A form an orthonormal basis for Rn.
Thus, we have the following spectral decomposition theorem.
Theorem 1.3. Let λ1 ≤ ··· ≤ λn be the spectrum of A with corre-
sponding eigenvalues u1,...,un. Then, A = �n
i=1 λiuiu⊤
i .
Proof. Let B def
= �n
i=1 λiuiu⊤
i . Then,
Buj =
n
�
i=1
λiuiu⊤
i uj
= λjuj
= Auj.
The above is true for all j. Since ujs are orthonormal basis of Rn, we
have for all v ∈ Rn, Av = Bv. This implies A = B.
Thus, when A is a real and symmetric matrix, Im(A) is spanned by the
eigenvectors with nonzero eigenvalues. From a computational perspec-
tive, such a decomposition can be computed in time polynomial in the
bits needed to represent the entries of A.1
1 To be very precise, one can only compute eigenvalues and eigenvectors to a high enough
precision in polynomial time. We will ignore this distinction for this monograph as we do
not need to know the exact values.

12
Basic Linear Algebra
1.2
Min–Max Characterizations of Eigenvalues
Now we present a variational characterization of eigenvalues which is
very useful.
Lemma 1.4. If A is an n × n real symmetric matrix, then the largest
eigenvalue of A is
λn(A) =
max
v∈Rn\{0}
v⊤Av
v⊤v .
Proof. Let λ1 ≤ λ2 ≤ ··· ≤ λn
be the eigenvalues of A, and let
u1,u2,...,un be the corresponding orthonormal eigenvectors which
span Rn. Hence, for all v ∈ Rn, there exist c1,...,cn ∈ R such that
v = �
i ciui. Thus,
⟨v,v⟩ =
��
i
ciui,
�
i
ciui
�
=
�
i
c2
i .
Moreover,
v⊤Av =
��
i
ciui
�⊤ 
�
j
λjuju⊤
j


��
k
ckuk
�
=
�
i,j,k
cickλj(u⊤
i uj) · (u⊤
j uk)
=
�
i
c2
i λi
≤ λn
�
i
c2
i = λn ⟨v,v⟩.
Hence, ∀ v ̸= 0, v⊤Av
v⊤v ≤ λn. This implies,
max
v∈Rn\{0}
v⊤Av
v⊤v ≤ λn.

1.2 Min–Max Characterizations of Eigenvalues
13
Now note that setting v = un achieves this maximum. Hence, the
lemma follows.
If one inspects the proof above, one can deduce the following lemma
just as easily.
Lemma 1.5. If A is an n × n real symmetric matrix, then the smallest
eigenvalue of A is
λ1(A) =
min
v∈Rn\{0}
v⊤Av
v⊤v .
More generally, one can extend the proof of the lemma above to the
following. We leave it as a simple exercise.
Theorem 1.6. If A is an n × n real symmetric matrix, then for all
1 ≤ k ≤ n, we have
λk(A) =
min
v∈Rn\{0},v⊤ui=0,∀i∈{1,...,k−1}
v⊤Av
v⊤v ,
and
λk(A) =
max
v∈Rn\{0},v⊤ui=0,∀i∈{k+1,...,n}
v⊤Av
v⊤v .
Notes
Some good texts to review basic linear algebra are [35, 82, 85]. Theo-
rem 1.6 is also called the Courant–Fischer–Weyl min–max principle.

2
The Graph Laplacian
This section introduces the graph Laplacian and connects the second
eigenvalue to the connectivity of the graph.
2.1
The Graph Laplacian and Its Eigenvalues
Consider an undirected graph G = (V,E) with n def
= |V | and m def
= |E|.
We assume that G is unweighted; this assumption is made to simplify
the presentation but the content of this section readily generalizes to
the weighted setting. Two basic matrices associated with G, indexed
by its vertices, are its adjacency matrix A and its degree matrix D.
Let di denote the degree of vertex i.
Ai,j
def
=
�1
if ij ∈ E,
0
otherwise,
and
Di,j
def
=
�di
if i = j,
0
otherwise.
The graph Laplacian of G is deﬁned to be L def
= D − A. We often refer
to this as the Laplacian. In Section 5, we introduce the normalized
14

2.1 The Graph Laplacian and Its Eigenvalues
15
Laplacian which is diﬀerent from L. To investigate the spectral proper-
ties of L, it is useful to ﬁrst deﬁne the n × n matrices Le as follows: For
every e = ij ∈ E, let Le(i,j) = Le(j,i) def
= −1, let Le(i,i) = Le(j,j) = 1,
and let Le(i′,j′) = 0, if i′ /∈ {i,j} or j′ /∈ {i,j}. The following then fol-
lows from the deﬁnition of the Laplacian and Les.
Lemma 2.1. Let L be the Laplacian of a graph G = (V,E). Then,
L = �
e∈E Le.
This can be used to show that the smallest eigenvalue of a Laplacian
is always nonnegative. Such matrices are called positive semideﬁnite,
and are deﬁned as follows.
Deﬁnition 2.1. A symmetric matrix A is called positive semideﬁnite
(PSD) if λ1(A) ≥ 0. A PSD matrix is denoted by A ⪰ 0. Further, A is
said to be positive deﬁnite if λ1(A) > 0, denoted A ≻ 0.
Note that the Laplacian is a PSD matrix.
Lemma 2.2. Let L be the Laplacian of a graph G = (V,E). Then,
L ⪰ 0.
Proof. For any v = (v1,...,vn),
v⊤Lv = v⊤ �
e∈E
Lev
=
�
e∈E
v⊤Lev
=
�
e=ij∈E
(vi − vj)2
≥ 0.
Hence, minv̸=0 v⊤Lv ≥ 0. Thus, appealing to Theorem 1.6, we obtain
that L ⪰ 0.

16
The Graph Laplacian
Is L ≻ 0? The answer to this is no: Let 1 denote the vector with all
coordinates 1. Then, it follows from Lemma 2.1 that L1 = 0 · 1. Hence,
λ1(L) = 0.
For weighted a graph G = (V,E) with edge weights given by a weight
function wG : E �→ R≥0, one can deﬁne
Ai,j
def
=
�wG(ij)
if ij ∈ E
0
otherwise,
and
Di,j
def
=
��
l wG(il)
if i = j
0
otherwise.
Then, the deﬁnition of the Laplacian remains the same, namely, L =
D − A.
2.2
The Second Eigenvalue and Connectivity
What about the second eigenvalue, λ2(L), of the Laplacian? We will
see in later sections on graph partitioning that the second eigenvalue
of the Laplacian is intimately related to the conductance of the graph,
which is a way to measure how well connected the graph is. In this
section, we establish that λ2(L) determines if G is connected or not.
This is the ﬁrst result where we make a formal connection between the
spectrum of the Laplacian and a property of the graph.
Theorem 2.3. Let L be the Laplacian of a graph G = (V,E). Then,
λ2(L) > 0 iﬀ G is connected.
Proof. If G is disconnected, then L has a block diagonal structure. It
suﬃces to consider only two disconnected components. Assume the dis-
connected components of the graph are G1 and G2, and the correspond-
ing vertex sets are V1 and V2. The Laplacian can then be rearranged
as follows:
L =
�L(G1)
0
0
L(G2)
�
,

2.2 The Second Eigenvalue and Connectivity
17
where L(Gi) is a |Vi| × |Vi| matrix for i = 1,2. Consider the vector
x1
def
=
�0|V1|
1|V2|
�
, where 1|Vi| and 0|Vi| denote the all 1 and all 0 vectors of
dimension |Vi|, respectively. This is an eigenvector corresponding to the
smallest eigenvalue, which is zero. Now consider x2
def
=
�1|V1|
0|V2|
�
. Since
⟨x2,x1⟩ = 0, the smallest eigenvalue, which is 0 has multiplicity at least
2. Hence, λ2(L) = 0.
For the other direction, assume that λ2(L) = 0 and that G is con-
nected. Let u2 be the second eigenvector normalized to have length 1.
Then, λ2(L) = u⊤
2 Lu2. Thus, �
e=ij∈E(u2(i) − u2(j))2 = 0. Hence, for
all e = ij ∈ E, u2(i) = u2(j).
Since G is connected, there is a path from vertex 1 to every
vertex j ̸= 1, and for each intermediate edge e = ik, u2(i) = u2(k).
Hence, u2(1) = u2(j),∀j. Hence, u2 = (c,c,...,c). However, we also
know ⟨u2,1⟩ = 0 which implies that c = 0. This contradicts the fact
that u2 is a nonzero eigenvector and establishes the theorem.
Notes
There are several books on spectral graph theory which contain numer-
ous properties of graphs, their Laplacians and their eigenvalues; see
[23, 34, 88]. Due to the connection of the second eigenvalue of the
Laplacian to the connectivity of the graph, it is sometimes called its
algebraic connectivity or its Fiedler value, and is attributed to [28].

3
Laplacian Systems and Solvers
This section introduces Laplacian systems of linear equations and notes
the properties of the near-linear-time Laplacian solver relevant for the
applications presented in this monograph. Sections 5–12 cover several
applications that reduce fundamental graph problems to solving a small
number of Laplacian systems.
3.1
System of Linear Equations
An n × n matrix A and vector b ∈ Rn together deﬁne a system of linear
equations Ax = b, where x = (x1,...,xn) are variables. By deﬁnition,
a solution to this linear system exists if and only if (iﬀ) b lies in the
image of A. A is said to be invertible, i.e., a solution exists for all b, if
its image is Rn, the entire space. In this case, the inverse of A is denoted
by A−1. The inverse of the linear operator A, when b is restricted to
the image of A, is also well deﬁned and is denoted by A+. This is called
the pseudo-inverse of A.
18

3.2 Laplacian Systems
19
3.2
Laplacian Systems
Now consider the case when, in a system of equations Ax = b, A = L
is the graph Laplacian of an undirected graph. Note that this sys-
tem is not invertible unless b ∈ Im(L). It follows from Theorem 2.3
that for a connected graph, Im(L) consists of all vectors orthogonal to
the vector 1. Hence, we can solve the system of equations Lx = b if
⟨b,1⟩ = 0. Such a system will be referred to as a Laplacian system of
linear equations or, in short, a Laplacian system. Hence, we can deﬁne
the pseudo-inverse of the Laplacian as the linear operator which takes
a vector b orthogonal to 1 to its pre-image.
3.3
An Approximate, Linear-Time Laplacian Solver
In this section we summarize the near-linear algorithm known for
solving a Laplacian system Lx = b. This result is the basis of the appli-
cations and a part of the latter half of this monograph is devoted to its
proof.
Theorem 3.1. There is an algorithm LSolve which takes as input
a graph Laplacian L, a vector b, and an error parameter ε > 0, and
returns x satisfying
∥x − L+b∥L ≤ ε∥L+b∥L,
where ∥b∥L
def
=
√
bT Lb. The algorithm runs in �O(mlog 1/ε) time, where
m is the number of nonzero entries in L.
Let us ﬁrst relate the norm in the theorem above to the Euclidean
norm. For two vectors v,w and a symmetric PSD matrix A,
λ1(A)∥v − w∥2 ≤ ∥v − w∥2
A = (v − w)T A(v − w) ≤ λn(A)∥v − w∥2.
In other words,
∥A+∥
1/2∥v − w∥ ≤ ∥v − w∥A ≤ ∥A∥
1/2∥v − w∥.
Hence, the distortion in distances due to the A-norm is at most
�
λn(A)/λ1(A).

20
Laplacian Systems and Solvers
For the graph Laplacian of an unweighted and connected graph,
when all vectors involved are orthogonal to 1, λ2(L) replaces λ1(L).
It can be checked that when G is unweighted, λ2(L) ≥ 1/poly(n) and
λn(L) ≤ Tr(L) = m ≤ n2. When G is weighted the ratio between the
L-norm and the Euclidean norm scales polynomially with the ratio of
the largest to the smallest weight edge. Finally, note that for any two
vectors, ∥v − w∥∞ ≤ ∥v − w∥. Hence, by a choice of ε def
= δ/poly(n) in
Theorem 3.1, we ensure that the approximate vector output by LSolve
is δ close in every coordinate to the actual vector. Note that since the
dependence on the tolerance on the running time of Theorem 3.1 is
logarithmic, the running time of LSolve remains ˜O(mlog 1/ε).
3.4
Linearity of the Laplacian Solver
While the running time of LSolve is ˜O(mlog 1/ε), the algorithm pro-
duces an approximate solution that is oﬀ by a little from the exact
solution. This creates the problem of estimating the error accumula-
tion as one iterates this solver. To get around this, we note an impor-
tant feature of LSolve: On input L, b, and ε, it returns the vector
x = Zb, where Z is an n × n matrix and depends only on L and ε. Z
is a symmetric linear operator satisfying
(1 − ε)Z+ ⪯ L ⪯ (1 + ε)Z+,
(3.1)
and has the same image as L. Note that in applications, such as that in
Section 9, for Z to satisfy ∥Z − L+∥ ≤ ε, the running time is increased
to ˜O(mlog(1/ελ2(L))). Since in most of our applications λ2(L) is at least
1/poly(n), we ignore this distinction.
Notes
A good text to learn about matrices, their norms, and the inequalities
that relate them is [17]. The pseudo-inverse is sometimes also referred to
as the Moore–Penrose pseudo-inverse. Laplacian systems arise in many
areas such as machine learning [15, 56, 89], computer vision [57, 73],
partial diﬀerential equations and interior point methods [24, 30], and
solvers are naturally needed; see also surveys [75] and [84].

3.4 Linearity of the Laplacian Solver
21
We will see several applications that reduce fundamental graph
problems
to
solving
a
small
number
of
Laplacian
systems
in
Sections 5–12. Sections 8 and 9 (a result of which is assumed in
Sections 5 and 6) require the Laplacian solver to be linear. The notable
applications we will not be covering are an algorithm to sample a ran-
dom spanning tree [45] and computing multicommodity ﬂows [46].
Theorem 3.1 was ﬁrst proved in [77] and the full proof appears in a
series of papers [78, 79, 80]. The original running time contained a very
large power of the logn term (hidden in ˜O(·)). This power has since
been reduced in a series of work [8, 9, 49, 62] and, ﬁnally, brought down
to logn in [50]. We provide a proof of Theorem 3.1 in Section 18 along
with its linearity property mentioned in this section. Recently, a simpler
proof of Theorem 3.1 was presented in [47]. This proof does not require
many of the ingredients such as spectral sparsiﬁers (Section 10), pre-
conditioning (Section 17), and conjugate gradient (Section 16). While
we present a sketch of this proof in Section 14, we recommend that the
reader go through the proof in Section 18 and, in the process, familiar-
ize themselves with this wide array of techniques which may be useful
in general.

4
Graphs as Electrical Networks
This section introduces how a graph can be viewed as an electrical
network composed of resistors. It is shown how voltages and currents
can be computed by solving a Laplacian system. The notions of eﬀec-
tive resistance, electrical ﬂows, and their energy are presented. Viewing
graphs as electrical networks will play an important role in applications
presented in Sections 10–12 and in the approach to a simple proof of
Theorem 3.1 presented in Section 14.
4.1
Incidence Matrices and Electrical Networks
Given an undirected, unweighted graph G = (V,E), consider an arbi-
trary orientation of its edges. Let B ∈ {−1,0,1}m×n be the matrix
whose rows are indexed by the edges and columns by the vertices of G
where the entry corresponding to (e,i) is 1 if a vertex i is the tail of the
directed edge corresponding to e, is −1 if i is the head of the directed
edge e, and is zero otherwise. B is called the (edge-vertex) incidence
matrix of G. The Laplacian can now be expressed in terms of B. While
B depends on the choice of the directions to the edges, the Laplacian
does not.
22

4.1 Incidence Matrices and Electrical Networks
23
Lemma 4.1. Let G be a graph with (arbitrarily chosen) incidence
matrix B and Laplacian L. Then, B⊤B = L.
Proof. For the diagonal elements of B⊤B, i.e., (B⊤B)i,i = �
e Bi,eBe,i.
The terms are nonzero only for those edges e which are incident to i, in
which case the product is 1, and hence, this sum gives the degree of ver-
tex i in the undirected graph. For other entries, (B⊤B)i,j = �
e Bi,eBe,j.
The product terms are nonzero only when the edge e is shared by i
and j. In either case the product is −1. Hence, (B⊤B)i,j = −1, ∀i ̸= j,
ij ∈ E. Hence, B⊤B = L.
Now we associate an electrical network to G. Replace each edge
with a resistor of value 1. To make this circuit interesting, we need to
add power sources to its vertices. Suppose cext ∈ Rn is a vector which
indicates how much current is going in at each vertex. This will induce
voltages at each vertex and a current across each edge. We capture
these by vectors v ∈ Rn and i ∈ Rm, respectively. Kirchoﬀ’s law asserts
that, for every vertex, the diﬀerence of the outgoing current and the
incoming current on the edges adjacent to it equals the external current
input at that vertex. Thus,
B⊤i = cext.
On the other hand, Ohm’s law asserts that the current in an edge equals
the voltage diﬀerence divided by the resistance of that edge. Since in
our case all resistances are one, this gives us the following relation.
i = Bv.
Combining these last two equalities with Lemma 4.1 we obtain that
B⊤Bv = Lv = cext.
If ⟨cext,1⟩ = 0, which means there is no current accumulation inside
the electrical network, we can solve for v = L+cext. The voltage vector
is not unique since we can add the same constant to each of its entries
and it still satisﬁes Ohm’s law. The currents across every edge, however,

24
Graphs as Electrical Networks
are unique. Deﬁne vectors ei ∈ Rn for i ∈ V which have a 1 at the i-th
location and zero elsewhere. Then the current through the edge e = ij,
taking into account the sign, is (ei − ej)⊤v = (ei − ej)⊤L+cext. Thus,
computing voltages and currents in such a network is equivalent to
solving a Laplacian system.
4.2
Eﬀective Resistance and the Π Matrix
Now we consider a speciﬁc vector cext and introduce an important
quantity related to each edge, the eﬀective resistance. Consider cext
def
=
ei − ej. The voltage diﬀerence between vertices i and j is then (ei −
ej)⊤L+(ei − ej). When e = ij is an edge, this quantity is called the
eﬀective resistance of e.
Deﬁnition 4.1. Given a graph G = (V,E) with Laplacian L and edge
e = ij ∈ E,
Reﬀ(e) def
= (ei − ej)⊤L+(ei − ej).
Reﬀ(e) is the potential diﬀerence across e when a unit current is
inducted at i and taken out at j.
We can also consider the current through an edge f when a unit
current is inducted and taken out at the endpoints of a possibly dif-
ferent edge e ∈ E. We capture this by the Π matrix which we now
deﬁne formally. Let us denote the rows from the B matrix be and bf
respectively. Then,
Π(f,e) def
= b⊤
f L+be.
In matrix notation,
Π = BL+B⊤.
Note also that Π(e,e) is the eﬀective resistance of the edge e. The matrix
Π has several interesting properties. The ﬁrst is trivial and follows from
the fact the Laplacian and, hence, its pseudo-inverse is symmetric.

4.3 Electrical Flows and Energy
25
Proposition 4.2. Π is symmetric.
Additionally, Π is a projection matrix.
Proposition 4.3. Π2 = Π.
Proof. Π2 = BL+B⊤ · BL+B⊤ = B L+B⊤B
�
��
�
=I
L+B⊤ = BL+B⊤ = Π.
The third equality comes from the fact that the rows of B are orthog-
onal to 1.
Proposition 4.4. The eigenvalues of Π are all either 0 or 1.
Proof. Let v be an eigenvector of Π corresponding to the eigen-
value λ. Hence, Πv = λv ⇒ Π2v = λΠv ⇒ Πv = λ2v ⇒ λv = λ2v ⇒
(λ2 − λ)v = 0 ⇒ λ = 0,1.
Hence, it is an easy exercise to prove that if G is connected, then
rank(Π) = n − 1. In this case, Π has exactly n − 1 eigenvalues which
are 1 and m − n + 1 eigenvalues which are 0.
Finally, we note the following theorem which establishes a connec-
tion between spanning trees and eﬀective resistances. While this theo-
rem is not explicitly used in this monograph, the intuition arising from
it is employed in sparsifying graphs.
Theorem 4.5. Let T be a spanning tree chosen uniformly at random
from all spanning trees in G. Then, the probability that an edge e
belongs to T is given by
P[e ∈ T] = Reﬀ(e) = Π(e,e).
4.3
Electrical Flows and Energy
Given a graph G = (V,E), we ﬁx an orientation of its edges thus
resulting in an incidence matrix B. Moreover, we associate unit

26
Graphs as Electrical Networks
resistance with each edge of the graph. Now suppose that for vertices s
and t one unit of current is injected at vertex s and taken out at t. The
electrical ﬂow on each edge is then captured by the vector
f⋆ def
= BL+(es − et).
In general, an s,t-ﬂow is an assignment of values to directed edges such
that the total incoming ﬂow is the same as the total outgoing ﬂow at
all vertices except s and t. For a ﬂow vector f, the energy it consumes
is deﬁned to be
E(f) def
=
�
e
f2
e .
Thus, the energy of f⋆ is
�
e
(b⊤
e L+(es − et))2 =
�
e
(es − et)⊤L+beb⊤
e L+(es − et).
(4.1)
By taking the summation inside and noting that L = B⊤B = �
e beb⊤
e ,
Equation (4.1) is equivalent to
(es − et)⊤L+LL+(es − et) = (es − et)⊤L+(es − et).
Thus we have proved the following proposition.
Proposition 4.6. If f⋆ is the unit s,t-ﬂow, then its energy is
E(f⋆) = (es − et)⊤L+(es − et).
The following is an important property of electrical s,t-ﬂows and will be
useful in ﬁnding applications related to combinatorial ﬂows in graphs.
Theorem 4.7. Given a graph G = (V,E) with unit resistances across
all edges, if f⋆ def
= BL+(es − et), then f⋆ minimizes the energy con-
sumed E(f) def
= �
e f2
e among all unit ﬂows from s to t.
Proof. Let Π = BL+B⊤ be as before. Proposition 4.3 implies that
Π2 = Π and, hence, for all vectors g, ∥g∥2 ≥ ∥Πg∥2 where equality holds

4.4 Weighted Graphs
27
iﬀ g is in the image of Π. Let f be any ﬂow such that B⊤f = es − et,
i.e., any unit s,t-ﬂow. Then,
E(f) = ∥f∥2 ≥ ∥Πf∥2 = f⊤ΠΠf = f⊤Πf = f⊤BL+Bf.
Hence, using the fact that B⊤f = es − et, one obtains that
E(f) ≥ f⊤BL+Bf = (es − et)⊤L+(es − et)
Prop. 4.6
=
E(f⋆).
Hence, for any unit s,t-ﬂow f, E(f) ≥ E(f⋆).
4.4
Weighted Graphs
Our results also extend to weighted graphs. Suppose the graph
G = (V,E) has weights given by w : E �→ R≥0. Let W be the m × m
diagonal matrix such that W(e,e) = w(e). Then, for an incidence
matrix B given an orientation of G, the Laplacian is L def
= B⊤WB.
The Π matrix in this setting is W
1/2BL+B⊤W
1/2. To set up a cor-
responding electrical network, we associate a resistance re
def
= 1/w(e)
with each edge. Thus, for a given voltage vector v, the current
vector, by Ohm’s Law, is i = WBv. The eﬀective resistance remains
Reﬀ(e) = (ei − ej)⊤L+(ei − ej) where L+ is the pseudo-inverse of the
Laplacian which involves W. For vertices s and t, the unit s,t-ﬂow is
f⋆ def
= WBL+(es − et) and its energy is deﬁned to be �
e re(f⋆
e )2, which
turns out to be (es − et)⊤L+(es − et). It is an easy exercise to check
that all the results in this section hold for this weighted setting.
Notes
The connection between graphs, random walks, and electrical networks
is an important one, and its study has yielded many surprising results.
The books [25] and [54] are good pointers for readers intending to
explore this connection.
While we do not need Theorem 4.5 for this monograph, an interested
reader can try to prove it using the Matrix-Tree Theorem, or refer to
[34]. It forms the basis for another application that uses Laplacian
solvers to develop fast algorithms: Generating a random spanning tree
in time ˜O(mn
1/2), see [45].

Part II
Applications

5
Graph Partitioning I
The Normalized Laplacian
This section introduces the fundamental problem of ﬁnding a cut of
least conductance in a graph, called Sparsest Cut. A quadratic pro-
gram is presented which captures the Sparsest Cut problem exactly.
Subsequently, a relaxation of this program is considered where the
optimal value is essentially the second eigenvalue of the normalized
Laplacian; this provides a lower bound on the conductance of the graph.
In Sections 6 and 7 this connection is used to come up with approxima-
tion algorithms for the Sparsest Cut and related Balanced Edge-
Separator problems. Finally, in Section 8, it is shown how Laplacian
solvers can be used to compute the second eigenvector and the associ-
ated second eigenvector in ˜O(m) time.
5.1
Graph Conductance
Given an undirected, unweighted graph G = (V,E) with n vertices and
m edges, we are interested in vertex cuts in the graph. A vertex cut
is a partition of V into two parts, S ⊆ V and ¯S def
= V \S, which we
denote by (S, ¯S). Before we go on to deﬁne conductance, we need a
way to measure the size of a cut given by S ⊆ V. One measure is its
29

30
Graph Partitioning I The Normalized Laplacian
cardinality |S|. Another is the sum of degrees of all the vertices in S.
If the graph is regular, i.e., all vertices have the same degree, then
these two are the same up to a factor of this ﬁxed degree. Otherwise,
they are diﬀerent and a part of the latter is called the volume of the
set. Formally, for S ⊆ V, the volume of S is vol(S) def
= �
i∈S di, where
di is the degree of vertex i. By a slight abuse of notation, we deﬁne
vol(G) def
= vol(V ) = �
i∈V di = 2m. The number of edges that cross the
cut (S, ¯S), i.e., have one end point in S and the other in ¯S, is denoted
|E(S, ¯S)|. Now we deﬁne the conductance of a cut.
Deﬁnition 5.1. The conductance of a cut (S, ¯S) (also referred to as
its normalized cut value or cut ratio) is deﬁned to be
φ(S) def
=
|E(S, ¯S)|
min{vol(S),vol( ¯S)}.
The conductance of a cut measures the ratio of edges going out of a cut
to the total edges incident to the smaller side of the cut. This is always
a number between 0 and 1. Some authors deﬁne the conductance to be
|E(S, ¯S)|
min{|S|,| ¯S|},
where |S| denotes the cardinality of S. The former deﬁnition is preferred
to the latter one as φ(S) lies between 0 and 1 in case of the former
while there is no such bound on φ(S) in the latter. Speciﬁcally, the
latter is not a dimension-less quantity: if we replace each edge by k
copies of itself, this value will change, while the value given by the
former deﬁnition remains invariant. The graph conductance problem is
to compute the conductance of the graph which is deﬁned to be
φ(G) def
=
min
∅̸=S⊊V φ(S).
This problem is often referred to as the Sparsest Cut problem and
is NP-hard. This, and its cousin, the Balanced Edge-Separator
problem (to be introduced in Section 7) are intensely studied, both in
theory and practice, and have far reaching connections to spectral graph
theory, the study of random walks, and metric embeddings. Besides

5.2 A Mathematical Program
31
being theoretically rich, they are of great practical importance as they
play a central role in the design of recursive algorithms, image segmen-
tation, community detection, and clustering.
Another quantity which is closely related to the conductance and is
often easier to manipulate is the following.
Deﬁnition 5.2. For a cut (S, ¯S), the h-value of a set is deﬁned to be
h(S) def
=
|E(S, ¯S)|
vol(S) · vol( ¯S) · vol(G)
and the h-value of the graph is deﬁned to be
h(G) def
=
min
∅̸=S⊊V h(S).
We ﬁrst observe the relation between h and φ.
Lemma 5.1. For all S, φ(S) ≤ h(S) ≤ 2φ(S).
Proof. This follows from the observations that for any cut (S, ¯S),
vol(G) ≥ max{vol(S),vol( ¯S)} ≥ vol(G)/2
and
max{vol(S),vol( ¯S)} · min
�
vol(S),vol( ¯S)
�
= vol(S) · vol( ¯S).
Hence, φ(G) ≤ h(G) ≤ 2φ(G).
Thus, the h-value captures the conductance of a graph up to a factor
of 2. Computing the h-value of a graph is not any easier than comput-
ing its conductance. However, as we see next, it can be formulated as
a mathematical program which can then be relaxed to an eigenvalue
problem involving the Laplacian.
5.2
A Mathematical Program
We will write down a mathematical program which captures the h-value
of the conductance. First, we introduce some notation which will be
useful.

32
Graph Partitioning I The Normalized Laplacian
5.2.1
Probability Measures on Vertices and
Cuts as Vectors
Let ν : E → [0,1] be the uniform probability measure deﬁned on the set
of edges E,
ν(e) def
= 1
m.
For a subset of edges F ⊆ E, ν(F) def
= �
e∈F ν(e). Next, we consider a
measure on the vertices induced by the degrees. For i ∈ V,
µ(i) def
=
di
vol(G) = di
2m.
Note that µ is a probability measure on V. We extend µ to subsets
S ⊆ V ,
µ(S) def
=
�
i∈S
µ(i) =
�
i∈S
di
vol(G) = vol(S)
vol(G).
With these deﬁnitions it follows that
φ(S) =
ν(E(S, ¯S))
2min{µ(S),µ( ¯S)}
and
h(S) = ν(E(S, ¯S))
2µ(S)µ( ¯S).
Given S ⊆ V , let 1S : V �→ {0,1} denote the indicator function for the
set S by
1S(i) def
=
�1
if i ∈ S
0
otherwise.
Then,
(1S(i) − 1S(j))2 =
�1
if (i ∈ S and j ∈ ¯S) or (i ∈ ¯S and j ∈ S),
0
if (i ∈ S and j ∈ S) or (i ∈ ¯S and j ∈ ¯S).
Therefore,
E
ij←ν[(1S(i) − 1S(j))2] = |E(S, ¯S)|
m
= ν(E(S, ¯S)).

5.2 A Mathematical Program
33
Moreover, for any S
E
(i,j)←µ×µ[(1S(i) − 1S(j))2]
=
P
(i,j)←µ×µ[(1S(i) − 1S(j))2 = 1]
=
P
(i,j)←µ×µ[i ∈ S,j ∈ ¯S or i ∈ ¯S,j ∈ S]
= P
i←µ[i ∈ S] P
j←µ[j ∈ ¯S] + P
i←µ[i ∈ ¯S] P
j←µ[j ∈ S]
(since i and j are chosen independently)
= µ(S)µ( ¯S) + µ( ¯S)µ(S) = 2µ(S)µ( ¯S).
Therefore,
h(S) = ν(E(S, ¯S))
2µ(S)µ( ¯S) =
Eij←ν[(1S(i) − 1S(j))2]
E(i,j)←µ×µ[(1S(i) − 1S(j))2].
Hence, noting the one-to-one correspondence between sets S ⊆ V and
functions x ∈ {0,1}n, we have proved the following mathematical pro-
gramming characterization of the conductance.
Lemma 5.2. Consider the h-value of a graph G and the probability
distributions ν and µ as above. Then, for x ̸= 0,1,
h(G) =
min
x∈{0,1}n
Eij←ν[(xi − xj)2]
Ei←µ Ej←µ[(xi − xj)2].
As noted before, this quantity in the right-hand side (r.h.s.) of the
lemma above is hard to compute. Let us now try to relax the condition
that x ∈ {0,1}n to x ∈ Rn. We will refer to this as the real conductance;
this notation, as we will see shortly, will go away.
Deﬁnition 5.3. The real conductance of a graph G is
hR(G) def
= min
x∈Rn
Eij←ν[(xi − xj)2]
E(i,j)←µ×µ[(xi − xj)2].
Since we are relaxing from a 0/1 embedding of the vertices to a real
embedding, it immediately follows from the deﬁnition that hR(G) is at

34
Graph Partitioning I The Normalized Laplacian
most h(G). The optimal solution of the optimization problem above
provides a real embedding of the graph. Two questions need to be
addressed: Is hR(G) eﬃciently computable? How small can hR(G) get
when compared to h(G); in other words, how good an approximation
is hR(G) to h(G)? In the remainder of the section we address the ﬁrst
problem. We show that computing hR(G) reduces to computing an
eigenvalue of a matrix, in fact closely related to the graph Laplacian.
In the next section we lower bound hR(G) by a function in h(G) and
present an algorithm that ﬁnds a reasonable cut using hR.
5.3
The Normalized Laplacian and Its Second Eigenvalue
Recall that hR(G) def
= minx∈Rn
Eij←ν[(xi−xj)2]
E(i,j)←µ×µ[(xi−xj)2]. Note that the r.h.s.
above remains unchanged if we add or subtract the same quantity
to every xi. One can thereby subtract Ei←µ[xi] from every xi, and
assume that we optimize with an additional constraint on x, namely,
Ei←µ[xi] = 0. This condition can be written as ⟨x,D1⟩ = 0. First note
the following simple series of equalities for any x ∈ Rn based on simple
properties of expectations.
E
(i,j)←µ×µ[(xi − xj)2] =
E
(i,j)←µ×µ[xi2 + xj2 − 2xixj]
=
E
(i,j)←µ×µ[xi2 + x2
j] − 2
E
(i,j)←µ×µ[xixj]
= E
i←µ[xi2 + x2
j] − 2 E
i←µ[xi] E
j←µ[xj]
= 2 E
i←µ[xi2] − 2( E
i←µ[xi])2.
Therefore, by our assumption, E(i,j)←µ×µ[(xi − xj)2] = 2Ei←µ[xi2].
Hence, we obtain
E
(i,j)←µ×µ[(xi − xj)2] = 2 E
i←µ[x2
i ]
= 2�
i∈V dixi2
vol(G)
= 2x⊤Dx
vol(G) .

5.3 The Normalized Laplacian and Its Second Eigenvalue
35
Moreover, from the deﬁnition of L it follows that
E
ij←ν
�
(xi − xj)2�
=
�
e=ij∈E(xi − xj)2
m
= x⊤Lx
m
= 2x⊤Lx
vol(G) .
Therefore, we can write
hR(G) =
min
x∈Rn, ⟨x,D1⟩=0
x⊤Lx
x⊤Dx.
This is not quite an eigenvalue problem. We will now reduce it to one.
Substitute y def
= D
1/2x in the equation above. Then,
hR(G) =
min
y∈Rn, ⟨D−1/2y,D1⟩=0
(D−1/2y)⊤L(D−1/2y)
(D−1/2y)⊤D(D−1/2y)
=
min
y∈Rn, ⟨y,D1/21⟩=0
y⊤D−1/2LD−1/2y
y⊤y
.
This is an eigenvalue problem for the following matrix which we refer
to as the normalized Laplacian.
Deﬁnition 5.4. The normalized Laplacian of a graph G is deﬁned
to be
L def
= D−1/2LD−1/2,
where D is the degree matrix and L is the graph Laplacian as in
Section 2.
Note that L is symmetric and, hence, has a set of orthonormal eigenvec-
tors which we denote D
1/21 = u1,...,un with eigenvalues 0 = λ1(L) ≤
··· ≤ λn(L). Hence,
hR(G) =
min
y∈Rn,⟨y,D1/21⟩=0
y⊤Ly
y⊤y = λ2(L).
We summarize what we have proved in the following theorem.

36
Graph Partitioning I The Normalized Laplacian
Theorem 5.3. λ2(L) = hR(G).
We also obtain the following corollary.
Corollary 5.4. λ2(L) ≤ 2φ(G).
Thus, we have a handle on the quantity λ2(L), which we can compute
eﬃciently. Can we use this information to recover a cut in G of conduc-
tance close to λ2(L)? It turns out that the eigenvector corresponding
to λ2(L) can be used to ﬁnd a cut of small conductance, which results
in an approximation algorithm for Sparsest Cut. This is the content
of the next section.
Notes
For a good, though out-dated, survey on the graph partitioning problem
and its applications see [74]. There has been a lot of activity on and
around this problem in the last decade. In fact, an important compo-
nent of the original proof of Theorem 3.1 by Spielman and Teng relied
on being able to partition graphs in near-linear-time, see [79]. The proof
of NP-hardness of Sparsest Cut can be found in [32]. Theorem 5.3
and Corollary 5.4 are folklore and more on them can be found in the
book [23].

6
Graph Partitioning II
A Spectral Algorithm for Conductance
This section shows that the conductance of a graph can be roughly
upper bounded by the square root of the second eigenvalue of the nor-
malized Laplacian. The proof that is presented implies an algorithm to
ﬁnd such a cut from the second eigenvector.
6.1
Sparse Cuts from ℓ1 Embeddings
Recall the ℓ2
2 problem which arose from the relaxation of h(G) and
Theorem 5.31
λ2(L) = min
x∈Rn
Eij←ν[(xi − xj)2]
E(i,j)←µ×µ[(xi − xj)2] ≤ h(G) ≤ 2φ(G).
We will relate the mathematical program that captures graph conduc-
tance with an ℓ1-minimization program (Theorem 6.1) and then relate
that ℓ1-minimization to the ℓ2
2 program (Theorem 6.3). In particular,
1 Here by ℓ2
2 we mean that it is an optimization problem where both the numerator and
the denominator are squared-Euclidean distances. This is not to be confused with an ℓ2
2
metric space.
37

38
Graph Partitioning II A Spectral Algorithm for Conductance
we will show that
φ(G) =
min
y∈Rn
µ1/2(y)=0
Eij←ν[|yi − yj|]
Ei←µ[|yi|]
≤ 2
�
λ2(L).
Here, µ1/2(y) is deﬁned to be a t such that µ({i : yi < t}) ≤ 1/2 and
µ({i : yi > t}) ≤ 1/2. Note that µ1/2(·) is not uniquely deﬁned.
Theorem 6.1. For any graph G on n vertices, the graph conductance
φ(G) =
min
y∈Rn,µ1/2(y)=0
Eij←ν[|yi − yj|]
Ei←µ[|yi|]
.
Proof. Let (S, ¯S) be such that φ(S) = φ(G). Without loss of general-
ity (W.l.o.g.) assume that µ(S) ≤ µ( ¯S). Then Eij←ν[|1S(i) − 1S(j)|] =
ν(E(S, ¯S))
and
Ei←µ[|1S(i)|] = µ(S) = min{µ(S),µ( ¯S)}.
Since
{i :
1S(i) ≤ t} is ∅ for t < 0 and ¯S for t = 0, we have µ1/2(1S) = 0. Combin-
ing, we obtain
min
y∈Rn,µ1/2(y)=0
Eij←ν[|yi − yj|]
Ei←µ[|yi|]
≤ Eij←ν[|1S(i) − 1S(j)|]
Ei←µ[|1S(i)|]
= φ(G).
It remains to show that Eij←ν[|yi−yj|]
Ei←µ[|yi|]
≥ φ(G) for every y ∈ Rn such that
µ1/2(y) = 0. Fix an arbitrary y ∈ R with µ1/2(y) = 0. For convenience
we assume that all entries of y are distinct. Re-index the vertices of G
such that y1 < y2 < ··· < yn. This gives an embedding of the vertices
of G on R. The natural cuts on this embedding, called sweep cuts, are
given by Si
def
= {1,...,i},1 ≤ i ≤ n − 1. Let �S be the sweep cut with
minimum conductance. We show that the conductance φ(�S) is a lower
bound on Eij←ν[|yi−yj|]
Ei←µ[|yi|]
. This completes the proof since φ(�S) itself is
bounded below by φ(G). First, note that
Eij←ν[|yi − yj|] = 1
m
�
ij∈E
|yi − yj|
= 1
m
�
ij∈E
max{i,j}−1
�
l=min{i,j}
(yl+1 − yl)
=
n−1
�
l=1
ν(E(Sl, ¯Sl))(yl+1 − yl).
(6.1)

6.1 Sparse Cuts from ℓ1 Embeddings
39
The third equality above follows since, in the double summation, the
term (yl+1 − yl) is counted once for every edge that crosses (Sl, ¯Sl).
For sake of notational convenience we assign S0
def
= ∅ and Sn
def
= V as
two limiting sweep cuts so that {i} = Si − Si−1 = ¯Si−1 − ¯Si for every
i ∈ [n]. Hence µi = µ(Si) − µ(Si−1) = µ( ¯Si−1) − µ( ¯Si). We use this to
express Ei←µ[|yi|] below. Assume that there is a k such that yk = 0.
Thus, since µ1/2(y) = 0, µ( ¯Sk) ≤ µ(Sk). If no such k exists, the proof is
only simpler.
Ei←µ[|yi|] =
n
�
i=1
µi|yi| =
k
�
i=1
µi(−yi) +
n
�
i=k+1
µi(yi)
=
k
�
i=1
(µ(Si) − µ(Si−1))(−yi) +
n
�
i=k+1
(µ( ¯Si−1) − µ( ¯Si))(yi)
= µ(S0)y1 +
k−1
�
i=1
µ(Si)(yi+1 − yi) + µ(Sk)(−yk) +
µ( ¯Sk)yk+1 +
n−1
�
i=k+1
µ( ¯Si)(yi+1 − yi) + µ( ¯Sn)(−yn)
=
k−1
�
i=1
µ(Si)(yi+1 − yi) +
n−1
�
i=k
µ( ¯Si)(yi+1 − yi)
(since µ(S0) = µ( ¯Sn) = 0, yk = 0 and µ( ¯Sk) ≤ µ(Sk))
=
n−1
�
i=1
min{µ(Si),µ( ¯Si)}(yi+1 − yi).
(6.2)
Where the last equality follows since µ( ¯Si) ≥ µ(Si) for all i ≤ k and
µ( ¯Si) ≤ µ(Si) for all i ≥ k. Putting Equations (6.1) and (6.2) together,
we get the desired inequality:
Eij←ν[|yi − yj|]
Ei←µ[|yi|]
=
�n−1
�n−1i=1 ν(E(Si, ¯Si))(yi+1 − yi)
i=1 min{µ(Si),µ( ¯Si)}(yi+1 − yi)

40
Graph Partitioning II A Spectral Algorithm for Conductance
Prop. 6.2
≥
min
i∈[n−1]
�
ν(E(Si, ¯Si)
min{µ(Si),µ( ¯Si)}
�
=
min
i∈[n−1]φ(Si)
=
φ(�S).
In the proof we have used the following simple proposition.
Proposition 6.2. For b1,...,bn ≥ 0,
a1 + ··· + an
b1 + ··· + bn
≥
n
min
i=1
ai
bi
.
Proof. Let mini ai/bi = θ. Then, since bi ≥ 0, for all i, ai ≥ θ · bi. Hence,
�
i ai ≥ θ · �
i bi. Thus,
�
i ai/�
i bi ≥ θ.
6.2
An ℓ1 Embedding from an ℓ2
2 Embedding
The next theorem permits us to go from an ℓ2
2 solution to an ℓ1 solution
which satisﬁes the conditions of Theorem 6.1 with a quadratic loss in
the objective value.
Theorem 6.3. If there is an x ∈ Rn such that
Eij←ν[(xi−xj)2]
E(i,j)←µ×µ[(xi−xj)2] = ε,
then there exists a y ∈ Rn with µ1/2(y) = 0 such that Eij←ν[|yi−yj|]
Ei←µ[|yi|]
≤
2√ε.
Before we prove this theorem, we will prove a couple simple propositions
which are used in its proof. For y ∈ R, let sgn(y) = 1 if y ≥ 0, and −1
otherwise.
Proposition 6.4. For all y ≥ z ∈ R,
��sgn(y) · y2 − sgn(z) · z2�� ≤ (y − z)(|y| + |z|).

6.2 An ℓ1 Embedding from an ℓ2
2 Embedding
41
Proof.
(1) If sgn(y) = sgn(z), then |sgn(y) · y2 − sgn(z) · z2| = |y2 −
z2| = (y − z) · |y + z| = (y − z)(|y| + |z|) as y ≥ z.
(2) If sgn(y) ̸= sgn(y), then y ≥ z, (y − z) = |y| + |z|. Hence,
|sgn(y) · y2 − sgn(z) · z2| = y2 + z2 ≤ (|y| + |z|)2 = (y − z)
(|y| + |z|).
Proposition 6.5. For a,b ∈ R, (a + b)2 ≤ 2(a2 + b2).
Proof. Observe that a2 + b2 − 2ab ≥ 0. Hence, 2a2 + 2b2 − 2ab ≥ a2 +
b2. Hence,
2a2 + 2b2 ≥ a2 + 2ab + b2 = (a + b)2.
Proof. [of Theorem 6.3] Since the left-hand side (l.h.s.) of the hypothe-
sis is shift invariant for x, we can assume w.l.o.g. that µ1/2(x) = 0. Let
y = (y1,...,yn) be deﬁned such that
yi
def
= sgn(xi)x2
i .
Hence, µ1/2(y) = 0. Further,
Eij←ν[|yi − yj|]
Prop. 6.4
≤
Eij←ν[|xi − xj|(|xi| + |xj|)]
Cauchy-Schwarz
≤
�
Eij←ν[|xi − xj|2]
�
Eij←ν[(|xi| + |xj|)2]
Prop. 6.5
≤
�
Eij←ν[(xi − xj)2]
�
2Eij←ν[xi2 + xj2]
double counting
=
�
Eij←ν[(xi − xj)2]
�
2Ei←µ[x2
i ]
=
�
Eij←ν[(xi − xj)2]
�
2Ei←µ[|yi|].
Hence,
Eij←ν[|yi − yj|]
Ei←µ[|yi|]
≤
�
2Eij←ν[(xi − xj)2]
Ei←µ[|yi|]

42
Graph Partitioning II A Spectral Algorithm for Conductance
=
�
2Eij←ν[(xi − xj)2]
Ei←µ[x2
i ]
≤
�
4Eij←ν[(xi − xj)2]
E(i,j)←µ×µ[(xi − xj)2]
(since E(i,j)←µ×µ[(xi − xj)2] ≤ 2Ei←µ[x2
i ])
= 2√ε.
As a corollary we obtain the following theorem known as Cheeger’s
inequality.
Theorem 6.6. For any graph G,
λ2(L)
2
≤ φ(G) ≤ 2
�
2λ2(L).
Proof. The ﬁrst inequality was proved in the last section, see Corol-
lary 5.4. The second inequality follows from Theorems 6.1 and 6.3 by
choosing x to be the vector that minimizes
Eij←ν[(xi−xj)2]
E(i,j)←µ×µ[(xi−xj)2].
It is an easy exercise to show that Cheeger’s inequality is tight up to
constant factors for the cycle on n vertices.
Algorithm 6.1 SpectralCut
Input: G(V,E), an undirected graph with V = {1,...,n}
Output: S, a subset of V with conductance φ(S) ≤ 2
�
φ(G)
1: D ← degree matrix of G
2: L ← Laplacian of G
3: L ← D
−1/2LD
−1/2
4: y⋆ ← argminy∈Rn:⟨y,D1/21⟩=0
y⊤Ly
y⊤y
5: x⋆ ← D
−1/2y⋆
6: Re-index V so that x⋆
1 ≤ x⋆
2 ≤ ··· ≤ x⋆
n
7: Si ← {1,...,i} for i = 1,2,...,n − 1
8: Return Si that has minimum conductance from among 1,...,n − 1

6.2 An ℓ1 Embedding from an ℓ2
2 Embedding
43
The spectral algorithm for Sparsest Cut is given in Algorithm 6.1.
Note that x⋆ = argminx∈Rn
Eij←ν[(xi−xj)2]
E(i,j)←µ×µ[(xi−xj)2] is computed by solving
an eigenvector problem on the normalized Laplacian L. The proof of
correctness follows from Theorems 5.3 and 6.6.
Notes
Theorem 6.6 is attributed to [7, 20]. There are several ways to prove
Theorem 6.3 and the proof presented here is inﬂuenced by the work
of [58].
In terms of approximation algorithms for Sparsest Cut, the best
result is due to [13] who gave an O(√logn)-factor approximation algo-
rithm. Building on a sequence of work [10, 12, 48, 61], Sherman [71]
showed how to obtain this approximation ratio of √logn in essentially
single-commodity ﬂow time. Now there are improved algorithms for
this ﬂow problem, see Section 12, resulting in an algorithm that runs
in time ˜O(m
4/3). Obtaining a √logn approximation in ˜O(m) time, pos-
sibly bypassing the reduction to single-commodity ﬂows, remains open.
Toward this, getting a logn factor approximation in ˜O(m) seems like
a challenging problem. Finally, it should be noted that Madry [55]
gave an algorithm that, for every integer k ≥ 1, achieves, roughly, an
O((logn)(k+1/2)) approximation in ˜O(m + 2k · n1+2−k) time.

7
Graph Partitioning III
Balanced Cuts
This section builds up on Section 6 to introduce the problem of ﬁnding
sparse cuts that are also balanced. Here, a balance parameter b ∈ (0, 1/2]
is given and the goal is to ﬁnd the cut of least conductance among cuts
whose smaller side has a fractional volume of at least b. We show how
one can recursively apply Algorithm 6.1 to solve this problem.
7.1
The Balanced Edge-Separator Problem
In many applications, we not only want to ﬁnd the cut that minimizes
conductance, but would also like the two sides of the cut to be
comparable in volume. Formally, given a graph G and a balance
parameter b ∈ (0, 1/2], the goal is to ﬁnd a cut (S, ¯S) with minimum
conductance such that min{µ(S),µ( ¯S)} ≥ b. This problem, referred
to as Balanced Edge-Separator, is also NP-hard and we present
an approximation algorithm for it in this section. In fact, we recur-
sively use the algorithm SpectralCut presented in the previous
section to give a pseudo-approximation algorithm to this problem:
We present an algorithm (BalancedCut, see Algorithm 7.1) that
accepts an undirected graph G, a balance requirement b ∈ (0, 1/2], and
a conductance requirement γ ∈ (0,1) as input. If the graph contains
44

7.1 The Balanced Edge-Separator Problem
45
Algorithm 7.1 BalancedCut
Input: G(V,E), an undirected graph with V = {1,...,n}. A balance
parameter b ∈ (0, 1/2]. A target conductance parameter γ ∈ (0,1).
Output: Either a b/2-balanced cut (S, ¯S) with φ(S) ≤ 4√γ or certify
that every b-balanced cut in G has conductance at least γ.
1: µG ← degree measure of G
2: H ← G
3: S′ ← ∅
4: repeat
5:
LH ← normalized Laplacian of H
6:
if λ2(LH) > 4γ then
7:
return NO
8:
end if
9:
S ← SpectralCut(H)
10:
S′ ← argmin{µG(S),µG(V (H) \ S)} ∪ S′
11:
if min{µG(S′),µG(V (G) \ S′)} ≥ b/2 then
12:
return (S′, ¯S′) and (S, ¯S)
// At least
one of the cuts will be shown to satisfy the output requirement
of being b/2 balanced and conductance at most 4√γ.
13:
else
// Restrict the problem to the induced subgraph of
H on V \ S with every edge that crosses V \ S to S replaced by
a self-loop at the V \ S end
14:
V ′ ← V (H) \ S
15:
E′ ← {ij : i,j ∈ V ′} ∪ {ii : ij ∈ E(V ′,S)} // Note that E′ is a
multiset
16:
H = (V ′,E′)
17:
end if
18: until
a b-balanced cut of conductance at most γ, then BalancedCut will
return a b/2-balanced cut of conductance O(√γ), otherwise it will
certify that every b-balanced cut in G has conductance at least γ. This
b/2 can be improved to (1 − ε)b for any ε > 0. Assuming this, even
if the algorithm outputs a (1 − ε)b balanced cut, the cut with least
conductance with this volume could be signiﬁcantly smaller than γ. In
this sense, the algorithm is a pseudo-approximation.

46
Graph Partitioning III Balanced Cuts
7.2
The Algorithm and Its Analysis
The following is the main theorem of this section.
Theorem 7.1. Given an undirected graph G, a balance parameter
b ∈ (0, 1/2] and a target conductance γ ∈ (0,1), BalancedCut either
ﬁnds a b/2-balanced cut in G of conductance at most O(√γ), or certiﬁes
that every b-balanced cut in G has conductance more than γ.
The algorithm BalancedCut, which appears below, starts by check-
ing if the second smallest eigenvalue of the normalized Laplacian of G,
L is at most O(γ), the target conductance. If not, then it outputs NO.
On the other hand, if λ2(L) ≤ O(γ), it makes a call to SpectralCut
which returns a cut (S0, ¯S0) of conductance at most O(√γ) by Theo-
rem 6.3. Assume that µ(S0) ≤ µ( ¯S0). If (S0, ¯S0) is already b/2-balanced,
we return (S0, ¯S0). Otherwise, we construct a smaller graph G1 by
removing S0 from G and replacing each edge that had crossed the cut
with a self-loop at its endpoint in ¯S0. We always remove the smaller
side of the cut output by SpectralCut from the current graph. This
ensures that the total number of edges incident on any subset of ver-
tices of G1 is the same as it was in G, i.e., the volume of a subset does
not change in subsequent iterations.
BalancedCut recurses on G1 and does so until the relative volume
of the union of all the deleted vertices exceeds b/2 for the ﬁrst time. In
this case, we are able to recover a b/2-balanced cut of conductance at
most O(√γ). It is obvious that BalancedCut either outputs an NO or
stops with an output of pair of cuts (S, ¯S) and (S′, ¯S′). Since the graph
is changing in every iteration, in the description of the algorithm and
the analysis we keep track of the degree measure µ and the normalized
Laplacian L by subscripting it with the graph involved. The following
two lemmata imply Theorem 7.1.
Lemma 7.2. If BalancedCut deletes a set of degree measure less
than b/2 before it returns an NO, then every b-balanced cut in G has
conductance at least γ.

7.2 The Algorithm and Its Analysis
47
Proof. When BalancedCut stops by returning an NO, we know that
(1) λ2(LH) > 4γ, and
(2) vol(V (H)) ≥ (1 − b/2)vol(G).
Suppose for sake of contradiction that there is a b-balanced cut in G
of conductance at most γ. Then, such a cut when restricted to H has
relative volume at least b/2, and the number of edges going out of it
in H is at most those going out in G. Thus, since the volume has
shrunk by a factor of at most 2 and the edges have not increased, the
conductance of the cut in H is at most 2 times that in G. Hence, such
a cut will have conductance at most 2γ in H. But λ2(LH) > 4γ and
hence, from the easy side of Cheeger’s inequality (Corollary 5.4), we can
infer that φ(H), even with degrees from G, is at least λ2(LH)/2 > 2γ.
Thus, H does not have a cut of conductance at most 2γ.
Lemma 7.3. When BalancedCut terminates by returning a pair of
cuts, one of the two cuts it outputs has a relative volume at least b/2
and conductance at most 4√γ.
Proof. There are two cases at the time of termination: If (S′, ¯S′) and
(S, ¯S) are returned, either
(1) vol(S′)/vol(G) ≥ 1/2, or
(2) b/2 ≤ vol(S′)/vol(G) ≤ 1/2.
In the ﬁrst case, since we stopped the ﬁrst time the relative volume
exceeded b/2 (and in fact went above 1/2), it must be the case that
output (S, ¯S) of SpectralCut in that iteration is b/2-balanced. The
conductance of this cut in H is at most 2√4γ = 4√γ by the guarantee
of SpectralCut. Hence, the lemma is satisﬁed.
In the second case, we have that (S′, ¯S′) is b/2-balanced. What about
its conductance? S′, the smaller side, consists of the union of disjoint
cuts S0,S1,...,St for some t and each is of conductance at most 4√γ in
the respective Gi by the guarantee of SpectralCut. We argue that
the conductance of S′ is also at most 4√γ in G. First note that
|EG(S′, ¯S′)| ≤ |EG(S0, ¯S0)| + |EG1(S1, ¯S1)| + ··· + |EGt(St, ¯St)|.

48
Graph Partitioning III Balanced Cuts
This is because every edge contributing to the l.h.s. also contributes
to the r.h.s. The inequality occurs because there could be more, e.g.,
if there is an edge going between S1 and S2. Now, the conductance of
each Si in Gi is at most 4√γ. Hence,
|EG(S′, ¯S′)| ≤ 4√γ · volG(S0)+4√γ · volG1(S1)+ ··· +4√γ · volGt(St).
Finally, note that volGi(Si) = volG(Si) for all i due to the way in which
we split the graph but retain the edges adjacent to the removed edges
as self-loops. Hence, volG(S′) = �t
i=0 volGi(Si). Thus,
|EG(S′, ¯S′)|
volG(S′)
≤ 4√γ.
This concludes the proof of Theorem 7.1.
Notes
Theorem 7.1 is folklore and appears implicitly in [79] and [13]. There
are other nearly-linear time algorithms, where we allow the running
time to depend on 1/γ for the BalancedEdge-Separator problem
which achieve a γ versus √γ polylog n bound, see for instance [8, 9, 79].
These algorithms are local in the sense that they instead bound the
total work by showing that each iteration runs in time proportional to
the smaller side of the cut. This approach was pioneered in [79] which
relied on mixing time results in [53]. The ﬁrst near-linear algorithm
for BalancedEdge-Separator that removes polylog n factors in the
approximation was obtained by [62]. Recently, this dependence of the
running time on γ has also removed and an ˜O(m) time algorithm, which
achieves a γ versus √γ bound, has been obtained by Orecchia et al. [60].
Their proof relies crucially on Laplacian solvers and highlights of their
algorithm are presented in Sections 9 and 19.

8
Graph Partitioning IV
Computing the Second Eigenvector
This ﬁnal section on graph partitioning addresses the problem of how
to compute an approximation to the second eigenvector, using the near-
linear-time Laplacian solver, a primitive that was needed in Sections 6
and 7.
8.1
The Power Method
Before we show how to compute the second smallest eigenvector of the
normalized Laplacian of a graph, we present the power method. This
is a well-known method to compute an approximation to the largest
eigenvector of a matrix A. It start with a random unit vector and
repeatedly applies A to it while normalizing it at every step.
Lemma 8.1. Given a symmetric matrix A ∈ Rn×n, an error parameter
ε > 0 and a positive integer k > 1/2εlog(9n/4), the following holds with
probability at least 1/2 over a vector v chosen uniformly at random
from the n-dimensional unit sphere,
∥Ak+1v∥
∥Akv∥
≥ (1 − ε)|λn(A)|,
where λn(A) is the eigenvalue with the largest magnitude of A.
49

50
Graph Partitioning IV Computing the Second Eigenvector
Proof. Let u1,...,un be the eigenvectors of A corresponding to the
eigenvalues |λ1| ≤ ··· ≤ |λn|. We can write the random unit vector v in
the basis {ui}i∈[n] as �n
i=1 αiui. From a standard calculation involving
Gaussian distributions we deduce that, with probability at least 1/2,
|αn| ≥
2
3√n. Using H¨older’s inequality,1
∥Akv∥2 =
�
i
α2
i λ2k
i
≤
��
i
α2
i λ2k+2
i
�k/k+1 ��
i
α2
i
�1/k+1
=
��
i
α2
i λ2k+2
i
�k/k+1
= ∥Ak+1v∥
2k/k+1.
Note that ∥Ak+1v∥
2/k+1 ≥ α
n2/k+1
λ2
n. Thus, it follows that
∥Akv∥2 ≤ ∥Ak+1v∥
2k/k+1 · ∥Ak+1v∥
2/k+1
α
n2/k+1
λ2n
= ∥Ak+1v∥2
α
n2/k+1
λ2n
.
Substituting k + 1 ≥ 1/2εlog(9n/4) in the r.h.s. above gives |αn|
1/k+1 ≥
e−ε ≥ 1 − ε, completing the proof.
8.2
The Second Eigenvector via Powering
The following is the main theorem of this section.
Theorem 8.2. Given an undirected, unweighted graph G on n vertices
and m edges, there is an algorithm that outputs a vector x such that
Eij←ν[(xi − xj)2]
E(i,j)←µ×µ[(xi − xj)2] ≤ 2λ2(L)
and runs in ˜O(m + n) time. Here L is the normalized Laplacian of G.
1 For vectors u,v, and p,q ≥ 0 such that 1/p + 1/q = 1, |⟨u,v⟩| ≤ ∥u∥p∥v∥q. In this applica-
tion we choose p def
= k + 1,q def
= k+1/k and ui
def
= α
i2/k+1
,vi
def
= α
i2k/k+1
λ2k
i .

8.2 The Second Eigenvector via Powering
51
8.2.1
The Trivial Application
Let us see what the power method in Lemma 8.1 gives us when we
apply it to I − L, where L is the normalized Laplacian of a graph
G. Recall that L = I − D−1/2AD−1/2, where D is the degree matrix of
G and A its adjacency matrix. Since we know the largest eigenvector
for D−1/2AD−1/2 explicitly, i.e., D
1/21, we can work orthogonal to it
by removing the component along it from the random starting vector.
Thus, the power method converges to the second largest eigenvector of
D−1/2AD−1/2, which is the second smallest eigenvector of L. Hence, if
we apply the power method to estimate λ2(L), we need ε ∼ λ2(L)/2 in
order to be able to approximate λ2(L) up to a factor of 2. This requires
the computation of (D−1/2AD−1/2)kv for k = Θ(logn/λ2(L)), which gives
a time bound of O(mlogn/λ2(L)) since λ2(L) can be as small as 1/m and,
hence, k may be large. In the next section we see how to remove this
dependency on 1/λ2(L) and obtain an algorithm which runs in time ˜O(m)
and computes a vector that approximates λ2(L) to within a factor of 2.
8.2.2
Invoking the Laplacian Solver
Start by observing that λ2
def
= λ2(L) is the largest eigenvalue of L+, the
pseudo-inverse of L. Thus, if we could compute L+u, for a vector u
which is orthogonal to D
1/21, it would suﬃce to use the power method
with ε = 1/2 in order to approximate λ2 up to a factor of 2. Hence, we
would only require k = Θ(logn). Unfortunately, computing L+ exactly
is an expensive operation. Theorem 3.1 from Section 3 implies that
there is an algorithm, LSolve, that given v can approximate L+v in
time O(mlogn/√λ2). Roughly, this implies that the total time required to
estimate λ2 via this method is O(m(logn)2/√λ2). This is the limit of meth-
ods which do not use the structure of L. Note that L+ = D
1/2L+D
1/2
hence, it is suﬃcient to compute L+v when ⟨v,1⟩ = 0.
Recall the procedure LSolve in Theorem 3.1 and its linearity in
Section 3.4. An immediate corollary of these results is that there is
a randomized procedure which, given a positive integer k, a graph
Laplacian L, a vector v ∈ Rn, and an ε > 0, returns the vector Zkv,
where Z is the symmetric linear operator implicit in LSolve satisfying

52
Graph Partitioning IV Computing the Second Eigenvector
(1 − ε/4)Z+ ⪯ L ⪯ (1 + ε/4)Z+ with the same image as L. Moreover,
this algorithm can be implemented in time O(mklognlog 1/ε).
Coming back to our application, suppose we are given ε < 1/5. We
choose k so that the Rayleigh quotient of Zkv, which is supposed to be
an approximation to (L+)kv, w.r.t. L+ becomes at most (1 + ε)λ2(L)
for a random vector v. Thus, k is O(1/εlog n/ε). The important point is
that k does not depend on λ2(L) (or in the normalized case, λ2(L)).
Let u1,...,un be the eigenvectors of Z+ corresponding to the eigen-
values λ1 ≥ ··· ≥ λn = 0. (For this proof it turns out to be more con-
venient to label the eigenvalues in decreasing order.) We express v
in the basis {ui}i as v = �
i αiui. We know that with probability at
least 1/2, we have, |α1| ≥ 2/3√n. Let j be the smallest index such that
λj ≤ (1 + ε/8)λn−1. Then,
(Zkv)⊤Z+(Zkv)
(Zkv)⊤(Zkv)
=
�n−1
i=1 α2
i λ−(2k−1)
i
�n−1
i=1 α2
i λ−2k
i
≤
�j
i=1 α2
i λ−(2k−1)
i
�n−1
i=1 α2
i λ−2k
i
+
�n−1
i>j α2
i λ−(2k−1)
i
�n−1
i>j α2
i λ−2k
i
≤
�j
i=1 α2
i λ−(2k−1)
i
α2
n−1λ−2k
n−1
+ λj
≤ λn−1
�j
i=1 α2
i (1 + ε/8)−(2k−1)
α2
n−1
+ (1 + ε/8)λn−1
≤ λn−1 · 9n
4
j
�
i=1
α2
i e− ε(2k−1)
16
+ (1 + ε/8)λn−1
≤ λn−1
ε
8 + λn−1(1 + ε/8)
≤ λn−1(1 + ε/4),
where the third last line has used (1 + x)−1 ≤ e−x/2 for x ≤ 1 and k ≥
8/εlog 18n/ε + 1. Let ˆv be the unit vector Zkv/∥Zkv∥. Thus, ˆvLˆv ≤ (1 +
ε/4)ˆvZ+ˆv ≤ (1 + ε/4)2λn−1(Z+) ≤ (1+ε/4)2
1−ε/4 λ2(L) ≤ (1 + ε)λ2(L), where
the last inequality uses ε < 1/5. This, when combined with Lemma 8.1,
completes the proof of Theorem 8.2.

8.2 The Second Eigenvector via Powering
53
Notes
Lemma 8.1 is folklore. Theorem 8.2 is from [78]. It is important to
note that there is no known proof of Theorem 8.2 without invoking a
Laplacian solver; it seems important to understand why.

9
The Matrix Exponential and Random Walks
This section considers the problem of computing the matrix exponen-
tial, exp(−tL)v, for a graph Laplacian L and a vector v. The matrix
exponential is fundamental in several areas of mathematics and sci-
ence, and of particular importance in random walks and optimization.
Combining results from approximation theory that provide rational
approximations to the exponential function with the Laplacian solver,
an algorithm that approximates exp(−tL)v up to ε-error is obtained.
This algorithm runs in ˜O(mlogtlog 1/ε) time.
9.1
The Matrix Exponential
Suppose A is a symmetric n × n matrix. The matrix exponential of A
is deﬁned to be
exp(A) def
=
∞
�
i=0
Ai
i! .
Since A is symmetric, an equivalent way to deﬁne exp(A) is to ﬁrst
write the spectral decomposition of A = �n
i=1 λiuiu⊤
i , where λis are
54

9.1 The Matrix Exponential
55
the eigenvalues of A and uis are its orthonormal eigenvectors. Then,
exp(A) =
n
�
i=1
exp(λi)uiu⊤
i .
Thus, exp(A) is a symmetric PSD matrix. This matrix plays a funda-
mental role in mathematics and science and, recently, has been used to
design fast algorithms to solve semideﬁnite programming formulations
for several graph problems. In particular, it has been used in an intricate
algorithm for Balanced Edge-Separator that runs in time ˜O(m)
and achieves the spectral bound as in Theorem 7.1. In such settings,
the primitive that is often required is exp(−L)v, where L is the graph
Laplacian of a weighted graph. One way to compute an approximation
to exp(−L)v is to truncate the power series of exp(−L) after t terms
and output
u def
=
t
�
i=0
(−1)iLiv
i!
.
If we want ∥u − exp(−L)v∥ ≤ ε∥v∥, then one may have to choose
t ∼ ∥L∥ + log 1/ε. Thus, the time to compute u could be as large as
O(m(∥L∥ + log 1/ε)). In matrix terms, this algorithm uses the fact that
the matrix polynomial �t
i=0
(−1)iLi
i!
approximates exp(−L) up to an
error of ε when t ∼ ∥L∥ + log 1/ε. This dependence on ∥L∥ is prohibitive
for many applications and in this section we present an algorithm where
the dependence is brought down to log∥L∥ as in the following theorem.
Theorem 9.1. There is an algorithm that, given the graph Laplacian
L of a weighted graph with n vertices and m edges, a vector v, and a
parameter 0 < δ ≤ 1, outputs a vector u such that ∥exp(−L)v − u∥ ≤
δ∥v∥ in time ˜O((m + n)log(1 + ∥L∥)polylog 1/δ).
The proof of this theorem essentially reduces computing exp(−L)v to
solving a small number of Laplacian systems by employing a powerful
result from approximation theory. Before we give the details of this
latter result, we mention a generalization of the Laplacian solver to

56
The Matrix Exponential and Random Walks
Symmetric, Diagonally Dominant (SDD) matrices. A symmetric matrix
A is said to be SDD if for all i,
Aii ≥
�
j
|Aij|.
Notice that a graph Laplacian is SDD. Moreover, for any γ > 0, I + γL
is also SDD. While it is not straightforward, it can be shown that if one
has black-box access to a solver for Laplacian systems, such as Theorem
3.1, one can solve SDD systems. We omit the proof.
Theorem 9.2. Given an n × n SDD matrix A with m nonzero entries,
a vector b, and an error parameter ε > 0, it is possible to obtain a
vector u such that
∥u − A+b∥A ≤ ε∥A+b∥A.
The time required for this computation is ˜O(mlognlog(1/(ε∥A+∥))).
Moreover, u = Zb where Z depends on A and ε, and is such that
∥Z − A+∥ ≤ ε.
9.2
Rational Approximations to the Exponential
The starting point for the proof of Theorem 9.2 is the following result
which shows that simple rational functions provide uniform approxima-
tions to exp(−x) over [0,∞) where the error term decays exponentially
with the degree. The proof of this theorem is beyond the scope of this
monograph.
Theorem 9.3. There exists constants c ≥ 1 and k0 such that, for any
integer k ≥ k0, there exists a polynomial Pk(x) of degree k such that,
sup
x∈[0,∞)
����exp(−x) − Pk
�
1
1 + x/k
����� ≤ ck · 2−k.
A corollary of this theorem is that for any unit vector v,
∥exp(−L)v − Pk((I + L/k)+)v∥ ≤ O(k2−k).

9.2 Rational Approximations to the Exponential
57
To see this, ﬁrst write the spectral decomposition L = �
i λiuiu⊤
i where
{u}n
i=1 form an orthonormal basis. Then, note that
(I + L/k)+ =
�
i
(1 + λi/k)−1uiu⊤
i
and, hence,
Pk((I + L/k)+) =
�
i
Pk((1 + λi/k)−1)uiu⊤
i .
Here, we have used the spectral decomposition theorem from Section 1
on pseudo-inverses which can be readily justiﬁed. Thus, if v = �
i βiui
with �
i β2
i = 1, then
Pk((I + L/k)+)v =
�
i
βiPk((1 + λi/k)−1)ui,
and
exp(−L)v =
�
i
βi exp(−λi)ui.
Thus, using orthonormality of {ui}n
i=1 we obtain that
∥exp(−L)v − Pk((I + L/k)+)v∥2
≤
�
i
β2
i · (exp(−λi) − Pk((1 + λi/k)−1))2.
Hence, ∥exp(−L)v − Pk((I + L/k)+)v∥ is at most
��
i
β2
i · max
i
|exp(−λi) − Pk((1 + λi/k)−1)|.
Since �
i β2
i = 1, this implies that
∥exp(−L)v − Pk((I + L/k)+)v∥
≤ max
x∈Λ(L)|exp(−x) − Pk((1 + x/k)−1)|,
where Λ(L) def
= [λ1(L),λn(L)]. Thus, by the choice of Pk as in Theo-
rem 9.3, since all eigenvalues of the Laplacian are nonnegative, we have
proved the following lemma.

58
The Matrix Exponential and Random Walks
Lemma 9.4. There exists constants c ≥ 1 and k0 such that, for any
integer k ≥ k0, there exists a polynomial Pk(x) of degree k such that,
for any graph Laplacian L and a vector v,
∥exp(−L)v − Pk((I + L/k)+)v∥ ≤ O(k2−k)∥v∥.
Proof of Theorem 9.1
To use Pk((I + L/k)+)v as an approximation to exp(−L)v, let us
assume that Pk is given to us as Pk(x) = �k
i=0 cixi with |ci| = O(kk).
This is not obvious and a justiﬁcation is needed to assume this. We
omit the details. Thus, we need to compute
k
�
i=0
ci((I + L/k)+)iv.
Here, we assume that on input v and (I + L/k), the output of Theorem
9.2 is Zv where ∥Z − (I + L/k)+∥ ≤ ε. Since ∥(I + L/k)+∥ ≤ 1 and Z
is ε-close to it, we obtain that ∥Z∥ ≤ (1 + ε). Thus, using the simple
inequality that for a,b ∈ R,
aj − bj = (a − b) ·
�j−1
�
i=0
(−1)iaj−1−ibi
�
,
and we obtain that
∥Zjv − ((I + L/k)+)jv∥ ≤ 2εj(1 + ε)j∥v∥.
Therefore, by the triangle inequality,
�����
k
�
i=0
ciZiv −
k
�
i=0
ci((I + L/k)+)iv
����� ≤ εk2(1 + ε)k max
i
|ci|.
Since u = �k
i=0 ciZiv, combining the inequality above with Lemma 9.4,
we get that
∥exp(−L)v − u∥ ≤ εk2(1 + ε)kkO(k)∥v∥ + O(k2−k)∥v∥

9.3 Simulating Continuous-Time Random Walks
59
where we have used that maxi |ci| = kO(k). For a parameter δ, choose
k = log 2/δ and ε = δ(1+∥L∥)
2kO(k)
to obtain that
∥exp(−L)v − u∥ ≤ δ∥v∥.
This completes the proof of Theorem 9.1.
9.3
Simulating Continuous-Time Random Walks
In this section we show how one can simulate continuous-time random
walks on an undirected graph with m edges for time t in ˜O(mlogt) time.
Not only are random walks of fundamental importance, the ability to
simulate them in near-linear-time results in near-linear-time algorithms
for the Balanced Edge-Separator problem. From an applications
point of view, the bound on the running time is essentially independent
of t. Now we phrase the problem and show how this simulation result is
an immediate corollary of Theorem 9.1. For an undirected graph G, let
D denote its degree matrix and A its adjacency matrix. Then, the tran-
sition matrix of a one-step random walk in G is W def
= AD−1. Starting
with a distribution v at time 0, the t-step transition probability of the
discrete-time random walk on G is given by W tv. The continuous-time
random walk on G for time t, also called the heat-kernel walk, is deﬁned
by the following probability matrix:
�
Wt
def
= exp(−t(I − W)) = exp(−t)
∞
�
i=0
ti
i!W i.
Note that this can be interpreted as a the discrete-time random walk
after a Poisson-distributed number of steps with rate t. Here, given a
starting vector v, the goal is to compute �
Wtv. Before we show how to
do this, note that W is not symmetric. However, it can be symmetrized
by considering the spectrally equivalent matrix D−1/2WD
1/2 which is
I − L where L is the normalized Laplacian of G. Thus, W = D
1/2(I −
L)D−1/2, and hence,
�
Wt = D
1/2 exp(−tL)D−1/2.
Now, it is easy to see that Theorem 9.1 applies since tL ⪰ 0 and
∥tL∥ ≤ O(t). Thus, given δ > 0, we can use the algorithm in the proof

60
The Matrix Exponential and Random Walks
of Theorem 9.1 to obtain a vector u such that
∥exp(−tL)D−1/2v − u∥ ≤ δ∥D−1/2v∥.
The approximation to �
Wtv is then D
1/2u. It follows that
∥�
Wtv − D
1/2u∥ ≤ δ∥D
1/2∥ · ∥D−1/2∥∥v∥.
This proves the following theorem.
Theorem 9.5. There is an algorithm that, given an undirected graph
G with m edges, a vector v, a time t ≥ 0, and a δ > 0, outputs a vector
ˆu such that
∥�
Wtv − ˆu∥ ≤ δ
�
dmax
dmin
· ∥v∥.
The time taken by the algorithm is ˜O(mlog(1 + t)polylog1/δ). Here,
dmax is the largest degree of G and dmin the smallest.
Notes
For more on the matrix exponential and the role it plays in the design
of fast algorithms for semideﬁnite programs the reader is referred to
sample applications as in [12, 40, 41, 60, 61, 62]. Two thesis on this
topic are [44] and [59].
The reduction from SDD systems to Laplacian systems appears
in [38]. Theorems 9.1 and 9.5 are implicit in the work of Orecchia
et al. [60]. Theorem 9.3 on rational approximations was proved in [70].
The fact that the coeﬃcients can be computed and are bounded is not
present in this paper, but can be proved. While this section shows how
to reduce the computation of exp(−Lv) to polylog n computations of
the form L+v, one may ask if the converse is true. Recently, Sachdeva
and Vishnoi [69] answered this question by presenting a simple reduc-
tion in the other direction: The inverse of a positive-deﬁnite matrix can
be approximated by a weighted-sum of a small number of matrix expo-
nentials. This proves that the problems of exponentiating and inverting
are essentially equivalent up to polylog factors. This reduction makes

9.3 Simulating Continuous-Time Random Walks
61
no use of the fact that L is a Laplacian, but only that it is a symmetric
positive-semideﬁnite matrix.
For more on continuous-time random walks, the reader is referred
to the book [51]. It is a challenging open problem to prove an analog
of Theorem 9.5 for discrete-time (lazy) random walks: Find a vector u
s.t. ∥W tv − u∥ ≤ ε∥v∥ in ˜O(mlog(1 + t)log 1/ε) time.

10
Graph Sparsiﬁcation I
Sparsiﬁcation via Eﬀective Resistances
In this section a way to spectrally sparsify graphs is introduced that
uses the connection to electrical networks presented in Section 4. In the
next section we show how sparse spectral sparsiﬁers can be computed
in ˜O(m) time using Laplacians. Spectral sparsiﬁers introduced in this
section play a crucial role in the proof of Theorem 3.1 presented in
Section 18.
10.1
Graph Sparsiﬁcation
In the cut sparsiﬁcation problem, one is given an undirected unweighted
graph G = (V,E) and a parameter ε > 0, and the goal is to ﬁnd a
weighted graph H = (V,E′) such that for every cut (S, ¯S) of V, the
weight of the edges that cross this cut in H is within a multiplicative
1 ± ε factor of the number of edges in G that cross this cut. The goal
is to keep the number of edges (not counting weights) of H small.
Moreover, ﬁnding H quickly is also important for applications. Benczur
and Karger gave a remarkable algorithm which produces cut sparsiﬁers
of size (i.e., number of edges) ˜O(n/ε2) in time ˜O(m). Such a procedure
has found use in many fundamental graph algorithms where it allows
the running time to be brought down from its dependency on m to n
62

10.1 Graph Sparsiﬁcation
63
(note that m can be as big as Ω(n2)). In particular, cut sparsiﬁers can
be used in the algorithms presented in Sections 6, 7, and 9.
A stronger notion than cut sparsiﬁcation is that of spectral
sparsiﬁcation. This will play an important role in constructing
Laplacian solvers in Section 18. However, in this section and the next,
we show how to construct spectral sparsiﬁers in ˜O(m) time using Lapla-
cian solvers.
Deﬁnition 10.1. Given an undirected graph G = (V,E) with a param-
eter ε > 0, a weighted graph H = (V,E′) is said to be an ε-spectral
sparsiﬁer of G if
1
(1 + ε) ≤ x⊤LHx
x⊤LGx ≤ (1 + ε),
∀x ∈ Rn.
Where LG and LH denote the graph Laplacians for G and H, respec-
tively.
The goal is to minimize the number of edges of H and construct it as
quickly as possible. In particular, we consider whether spectral sparsi-
ﬁers with ˜O(n/poly(ε)) edges exist and can be constructed in ˜O(m) time.
Before we go on, notice that if H is an ε-spectral sparsiﬁer for G then it
is an ε-cut sparsiﬁer for G as well. To see this, plug the vectors x = 1S,
(the indicator vector for a cut (S, ¯S)), into the deﬁnition above. There
are also easy-to-construct examples that show these notions are not
the same. An ε-cut sparsiﬁer of G can be an arbitrarily bad spectral
sparsiﬁer for G.
We show the existence of ε-spectral sparsiﬁers of G of size
O(nlogn/ε2). In the next section, we show how to construct such a spar-
siﬁer in time ˜O(mlog 1/ε).1 Formally, we prove the following theorem.
Theorem 10.1. There exists a randomized algorithm that, given a
graph G and an approximation parameter ε > 0, constructs a spectral
sparisifer H of size O(nlogn/ε2) with probability 1 − 1/n.
1 If G is weighted, then there is also a dependency on the log of the ratio of the largest
to the smallest weight in G in the running time. The running time crucially relies on the
Spielman–Teng Laplacian solver.

64
Graph Sparsiﬁcation I Sparsiﬁcation via Eﬀective Resistances
In the next section we reﬁne this theorem to prove the following, which
also includes a bound on the running time.
Theorem 10.2. There exists a randomized algorithm that, given a
graph G and an approximation parameter ε > 0, constructs a spectral
sparisifer H of size O(nlogn/ε2) in time ˜O(mlog 1/ε) with probability
1 − 1/n.
10.2
Spectral Sparsiﬁcation Using Eﬀective Resistances
The proof of Theorem 10.1 relies on an edge sampling algorithm:
Repeatedly sample (with replacement) edges from G according to a
carefully chosen probability function over the edges and weight each
sampled edge inversely to this probability. The resulting multiset of
weighted edges, normalized by the number of samples, is the output of
the algorithm.2 Formally, let pe be the probability that edge e is cho-
sen by the sampling algorithm. Let Y be the random variable such that
P[Y = e] = pe. Let T be a parameter denoting the number of samples
that are taken by the algorithm. Let Y1,Y2,...,YT be i.i.d. copies of Y .
Then the weighted multi-set of edges equals
��
Y1,
1
T · pY1
�
,
�
Y2,
1
T · pY2
�
,...,
�
YT ,
1
T · pYT
��
.
What is the Laplacian of the associated graph H? Let B be an inci-
dence matrix for G and let be be the column vector corresponding
to edge e in B⊤. Then, LG = �
e beb⊤
e = B⊤B. For notational conve-
nience, deﬁne ue
def
= be/√pe. Then, a simple calculation shows that
LH
def
= 1
T
⊤
�
i=1
bYib⊤
Yi
pYi
= 1
T
⊤
�
i=1
uYiu⊤
Yi.
To analyze the eﬀectiveness of this scheme, ﬁrst note that
E[uY u⊤
Y ] =
�
e
P[Y = e] · ueu⊤
e =
�
e
beb⊤
e = LG.
2 This multi-set can be converted to a weighted graph by additively combining weights of
repeated edges.

10.2 Spectral Sparsiﬁcation Using Eﬀective Resistances
65
Therefore,
E[LH] = 1
T
⊤
�
i=1
E[uYiuYi] = LG.
So far this is abstract because we have not speciﬁed the pes. We want
to choose pe so that LH behaves similar to LG w.h.p. This is where
eﬀective resistances and the intuition from Theorem 4.5 come into play.
We let
pe
def
=
Re
n − 1,
where Re
def
= Reﬀ(e) is the eﬀective resistance of the edge e deﬁned
to be b⊤
e L+
Gbe.3 The normalization factor of n − 1 is due to the fact
�
e Re = n − 1. What is the intuition behind choosing an edge with
probability proportional to its eﬀective resistance? Unfortunately, there
does not exist a very satisfactory answer to this question. We give a
rough idea here. By Theorem 4.5, Re is proportional to the probability
of an edge being in a random spanning tree and choosing a small num-
ber of random spanning trees from G independently seems like a good
strategy to spectrally sparsify G. We move on to analyze this strategy.
Proof of Theorem 10.1
For a ﬁxed orientation of the edges of G, recall that the matrix Π def
=
BL+
GB⊤ introduced in Section 4 satisﬁes the following properties:
(1) Π2 = Π,
(2) Let Πe denote the column of Π corresponding to edge e. Then
Re = ∥Πe∥2,
(3) �
e Re = n − 1, and
(4) Π is unitarily equivalent to �n−1
j=1 eje⊤
j , where ej is the j-th
standard basis vector in Rm. This is because Π is symmetric
and PSD, and all its eigenvalues are 0 or 1. Thus, if G is
connected, the multiplicity of eigenvalue 1 is n − 1 and that
of 0 is m − n + 1.
3 If the graph G is weighted, then we choose pe proportional to wG(e)Re.

66
Graph Sparsiﬁcation I Sparsiﬁcation via Eﬀective Resistances
Deﬁne ve
def
= Πe/√pe. Let M def
= vY v⊤
Y and Mi
def
= vYiv⊤
Yi, i = 1,2,...,T
be i.i.d. copies of M. The following theorem is required. It is an analog
of Chernoﬀ bound for sums of matrix-valued random variables.
Theorem 10.3. Let ε > 0 be a small enough constant. Let M ∈ Rd×d
be a random, symmetric PSD matrix such that E[M] = Id, where Id is
the d-dimensional identity matrix. Let ρ def
= supM ∥M∥. Let T be a non-
negative integer and let M1,...,MT be independent copies of M. Then,
P
������
1
T
T
�
i=1
Mi − Id
����� > ε
�
≤ 2d · exp
�
−Tε2
2ρ
�
.
Theorem 10.3 also holds when E[M] = �d′
j=1 eje⊤
j for some d′ ≤ d, in
which case the above bound holds with d′ in place of d. A slightly
more general result, which is needed for our application, is that the
bound also holds when E[M] is unitarily equivalent to the above matrix.
This follows because the operator norm is unitarily invariant. As an
application of Theorem 10.3, we ﬁrst calculate
E[M] = E[vev⊤
e ] =
�
e
ΠeΠ⊤
e = Π,
which by Property (4) above is unitarily equivalent to �n−1
j=1 eje⊤
j . Note
that Property (2) implies that
∥ve∥2 = ∥Πe∥2
pe
= Re
pe
= n − 1.
Therefore ∥Mi∥ ≤ n − 1 for all i. Applying Theorem 10.3, we obtain
P[∥�Π − Π∥ > ε] ≤ 2(n − 1) · exp
�
−
Tε2
2(n − 1)
�
,
(10.1)
where �Π def
=
T1
�vYiv⊤
Yi. Setting T = O(nlogn/ε2) ensures that this fail-
ure probability is n−Ω(1). How do we relate this to LH? Observe that
since Π = BL+
GB⊤, it follows that for any e:
ve = Πe
√pe
= BL+
Gbe
√pe
= BL+
Gue.

10.3 Crude Spectral Sparsﬁcation
67
Therefore,
�Π = 1
T
�
vYiv⊤
Yi = 1
T
�
BL+
GuYiu⊤
YiL+
GB⊤ = BL+
GLHL+
GB⊤,
and
Π = BL+
GB⊤ = BL+
GLGL+
GB⊤.
Now,
����Π − Π
��� = sup
x̸=0
�����
x⊤(�Π − Π)x
x⊤x
����� = sup
x̸=0
����
x⊤BL+
G(LH − LG)L+
GB⊤x
x⊤x
����.
Because G is connected, if z is a vector such that Bz = 0, then z must be
parallel to the all 1s vector. Hence, for any z ̸= 0 such that ⟨z,1⟩ = 0,
Bz ̸= 0. Therefore, we can substitute x = Bz in the above equation.
Using the property that LGL+
Gz = z for any such z, we obtain
����Π − Π
��� ≥
sup
z̸=0
⟨z,1⟩=0
����
z⊤B⊤BL+
G(LH − LG)L+
GB⊤Bz
z⊤B⊤Bz
����
=
sup
z̸=0
⟨z,1⟩=0
����
z⊤LGL+
G(LH − LG)L+
GLGz
z⊤LGz
����
=
sup
z̸=0
⟨z,1⟩=0
����
z⊤(LH − LG)z
z⊤LGz
����.
Combining with Equation (10.1) yields
P

 sup
z̸=0
⟨z,1⟩=0
����
z⊤(LH − LG)z
z⊤LGz
���� > ε

 ≤ P[∥�Π − Π∥ > ε] = n−Ω(1)
as desired. This completes the proof of Theorem 10.1.
10.3
Crude Spectral Sparsﬁcation
In this section we note that the algorithm and the proof underlying
Theorem 10.1 can be modiﬁed to obtain a crude spectral sparsiﬁer.

68
Graph Sparsiﬁcation I Sparsiﬁcation via Eﬀective Resistances
The exact knowledge of Re is no longer necessary. Instead, the proof
above can be modiﬁed to work when we have qe ≥ Re for all e. The
number of samples required to ensure that the resulting graph is a
spectral sparsiﬁer can be easily shown to be about W def
= �
e qe, which
replaces n − 1. We record this theorem here (for weighted graphs) and
show how to use it in Section 18.
Theorem 10.4. Consider graph G = (V,E) with edge weights wG, γ >
0, and numbers qe such that qe ≥ wG(e)Re for all e. If W def
= �
e qe, then
the spectral sparsiﬁer in Theorem 10.1 that takes O(W logW log 1/γ)
samples from the probability distribution induced by the qes produces
a sampled graph H that satisﬁes
G ⪯ 2H ⪯ 3G
with probability 1 − γ.
Notes
The notion of cut sparsiﬁcation was introduced in [16]. The state of the
art on cut sparsiﬁcation can be found in [31].
Spectral sparsiﬁcation was introduced in [80] and plays a crucial
role in the construction of Laplacian solvers, and the ﬁrst algorithm
for a spectral sparsiﬁer was given in this paper. Their sparsiﬁer had
size O(npolylogn) and could be constructed in ˜O(m) time. Theorem
10.1 was proved in [76] where it is showed how to construct ε-spectral
sparsiﬁers of size O(nlogn/ε2). This has been improved to O(n/ε2) in [14],
coming close to the so-called Ramanujan bound.
Theorem 10.4 was observed in [49] and will be used in Section 18.
Theorem 10.3, which is used in the proof of Theorem 10.1 was proved
in [66] using a non-commutative Khinchine inequality. Subsequently, it
has been proved in an elementary manner using the matrix exponential
(introduced in Section 9) independently by several researchers, see [4].

11
Graph Sparsiﬁcation II
Computing Electrical Quantities
In this section the near-linear-time Laplacian solver is used to approx-
imately compute eﬀective resistances for all edges in ˜O(m) time. This
was needed in Section 10. This section starts by presenting the sim-
pler task of approximately computing currents and voltages when a
unit current is input at a vertex and taken out at another, again using
Laplacian solvers. These primitives are also used in Section 12.
11.1
Computing Voltages and Currents
Given s,t ∈ V with one unit of current ﬂowing into s and one unit
ﬂowing out of t, the goal is to compute the vector L+(es − et); the
vector of voltages at the vertices of the graph. We employ Theorem
3.1 to compute this vector in an approximate sense in ˜O(m) time.
Recall that this theorem says there is an algorithm LSolve which
takes a graph Laplacian L, a vector y, and an error parameter δ > 0,
and returns an x satisfying
∥x − L+y∥L ≤ δ∥L+y∥L.
Note that for any two vectors ∥v − w∥∞ ≤ ∥v − w∥. Hence, by a choice
of δ def
= ε/poly(n) in Theorem 3.1, we can ensure the approximate vector of
69

70
Graph Sparsiﬁcation II Computing Electrical Quantities
voltages we produce is ε-close in every coordinate to the actual voltage
vector. Since the dependence of the running time in Theorem 3.1 on the
error tolerance is logarithmic, the running time remains ˜O(mlog 1/ε).
Thus, for any small enough constant ε > 0, we obtain a vector u of
voltages such that ∥u − L+(es − et)∥∞ ≤ ε. Further, if u is the vector
of voltages, then the vector of electrical currents, assuming all resis-
tances are 1, is Bu, and can be computed in additional O(m + n)
time. Note that since ∥u − L+(es − et)∥∞ ≤ ε,
∥Bu − BL+(es − et)∥∞ ≤ 2ε.
Hence,
∥B⊤Bu − B⊤BL+(es − et)∥∞ = ∥Lu − (es − et)∥∞ ≤ 2εn.
We may assume that our current vector satisﬁes
∥Lu − (es − et)∥∞ ≤ ε.
Thus, Lu may not be a unit s,t-ﬂow, but very close to it. If one desires,
one can compute in time O(m + n) a vector ˜v from u such that L˜v =
es − et which is indeed a unit s,t-ﬂow, and ∥u − ˜v∥∞ ≤ ε. We leave it
as an exercise for the reader. We summarize the results of this section
in the following theorem which states the result for a weighted graph;
the extension is straightforward.
Theorem 11.1. There is an �O(m(logr)log 1/ε) time algorithm which
on input ε > 0, G = (V,E,w) with r def
= wmax/wmin, and s,t ∈ V, ﬁnds
vectors ˜v ∈ RV and ˜f ∈ RE such that if v def
= L+(es − et) and f def
=
WBL+(es − et),
(1) L˜v = es − et and ˜f = WB˜v,
(2) ∥v − ˜v∥∞ ≤ ε,
(3) ∥f − ˜f∥∞ ≤ ε, and
(4) |�
e f2
e/we − �
e
˜f2
e/we| ≤ ε.
Here W is an m × m diagonal matrix with W(e,e) = we.

11.2 Computing Eﬀective Resistances
71
11.2
Computing Eﬀective Resistances
For an edge e = ij, let Re denote the eﬀective resistance of the edge
e which, recall, is deﬁned to be (ei − ej)⊤L+(ei − ej). For our spec-
tral sparsiﬁcation application we required a way to compute Re for all
edges. As noted in the previous section, if we are interested in near-
linear time algorithms, we have to use an approximation ˜Re of Re. It
can be shown that any O(1) approximation suﬃces for our spectral
sparsiﬁcation application; it just increases the sample size by a con-
stant factor. We saw in the last section that for a given ε > 0 and edge
e, we can compute ˜Re such that
(1 − ε)Re ≤ ˜Re ≤ (1 + ε)Re
in time ˜O(mlog 1/ε) using the LSolve. A naive extension that com-
putes ˜Re for all edges takes ˜O(m2 log 1/ε) time. Can we reduce this to
˜O(mlog 1/ε)? The answer, as we will see shortly, is yes. The key obser-
vation is that for an edge e = ij,
Re = ∥BL+(ei − ej)∥2.
Hence, if we let wi
def
= BL+ei, then
Re = ∥wi − wj∥2
for all e. The wis live in Rm. Is it necessary to compute wis if all we
are interested in the ℓ2 distance of m pairs among them? The answer
is no. The Johnson–Lindenstrauss Lemma postulates that computing
the projections of these vectors on a random logm/ε2 dimensional space
preserves distances up to a multiplicative factor of 1 ± ε. Hence, our
approach is to choose a random matrix A which is roughly logm × m
and compute Awi = ABL+ei for all i. Several ways to choose A work
and, in particular, we appeal to the following theorem where entries of
A are ±1 up to normalization. This random matrix A corresponds to
the random subspace.
Theorem 11.2. Given ﬁxed vectors w1,...,wn and ε > 0, let A ∈
{−1/
√
k, 1/
√
k}k×m be a random matrix with k ≥ clogn/ε2 for some con-
stant c > 0. Then, with probability at least 1 − 1/n, for all 1 ≤ i,j ≤ n
(1 − ε)∥wi − wj∥2 ≤ ∥Awi − Awj∥2 ≤ (1 + ε)∥wi − wj∥2.

72
Graph Sparsiﬁcation II Computing Electrical Quantities
Getting back to our problem of computing Re for all e, let Z def
= ABL+
where A is the random matrix promised in the theorem above. We
compute an approximation �Z by using LSolve to approximately com-
pute the rows of Z. Let the vectors zi and ˜zi denote the ith rows of Z
and ˜Z, respectively (so that zi is the i-th column of Z⊤). Now we can
construct the matrix �Z in the following three steps:
(1) Let A be a random ±1/
√
k matrix of dimension k × m where
k = O(logn/ε2).
(2) Compute Y = AB. Note that this takes 2m × O(logn/ε2) +
m = �O(m/ε2) time since B has 2m entries.
(3) Let y⊤
i , for 1 ≤ i ≤ k, denote the rows of Y , and compute
˜zi
def
= LSolve(L,yi,δ) for each i.
We now prove that, for our purposes, it suﬃces to call LSolve with
δ def
=
ε
poly(n). We do not explicitly specify the poly(n), but it can be
recovered from the proof below. First, we claim that if ∥zi − ˜zi∥ ≤ ε
for all i, then for any vector u with ∥u∥ = O(1),
|∥Zu∥2 − ∥ ˜Zu∥2|
∥Zu∥2
≤ ε · poly(n).
In our application u = ei − ej for some i,j. Hence, ∥u∥ = O(1). To see
why our claim is true, let ai
def
= ⟨zi,u⟩ and bi
def
= ⟨˜zi,u⟩. Then, ∥Zu∥2 =
�
i a2
i and ∥ ˜Zu∥2 = �
i b2
i . Thus,
|∥Zu∥2 − ∥ ˜Zu∥2| =
�����
�
i
(a2
i − b2
i )
����� ≤
��
i
(ai − bi)2 �
i
(ai + bi)2.
Note that
�
i
(ai + bi)2 ≤ 2
�
i
(a2
i + b2
i ) = 2∥Zu∥2 + 2∥ ˜Zu∥2 = poly(n).
This is because ∥Zu∥2 = ∥ABL+(ei − ej)∥2
for some i,j. From
Johnson–Lindenstrauss, we know that w.h.p. this is within a constant
factor of ∥BL+(ei − ej)∥2 = (ei − ej)⊤L+(ei − ej) = poly(n) as noted
before. We need to estimate ∥ ˜Zu∥2 and show that it is poly(n) as well.
Note that
|ai − bi| = |⟨zi − ˜zi,u⟩| ≤ ∥zi − ˜zi∥∥u∥ = ε∥u∥ = O(ε).

11.2 Computing Eﬀective Resistances
73
Hence,
�
i
(ai − bi)2 ≤ O(ε2 · k).
Thus, bi ≤ ai + O(ε) and, hence,
�
i
b2
i ≤
�
i
a2
i + O(ε
�
i
ai) + O(ε2 · k)
≤
�
i
a2
i + O(ε ·
√
k
�
i
a2
i ) + O(ε2 · k).
Therefore,
∥ ˜Zu∥2 =
�
i
b2
i ≤ a2
i + ε2 · poly(k) = ∥Zu∥2 + ε2 · poly(k) = poly(n),
and thus
|∥Zu∥2 − ∥ ˜Zu∥2| ≤
��
i
(ai − bi)2 �
i
(ai + bi)2 ≤ ε · poly(n).
Since ∥Zu∥2 is the eﬀective resistance of some edge, it is at least 1/poly(n)
when the graph is unweighted. Hence,
|∥Zu∥2 − ∥ ˜Zu∥2|
∥Zu∥2
≤ ε · poly(n).
To summarize, we have proved the following lemma.
Lemma 11.3. Suppose
(1 − ε)Rij ≤ ∥Z(ei − ej)∥2 ≤ (1 + ε)Rij
for every pair i,j ∈ V . If for all i, ∥zi − ˜zi∥L ≤ ε∥zi∥L where ε ≤
δ
poly(n),
then
(1 − δ)Rij ≤ ∥ �Z(ei − ej)∥2 ≤ (1 + δ)Rij
for all i,j ∈ V .
Thus, the construction of �Z takes �O(mlog(1/δ)/ε2) = �O(m/ε2) time. We
can then ﬁnd the approximate resistance ∥ �Z(ei − ej)∥2 ≈ Rij for any
i,j ∈ V in O(log n/ε2) time. When the edges are weighted, one can
extend this analysis to prove the following theorem.

74
Graph Sparsiﬁcation II Computing Electrical Quantities
Theorem 11.4.
There is an
�O(m(logr)/ε2) time algorithm which,
on input ε > 0 and G = (V,E,w) with r def
= wmax/wmin, computes an
O(logn/ε2) × n matrix �Z such that with probability at least 1 − 1/n
(1 − ε)Rij ≤ ∥ �Z(ei − ej)∥2 ≤ (1 + ε)Rij
for every pair of vertices i,j ∈ V .
Notes
More details concerning Theorem 11.1 can be found in [22]. Theo-
rem 11.2 was proved in [3]. Theorem 11.4 is from [76].

12
Cuts and Flows
This section presents an algorithm which reduces the s,t-max-ﬂow/
min-cut problems in undirected graphs to their corresponding electri-
cal analogs. Since electrical ﬂows/voltages can be computed in ˜O(m)
time using Laplacian solvers as shown in Section 11, this section is ded-
icated to showing how, using the multiplicative weight update method,
˜O(√m)-electrical ﬂow computations suﬃce to approximately compute
the max-ﬂow and min-cut between s and t.
12.1
Maximum Flows, Minimum Cuts
Given an undirected graph G = (V,E) with edge capacities c : E �→
R≥0, the s,t-MaxFlow problem is to compute a ﬂow of maximum
value from a source s ∈ V to a sink t ∈ V. To talk about a ﬂow on G,
it is convenient to direct its edges arbitrarily, captured by an incidence
matrix B, and allow the ﬂow value on an edge to be positive or negative.
A ﬂow f = (fe)e∈E is then a (combinatorial) s,t-ﬂow if
(1) f⊤Bev = 0 for all v ∈ V \{s,t},
(2) f⊤Bes + f⊤Bet = 0, and
(3) |fe| ≤ ce for all e.
75

76
Cuts and Flows
The goal of the s,t-MaxFlow problem is to compute a ﬂow which
maximizes the ﬂow out of s, i.e., |f⊤Bes|. It is well known that the
maximum s,t-ﬂow is the same as the minimum capacity cut that sep-
arates s and t. The capacity of a cut (S, ¯S) is the sum of the capacities
of all the edges with one endpoint in S and another in ¯S. This is the
Max-Flow–Min-Cut theorem. The corresponding problem of ﬁnding the
minimum cut separating s and t is called the s,t-MinCut problem.
In this section we give approximation algorithms for both the s,t-
MaxFlow and the s,t-MinCut problem. Departing from traditional
approaches, the algorithms presented in this section compute combi-
natorial ﬂows and cuts by combining the information obtained from
electrical ﬂows and potentials which can be computed quickly to a
good accuracy, see Section 11. We prove the following theorems.
Theorem 12.1.
There is an algorithm such that, given an undi-
rected, capacitated graph G = (V,E,c), two vertices s,t, and an ε > 0,
such that the maximum value from s to t is F ⋆, the algorithm out-
puts a (combinatorial) ﬂow of value at least (1 − ε)F ⋆ and runs in
˜O(m
3/2poly(1/ε)) time.
Theorem 12.2. There is an algorithm such that, given an undirected,
capacitated graph G = (V,E,c), two vertices s,t, and an ε > 0, such
that there is a cut separating s and t of value F ⋆, the algorithm outputs
an s,t-cut of value at most (1 + ε)F ⋆ and runs in ˜O(m
3/2poly(1/ε)) time.
It is important to note that these algorithms work only for undirected
graphs. Since we only compute a 1 ± ε-approximation to the max-ﬂow–
min-cut problems, there is a simple trick to ensure that the ratio of the
largest to the smallest capacity in the input graph is at most O(m2/ε).
Hence, the running times do not depend on the capacities. Proving
this is left as an exercise. Finally, the algorithms in these theorems can
be modiﬁed to obtain a running time which depends on m
4/3 rather
than m
3/2; refer to the notes at the end of this section. At this point
the reader may review the connection between graphs and electrical
networks in Section 4.

12.2 Combinatorial versus Electrical Flows
77
12.2
Combinatorial versus Electrical Flows
Recall that given resistances re for edges in G captured by an m × m
diagonal matrix R where R(e,e) def
= re and 0 otherwise, if F units of
current is injected at s and taken out at t, then the electrical ﬂow
vector f can be written as f def
= R−1BL⊤F(es − et). Here, the Laplacian
is deﬁned to be L def
= B⊤R−1B. Using Theorem 11.1 from Section 11,
we know that f can be computed approximately to a precision of ε
in ˜O(mlog 1/ε) time. How good a substitute is f for a combinatorial s,
t-ﬂow? First note that in the input to the s,t-MaxFlow problem there
are no resistances. We will have to determine how to set them. The
problem with electrical ﬂows is that they could violate edge capacities.
There does not seem to be an easy way to ﬁnd resistances such that
the electrical ﬂow obtained by the electrical network satisfies the edge
capacities and also maximizes the s,t-ﬂow. In fact, it is not immediately
clear that such resistances exist; we leave showing that they do as an
exercise to the reader. The algorithm in the proof of Theorem 12.1
is iterative and starts with a set of resistances. At each iteration it
computes the s,t-electrical ﬂow and updates the resistances of the edges
based on the congestion due to this ﬂow. The update rule is governed
by the intuition that the higher the resistance of an edge, the less the
electrical ﬂow will run through it. The ingenuity is in deﬁning an update
rule such that the number of iterations is small and the average of the
electrical ﬂows computed satisfies the capacity constraints.
Energy of Flows
In Section 4 we proved (see Theorem 4.7) that for graph G = (V,E) with
resistances re given by a diagonal matrix R, if f⋆ def
= R−1BL+(es − et),
then f⋆ minimizes Er(f) def
= �
e ref2
e among all unit ﬂows f from s to t.
The same also holds for ﬂows of magnitude F. Thus, any s,t ﬂow that
respects capacity and pumps the same ﬂow as the electrical ﬂow from s
to t has at least the energy of the corresponding electrical ﬂow. Let g
be such a combinatorial ﬂow, i.e., |ge| ≤ 1. Then we have that
Er(f⋆) ≤
�
e
reg2
e ≤
�
e
re.
(12.1)

78
Cuts and Flows
The last inequality follows from the fact that |ge| ≤ ce. Thus, if Er(f⋆) >
�
e re, we are certain there is no s,t-max-ﬂow of value corresponding
to that of f⋆. We use this observation crucially in the algorithm.
12.3
s,t-MaxFlow
In this section we prove Theorem 12.1. Let F ⋆ be the value of the s,t-
max-ﬂow in G. We present an algorithm that ﬁnds a (1 − ε)F ⋆ s,t-ﬂow
in time ˜O(m
3/2poly(1/ε)). For convenience, we assume that ce = 1, ∀e.
Refer to Section 12.3.1 for general ce.
It suﬃces to present an algorithm which, given a value F, either
outputs an s,t-ﬂow of value at least (1 − O(ε))F satisfying the capac-
ity constraints or certiﬁes that F > F ⋆. The algorithm is iterative
and appeals to the multiplicative weight update (MWU) method. The
MWU technique is very general and here we present it only from the
point of view of our application. At any given time t, the resistances
for the edges are given by a vector rt. The corresponding m × m diag-
onal matrix is denoted by Rt and the Laplacian Lt
def
= B⊤R−1
t B. The
algorithm ElecFlow appears below.
At any time t, the algorithm maintains weights wt
e which are positive
numbers. To start with, w0
e are set to one for all e. These weights are
used to compute the resistances rt
e at time t. The natural choice for rt
e is
wt
e itself. Unfortunately, we do not know how to show that using these
resistances, the number of iterations is bounded via the MWU method.
The issue is that one needs to be able to control the width in the MWU
method. In our case the width is the maximum ﬂow across an edge at
any time. Instead, we consider the update where the resistances are set
using the following equation:
rt
e = wt
e +
ε
3m
�
e
wt
e.
Thus, the resistance of an edge at time t has a local component (wt
and a global component (roughly ε times the average weight). Intu-e)
itively, the global component helps reduce the gap between the local
resistance and the average global resistance, thus, reducing the possi-
bility of a large current ﬂowing one edge compared to the rest of the

12.3 s,t-MaxFlow
79
Algorithm 12.1 ElecFlow
Input: G(V,E), source s, sink t, a target ﬂow value F and 0 < ε < 1
Output: Either an s,t-ﬂow f of value at least (1 − O(ε))F or FAIL
indicating that F > F ⋆
1: w0
e ← 1 for all e ∈ E
2: ρ ← 2� m
ε
3: T ← ρlogm
ε2
4: for t = 0 → T − 1 do
5:
∀e ∈ E, rt
e ← wt
e +
ε
3m
�
e wt
e
6:
ft def
= R−1
t BL+
t F(es − et)
7:
if Ert(ft) > (1 + ε
3)�
e rt
e then
8:
return FAIL
9:
else
10:
∀e ∈ E, wt+1
e
← wt
e(1 + ε|ft
e|
ρ )
11:
end if
12: end for
13: return f def
=
(1−ε)
(1+2ε) · 1
T · �T−1
t=0 ft
graph. This allows us to bound the width to ρ def
= O(
�
m/ε) and the
average congestion w.r.t. wt
es of the ﬂow to (1 + ε). Hence, the num-
ber of iterations is ˜O(
√m/ε5/2). Concretely, the upper bound on the
width allows one to almost reverse the inequality 1 + x ≤ ex, and its
use is demonstrated in Lemma 12.4. The key to this algorithm is the
update step and the width calculation. Once the resistances are set,
the corresponding electrical ﬂow ft is computed and the weights then
increase by a multiplicative factor of (1 + ε|ft
e|/ρ). Thus, the higher the
congestion, the higher the increase in resistance. Finally, the ﬂow ft
is computed using the Laplacian solver, see Theorem 11.1. It is an
approximate ﬂow, but we ignore this issue in the interest of readability
and leave it as exercise to adapt the proof in the approximate set-
ting. Assuming we can do so in ˜O(m) time, the overall running time is
˜O(m
3/2/ε
5/2). We also show in Section 12.4 that if the algorithm outputs
FAIL, we can recover an s,t cut of value at most F in additional linear
time.

80
Cuts and Flows
12.3.1
General Capacities
For general ces, the rt
e update rule is changed to rt
e ← 1
c2e (wt
e +
ε
3m
�
e wt
e) and |ft
e| is replaced by |ft
e|/ce. Note that the new bound for
the energy is Er(f) ≤ �
e ref2
e/c2
e. It is easy to verify that these changes
make the algorithm work for general capacities.
12.3.2
Analysis
Since we do not know F ⋆, the maximum ﬂow from s to t, we ﬁrst need
to show that the number of Fs we may need to run is bounded by
poly(logm, 1/ε). This can be done by binary search and is left as an
exercise. Next, we need to show the following for any F for which the
algorithm terminates but does not output FAIL:
• The ﬂow value from s to t is at least (1 − O(ε))F; and
• the output ﬂow does not violate any capacity constraints,
i.e., ∀e,|fe| ≤ (1−ε)
(1+2ε) · 1
T · �T−1
t=0 |ft
e| ≤ ce = 1.
The ﬁrst is easily seen as the ﬂow output by the algorithm is essentially
an average of T ﬂows, each pushing F units of ﬂow from s to t. Recall
that we showed in Equation (12.1) that when the algorithm ﬁnds a
ﬂow ft such that Ert(ft) > (1 + ε
3)�
e rt
e, it implies that the s,t-max-
ﬂow is less than F. Hence, when ElecFlow returns FAIL, F ⋆ < F. On
the other hand, when Ert(ft) ≤ (1 + ε
3)�
e rt
e we can show that, in ft,
the maximum congestion on an edge is bounded above and average
congestion is small.
Lemma 12.3. If Ert(ft) ≤ (1 + ε
3)�
e rt
e, then
• maxe |ft
e| ≤ 2�m
ε , and
•
�
e wt
e|ft
e|
�
e wte
≤ 1 + ε.
Proof. For convenience, we drop the superscript t in this proof. The
hypothesis then implies that
Er(f) =
�
e
ref2
e ≤
�
1 + ε
3
��
e
re.

12.3 s,t-MaxFlow
81
Hence, using the update rule for re from the algorithm, we get that
�
e ref2
e is at most
�
1 + ε
3
��
e
�
we +
ε
3m
�
e
we
�
=
�
1 + ε
3
�2 �
e
we ≤ (1 + ε)
�
e
we,
where the last inequality follows from the fact that ε < 1. On the other
hand,
�
e
ref2
e =
�
e
�
we +
ε
3m
�
e
we
�
f2
e .
(12.2)
Combining the two, we obtain that
∀e,
εf2
e
3m
�
e
we ≤
�
1 + ε
3
��
e
we,
which implies that |fe| ≤
�
3(1+ε/3)m
ε
≤ 2
�
m/ε for all e. Using the
Cauchy–Schwarz inequality on √we and √we|fe|, we get
��
e
we|fe|
�2
≤
��
e
we
���
e
wef2
e
�
,
which implies that
(�
e we|fe|)2
�
e we
≤
�
e
wef2
e ≤ (1 + ε)
�
e
we.
The last inequality follows from Equation (12.2) by taking the ﬁrst
term of summation. Hence,
�
�e we|fe|
e we
≤
√
1 + ε ≤ 1 + ε.
To track progress, we use the potential function Φt
def
= �
e wt
e. Clearly,
Φ0 = m and
Φt+1 =
�
e
wt+1
e
=
�
e
wt
e
�
1 + ε|ft
e|
ρ
�
=
�
e
wt
e + ε
ρ
�
e
wt
e|ft
e|,

82
Cuts and Flows
where ρ = 2
�
m/ε. Thus, using Lemma 12.3 we get that Φt+1 is at most
�
e
wt
e + ε(1 + ε)�
e wt
e
ρ
= Φt
�
1 + ε(1 + ε)
ρ
�
≤ m
�
1 + ε(1 + ε)
ρ
�t+1
,
by repeating this argument. Hence, we obtain the following lemma.
Lemma 12.4. ΦT−1 ≤ m
�
1 + ε(1+ε)
ρ
�T−1
≤ me
ε(1+ε)T
ρ
.
In the other direction,
∀e, wT−1
e
= ΠT−1
t=0
�
1 + ε|ft
e|
ρ
�
≤
�
e
wT−1
e
= ΦT−1.
Using the inequality 1 + εx ≥ eε(1−ε)x for all x ∈ [0,1], and 0 < ε < 1,
we get that
ΠT−1
t=0
�
1 + ε|ft
e|
ρ
�
≥ ΠT−1
t=0 e
(1−ε)ε|fte|
ρ
.
Combining the last two inequalities we obtain that for all e,
ΦT−1 ≥ wT−1
e
≥ ΠT−1
t=0 e
(1−ε)ε|fte|
ρ
= e
(1−ε)ε�
t |fte|
ρ
.
Now, using Lemma 12.4, we get that
e
(1−ε)ε�
t |fte|
ρ
≤ me
ε(1+ε)T
ρ
= e
ε(1+ε)T
ρ
+logm,
which implies that
(1 − ε)ε�
t ft
e
ρ
≤ ε(1 + ε)T
ρ
+ logm.
Dividing by Tε and multiplying by ρ, we get
(1 − ε)�
t |ft
e|
T − 1
≤ (1 + ε) + ρlogm
Tε
,

12.4 s,t-Min Cut
83
and by setting ρlogm
Tε
≤ ε, we get that for T ≥ ρlogm
ε2
,
(1 − ε)�
t |ft
e|
(1 + 2ε)T
≤ 1.
Therefore,
ρlogm
ε2
iterations suﬃce. This concludes the proof of
Theorem 12.1.
Note that the number of iterations of the algorithm depends on
the value of ρ. An immediate question is: Can it be decreased? The
following example shows that the width ρ = √m is tight.
Example 12.5. Consider a graph composed of k + 1 disjoint paths
between s and t; k paths of length k and one path of length 1. Hence,
the total number of edges is m = k2 + 1 and the max-ﬂow in this graph
is k + 1. The electrical ﬂow of value k + 1 sends k+1
2
ﬂow through the
edge of the length 1 path, which is approximately √m.
12.4
s,t-Min Cut
In this section we show how to complete the proof of Theorem 12.2.
The algorithm is the same as ElecFlow except now if a valid ﬂow is
returned for some F, we know that the minimum cut separating s and
t is more than F. On the other hand, if for some value F ElecFlow
returns FAIL, we show how to recover an s,t-cut of value (1 + O(ε))F,
thus completing the proof of Theorem 12.2.
Note that the vector of potentials used for setting r when
ElecFlow outputs FAIL is given by v def
= L+F(es − vt). Here, L+
corresponds to the resistances re. We again ignore the issue that v
is known only approximately. Hence, the cut we recover is in fact less
than F. The vector v gives an embedding of the vertex set V on the real
line. First, note that s and t are the two endpoints of this embedding.
Proposition 12.6. Given the embedding of V into R given by v as
above, max
i
vi = vs and min
i
vi = vt.
This is an easy exercise and can be proved using the fact that a har-
monic function achieves its maxima and minima at the boundary. If we

84
Cuts and Flows
pick a value uniformly at random from the interval [vt,vs], then the
probability that an edge e = ij is cut is |vi−vj|
|vs−vt|. Let Xe be indicator
random variable that is 1 if e is cut, and 0 otherwise. The expectation
of Xe is E[Xe] = P[Xe = 1] = |vi−vj|
|vs−vt|. Using linearity of expectation, we
get that
E
��
e
Xe
�
=
�
e
E[Xe] =
�
e=ij |vi − vj|
|vs − vt|
.
Thus, using Cauchy–Schwarz inequality on |vi−vj|
√re
and √re, we get
�
e=ij |vi − vj|
|vs − vt|
≤
1
|vs − vt|
�
�
�
�
�
e=ij
(vi − vj)2
re
��
e
re.
Now using the fact that Er(f) = �
e=ij
(vi−vj)2
re
and also Er(f) =
F|vs − vt|, we get that the above is at most
F
Er(f)
�
Er(f)
��
e
re = F
��
e re
Er(f).
Since f is such that Er(f) > �
e re, ﬁnally we get E[�
e Xe] < F; i.e.,
the expected value of cut is at most F. Therefore, one of the sweep cuts
that separates s and t has to be less than the expected value. Given
the vector v, the running time of this algorithm is ˜O(m). Thus, for a
given value of F, either ElecFlow returns a valid s,t-ﬂow of value at
least (1 − O(ε))F or the algorithm in this section ﬁnds a cut of value
at most F. The running time is dominated by that of ElecFlow. This
completes the proof of Theorem 12.2.
Notes
The max-ﬂow/min-cut theorem dates back to at least [26, 29], and a
proof of it can be found in any text book on algorithms, see [5].
Theorems 12.1 and 12.2 are from [22]. In fact, there the power in the
running time is 4/3 as opposed to the 3/2 presented in this section. Using
Laplacian solvers to speed up interior point methods to solve the linear
program for the max-ﬂow problem, Daitch and Spielman [24] obtained

12.4 s,t-Min Cut
85
the 3/2 result, which preceded the work of Christiano et al. [22]. The
techniques in this section have been extended by Kelner et al. [46] to
obtain algorithms for multicommodity ﬂows for a constant number of
commodities.
Two challenging problems remain: To decrease the dependence on
the error ε from poly 1/ε to polylog 1/ε; and to improve the running time
to ˜O(m) with possibly worse dependence on ε.
The multiplicative weight update paradigm has had a remarkable
success in a wide variety of areas. A recent survey of Arora et al. [11]
should give the interested reader a good idea of its usefulness.

Part III
Tools

13
Cholesky Decomposition Based Linear Solvers
The remainder of this monograph is concerned with methods to solve a
system of linear equations, Ax = b where A is an n × n matrix and b is
a vector in the column space of A. This section reviews an exact method
for solving a linear system of equations based on Cholesky decomposi-
tion. Subsequently, the Cholesky-based method is employed to present
an O(n) time algorithm for solving a linear system of equations Lx = b
when L is a Laplacian of a tree.
13.1
Cholesky Decomposition
We are concerned with matrices which are symmetric and PSD.
A matrix A is lower (respectively, upper) triangular if Aij = 0 whenever
i < j (respectively, i > j). Observe that if A is either lower or upper
triangular, then Ax = b can be solved by back-substitution with O(m)
arithmetic operations, where m is the number of nonzero entries of A.
Equivalently, A+b can be evaluated using O(m) arithmetic operations.
The undergraduate textbook algorithm to solve a system of linear equa-
tions is Gaussian elimination which performs row operations on the
matrix A to convert it to a lower or upper triangular matrix. When A
87

88
Cholesky Decomposition Based Linear Solvers
is symmetric and PSD, a more systematic way of solving Ax = b is via
its Cholesky decomposition, which is a modiﬁcation of Gaussian elimina-
tion. We present an algorithm to compute the Cholesky decomposition
of A when A is positive deﬁnite. There are appropriate generalizations
when A is not invertible. However, we will not cover these generaliza-
tions since in order to solve a Laplacian system of a connected graph
Lx = b, the vector b is orthogonal to the all-ones vector. In this setting
L+ ≻ 0.
Theorem 13.1. If A ≻ 0, then one can write A = ΛΛ⊤ where Λ is a
lower triangular matrix. Such a decomposition is called the Cholesky
decomposition of A.
Before we proceed with the proof of this theorem, we need the following
lemma, often referred to as Schur’s lemma.
Lemma 13.2. A =
�d1
u⊤
1
u1
B1
�
≻ 0 iﬀ
d1 > 0
and
B1 − u1u⊤
1/d1 ≻ 0.
Proof. Since A ≻ 0, d1 > 0. Consider minimizing the quadratic form
z2d1 + 2zu⊤
1 y + y⊤B1y over the variable z for a ﬁxed y. The solution
can be seen, by diﬀerentiating, to be z = −u⊤
1 y/d1. The minimum value
then is y⊤(B1 − u1u⊤
1/d1)y. Hence, if A ≻ 0, then for all y, y⊤(B1 −
u1u⊤
1/d1)y > 0. This implies that B1 − u1u⊤
1/d1 ≻ 0. The other direction
is straightforward.
Proof. [of Theorem 13.1] Since A ≻ 0, it suﬃces to express A = Λ∆Λ⊤
for a diagonal matrix ∆. Positive deﬁniteness implies that ∆ii > 0
and
thus
the
decomposition
in
the
theorem
is
obtained
via
A = (Λ∆
1/2)(Λ∆
1/2)⊤. For a symmetric, positive-deﬁnite matrix A, we
write it as
�d1
u⊤
1
u1
B1
�
=
� 1
0⊤
u1/d1
In−1
��d1
0⊤
0
B1 − u1u⊤
1/d1
��1
u⊤
1/d1
0
In−1
�
.

13.2 Fast Solvers for Tree Systems
89
Let the matrix in the middle of the r.h.s. be called A1. Note that the ﬁrst
matrix is a lower triangular matrix Λ1 and the third is Λ⊤
1 . Lemma 13.2
implies that B def
= B1 − u1u⊤
1/d1 ≻ 0 which gives A1 ≻ 0. Recursively, we
can write B = Λ′∆′Λ′⊤ as the product of (n − 1) × (n − 1) matrices,
and
A1 =
�1
0⊤
0
Λ′
��d1
0⊤
0
∆′
��1
0⊤
0
Λ′⊤
�
def
= Λ′′∆Λ′′⊤.
Thus, A = Λ1Λ′′∆Λ′′⊤Λ⊤
1 . Since product of lower triangular matrices
is lower triangular, the theorem follows.
Given the Cholesky decomposition, one can solve Ax = b easily. Indeed,
one ﬁrst evaluates b′ = Λ+b and then evaluates x = (Λ⊤)+b′. Note
that the number of such operations is of the order of nonzero entries
in Λ. In the method to compute the Cholesky decomposition of A
in the proof above, the choice of the ﬁrst row was arbitrary. In fact,
any ordering of the rows (and the same ordering of the columns) of
A can be used to recover a decomposition which results in a faster
algorithm to solve Ax = b. Formally, the Cholesky decomposition of A
and Q⊤AQ, (i.e., A with its rows and columns permuted by the same
permutation matrix Q,) can be very diﬀerent.1 Therefore, the number
of nonzero entries in the Cholesky decomposition of a matrix, called the
ﬁll in, depends on the permutation of rows and columns. Finding the
permutation which leads to the minimum ﬁll in is known to be NP-hard.
13.2
Fast Solvers for Tree Systems
Now we demonstrate how to use the combinatorial structure of the
linear system to get a good Cholesky decomposition. This results in a
fast solver for LT x = b when T is a tree. Any symmetric matrix A can
be associated with an n vertex graph (with self-loops) where we have
an edge ij of weight A(i,j) between vertices i and j. The number of
edges in the graph is precisely the number of nonzero entries in A. Let
us now view the Cholesky decomposition as described in the proof of
1 An n × n matrix Q is said to be a permutation matrix if all its entries are either 0 or 1
and there is exactly one 1 in each row and in each column.

90
Cholesky Decomposition Based Linear Solvers
Theorem 13.1 as a process which modiﬁes the graph: After the row i
has been processed, the resulting graph (which corresponds to A1) has
the following modiﬁcations:
(1) All edges ij with j ̸= i are deleted; this corresponds to setting
A1(i,j) = 0; and
(2) for every pair jk (j could be equal to k) neighboring to i a
(potentially new) edge is modiﬁed; this corresponds to setting
A1(j,k) = B1(j,k) − A(i,j)A(i,k)
A(i,i)
.
Now suppose the graph corresponding to the linear system as described
above is a tree and potentially self-loops. Then in every iteration of the
Cholesky decomposition, we can choose a leaf node i. Since there is a
single node adjacent to i, the graph associated with the matrix A1 is
a tree as well (potentially with an extra self-loop). In particular, this
implies that we can write A as Λ∆Λ⊤ where Λ = Λ1Λ2 ···Λn and each
Λi is a lower triangular matrix with at most one nonzero oﬀ-diagonal
entry. This gives a Cholesky decomposition with at most O(n) nonzero
entries. Moreover, in the computation of the Cholesky decomposition,
in each iteration, at most O(1) operations are done. Therefore, the
decomposition can be computed in O(n) time. Thus, we have proved
the following theorem.
Theorem 13.3. Given a symmetric, PSD matrix A and a vector b
such that the graph of A corresponds to a tree, one can ﬁnd in O(n)
time a permutation matrix Q such that the Cholesky decomposition of
Q⊤AQ has at most O(n) nonzero entries.
This immediately implies the following corollary which will be used in
Section 17.
Corollary 13.4. If LT is the Laplacian of a tree T and b is a vector
such that ⟨b,1⟩ = 0, then the solution of LT x = b can found in O(n)
time.
Proof. Note that the graph associated with the Laplacian of a tree is
the tree itself. Therefore, using Theorem 13.3, we can ﬁnd the Cholesky

13.2 Fast Solvers for Tree Systems
91
decomposition of the permuted Laplacian to get ΛΛ⊤y = b′ in O(n)
time. Here, y def
= Q⊤x and b′ def
= Q⊤b. Note that Λ is not full rank
since LT is not, however, ⟨b,1⟩ = 0 implies that b (and thus b′) is in
the column space of LT Q. Therefore, we are guaranteed a solution x
and this can be calculated in the number of nonzero entries of LT ,
which is also O(n).
Notes
More on Cholesky decomposition of factorization can be found in the
book [35]. George [33] pioneered the use of ﬁnding the right ordering
in the Cholesky decomposition for special graphs, e.g., square grids.
He showed that any PSD matrix whose graph is an n × n grid can
be solved in O(n1.5) time. This is the nested dissection algorithm.
Lipton et al. [52] generalized this result to planar graphs by showing
the existence of O(√n)-sized separators in planar graphs. This gives
an algorithm that runs in ˜O(n1.5) time and, in general, gives an algo-
rithm that runs roughly in time n1+ε for a family of graphs which have
separators of size nε and are closed under edge removal.

14
Iterative Linear Solvers I
The Kaczmarz Method
An old and simple iterative method to solve Ax = b, due to Kaczmarz,
starts with an arbitrary point, ﬁnds an equation that is not satisﬁed,
forces the current solution to satisfy it, and repeats. In this section, the
convergence of a randomized variant of the Kaczmarz is established.
Subsequently, it is shown how this method has been recently employed
to develop a new and simpler algorithm to solve Laplacian systems in
approximately ˜O(m) time.
14.1
A Randomized Kaczmarz Method
Let Ax = b be a system of equations where A is an m × n matrix and
x⋆ is one of its solutions. It is convenient to think of the solution x⋆ as
lying on all the hyperplanes
⟨ai,x⋆⟩ = bi,
where ai is the i-th row of A and bi is the corresponding entry of b.
Thus, starting with an arbitrary initial point x0, consider the following
92

14.1 A Randomized Kaczmarz Method
93
simple iterative algorithm to compute an approximation to x⋆: Given xt
such that Axt ̸= b, choose any hyperplane ⟨ai,x⟩ = bi which does not
contain xt and let xt+1 be the orthogonal projection of xt onto this
hyperplane. It can be shown that this method will converge to x⋆ as
long as every hyperplane is considered an inﬁnite number of times. For
instance, one could cycle over the hyperplanes in a ﬁxed order. However,
the convergence to x⋆ can be very slow and bounds on the rate of con-
vergence for general A exist for any deterministic hyperplane selection
rule. This motivates the use of randomization and we consider a simple
randomized rule called the Randomized Kaczmarz method: Set x0 = 0.
For t ≥ 0, given xt such that Axt ̸= b, choose a hyperplane ⟨ai,x⟩ = bi
with probability proportional to ∥ai∥2 and let xt+1 be the orthogonal
projection of xt onto this hyperplane. The rate of convergence of this
algorithm turns out to be related to a notion of an average condition
number deﬁned as follows.
Deﬁnition 14.1. Given an m × n matrix A, let ∥A+∥ be deﬁned as
follows
∥A+∥2 def
=
sup
z∈Rn, Az̸=0
∥z∥2
∥Az∥2 .
Recall that ∥A∥2
F is the squared-Frobenius norm of A which is deﬁned
to be the sum of squares of all the entries of A. The average condition
number is deﬁned to be
˜κ(A) def
= ∥A∥F ∥A+∥.
Theorem 14.1. The Randomized Kaczmarz method presented above,
starting with x0 = 0, satisﬁes
∥xτ − x⋆∥2 ≤ ε2∥x⋆∥2
for τ = O(˜κ2(A)log 1/ε) with probability at least 0.9.

94
Iterative Linear Solvers I The Kaczmarz Method
14.2
Convergence in Terms of Average Condition Number
The proof of Theorem 14.1 follows from the following lemma.
Lemma 14.2.
Fix the choice of the hyperplanes up to t and
let ⟨ai,x⟩ = bi be the random hyperplane selected with probabil-
ity ∥ai∥2/∥A∥2
F. If xt+1 is the orthogonal projection of xt on to this
hyperplane,
E∥xt+1 − x⋆∥2 ≤ (1 − 1/˜κ2(A))∥xt − x⋆∥2.
As a ﬁrst step in the proof of this lemma, let us calculate xt+1 in
terms of the hyperplane and xt. The unit normal of the hyperplane
⟨ai,x⟩ = bi is ˆai
def
=
ai
∥ai∥. Since xt+1 is an orthogonal projection of xt
onto this hyperplane, there is some γ ≥ 0 such that
xt − xt+1 = γ · ˆai.
Since xt+1 lies on the hyperplane, we know that
⟨ai,xt+1⟩ = bi = ⟨ai,x⋆⟩.
Thus, γ = ⟨ai,xt−x⋆⟩
∥ai∥
, implying that
xt+1 = xt − ⟨ai,xt − x⋆⟩
∥ai∥2
ai.
(14.1)
Since xt+1 and x⋆ lie in the hyperplane, ⟨ai,xt+1 − x⋆⟩ = 0, which
implies that xt+1 − x⋆ is orthogonal to xt+1 − xt. Thus, by Pythagoras
Theorem,
∥xt+1 − x⋆∥2 = ∥xt − x⋆∥2 − ∥xt+1 − xt∥2.
(14.2)
Since the hyperplane ⟨ai,x⟩ = bi was chosen with probability
∥ai∥2
∥A∥2
F ,
combining Equations (14.1) and (14.2), we obtain that
E∥xt+1 − x⋆∥2 = ∥xt − x⋆∥2 −
�
i
∥ai∥2
∥A∥2
F
|⟨ai,xt − x⋆⟩|2
∥ai∥4
∥ai∥2
= ∥xt − x⋆∥2
�
1 −
1
∥A∥2
F
�
i
|⟨ai,xt − x⋆⟩|2
∥xt − x⋆∥2
�

14.2 Convergence in Terms of Average Condition Number
95
= ∥xt − x⋆∥2
�
1 −
1
∥A∥2
F
∥A(xt − x⋆)∥2
∥xt − x⋆∥2
�
Defn.14.1
≤
∥xt − x⋆∥2
�
1 −
1
∥A∥2
F ∥A+∥2
�
= ∥xt − x⋆∥2
�
1 −
1
˜κ2(A)
�
.
This completes the proof of the Lemma 14.2. By repeated application
of this lemma we obtain that for t ≥ 0,
E∥xt+1 − x⋆∥2 ≤ (1 − 1/˜κ2(A))t∥x0 − x⋆∥2 = (1 − 1/˜κ2(A))t∥x⋆∥2.
Thus, from Markov’s Inequality, with probability at least 1 − 1/10,
∥xt+1 − x⋆∥2 ≤ 10(1 − 1/˜κ2(A))t∥x⋆∥2.
(14.3)
Hence, by selecting τ = O(˜κ2(A)log 1/ε), we obtain that
∥xτ − x⋆∥2 ≤ ε2∥x⋆∥2
with probability at least 0.9, completing the proof of Theorem 14.1.
The Kaczmarz method restricted to a subspace.
Note that the
Randomized Kaczmarz method and Theorem 14.1 above can be easily
extended to the case when the optimal solution x⋆ is known to satisfy
Ux⋆ = v for some U and v, and A is such that UA⊤ = 0. This will be
useful in the next section. In this setting, the Randomized Kaczmarz
method maintains that Uxt = v for all t. Thus, we ﬁrst need an initial
choice x0 s.t. Ux0 = v. As a consequence, x0 may no longer be 0 and,
hence, in Equation (14.3) we can no longer replace ∥x0 − x⋆∥ by ∥x⋆∥.
Further, Ux0 − Uxt = 0 for all t. Thus, a careful look at the proof of
Theorem 14.1 suggests that the following deﬁnition of ∥A+∥ suﬃces.
∥A+∥2 def
=
sup
z∈Rn, Az̸=0, Uz=0
∥z∥2
∥Az∥2 .
This can be smaller, giving rise to the possibility that ˜κ(A) in this
setting is smaller, which in turn could mean a reduced number of
iterations. Without any additional eﬀort, an analysis identical to that
of Theorem 14.1 shows that after τ ∼ ˜κ2(A)log 1/ε iterations, w.h.p.
∥xτ − x⋆∥2 ≤ ε2∥x0 − x⋆∥2.

96
Iterative Linear Solvers I The Kaczmarz Method
14.3
Toward an ˜
O(m)-Time Laplacian Solver
In this section we illustrate how the Randomized Karcmarz method can
be applied to a Laplacian system. We focus on presenting the algorithm
and derive a bound on the number of iterations. This approach has
recently led to a new algorithm and a simpler proof of the main theorem
(Theorem 3.1) in this monograph.
For this section let us focus on the case of computing the ﬂow
through each edge when one unit of current is pumped from s to t
and the edges have unit resistances. Such a computational primitive
was required in Section 12. It is an easy exercise to extend this argu-
ment to solve Lx = b when b is not necessarily es − et. To set things
up, let G = (V,E) be an undirected, unweighted graph on n vertices
and m edges, and let B ∈ {−1,0,1}n×m be an edge-vertex incidence
matrix corresponding to a ﬁxed orientation of the edges. Recall from
Section 4 the unit s,t ﬂow vector i = B⊤L+(es − et). Given i, it is easy
to compute L+(es − et) in O(m) time. Thus, let us focus on computing
an approximation to i.
For an undirected cycle C in G, let 1C ∈ {−1,0,1}m be a vector
supported exactly on the edges of C that satisﬁes B1C = 0. Since the
sum of voltages across any cycle sum up to zero if all resistances are
one, ⟨1C,i⟩ = 0 for all cycles C in G. Fix a spanning tree T in G and,
for every e ̸∈ T, let Ce be the unique cycle formed by adding e to T.
The indicator vectors for these m − n + 1 cycles form a basis of the
space of cycle vectors and can be used to generate the indicator vector
of any cycle in G. Hence, to ﬁnd i it is suﬃcient to solve the following
system of equations
∀e ̸∈ T
⟨1Ce,f⟩ = 0
s.t. Bf = es − et.
Let A be an m − n + 1 × m matrix whose rows are indexed by edges
e ̸∈ T and are equal to 1Ce. Then the problem reduces to solving Af = 0
subject to Bf = es − et. We will apply the Randomized Karcmarz
method to the system Af = 0 while working in the space Bf = es − et:
Initially choose f0 such that Bf0 = es − et. One such choice is to put a
unit ﬂow from s to t on the unique path from s to t in T. At time t,
given ft, compute ft+1 by orthogonally projecting ft onto the hyperplane

14.3 Toward an ˜O(m)-Time Laplacian Solver
97
⟨1C,f⟩ = 0, where C is chosen from one of the m − n + 1 cycles with
probability proportional to ∥1C∥2 = |C|. The number of iterations nec-
essary to get within ε of the optimal solution, from Theorem 14.1, is
O(˜κ2(A)log 1/ε).
In the remainder of the section we calculate ˜κ2(A). Recall that
˜κ2(A) = ∥A∥2
F ∥A+∥2. Fixing some T, ∥A∥2
F equals
�
e̸∈T
∥1Ce∥2 =
�
e̸∈T
|C| = m − n + 1 +
�
e̸∈T
strT (e),
where strT (e) is the stretch of e in T deﬁned as the length of the unique
path in T between the endpoints of e.1 On the other hand,
∥A+∥2 =
sup
f, Af̸=0, Bf=0
∥f∥2
∥Af∥2 .
We will now show that this is at most 1.
Let fT denote the entries of f corresponding to the tree edges and
fN denote those corresponding to the non-tree edges. Similarly, we can
split the columns of A into those corresponding to tree edges and non-
tree edges. It follows that one can write A = [AT IN], where IN is a
diagonal matrix on the non-tree edges, if the direction of each cycle is
chosen to coincide with the edge that was added to form it from T.
Suppose f is supported only on one of the cycles determined by T, then
A⊤
T fN = fT . Thus, by linearity, the following holds for any vector f such
that Bf = 0,
A⊤
T fN = fT .
(14.4)
Note that ∥Af∥2 = ∥AT fT ∥2 + 2f⊤
T A⊤
T fN + ∥fN∥2. Thus, by (14.4),
∥Af∥2 = ∥AT fT ∥2 + ∥fT ∥2 + ∥fN∥2 ≥ ∥fT ∥2 + ∥fN∥2 = ∥f∥2.
This proves the claim that ˜κ2(A) ≤ m − n + 1 + �
e̸∈T strT (e). We will
see in Theorem 17.3 (without proof) that one can construct, in ˜O(m)
time, trees with �
e̸∈T strT (e) = ˜O(m). Since, ∥f0∥2 ≤ n as the initial
ﬂow is just on a path from s to t, applying Theorem 14.1 in the sub-
space restricted setting of the previous section, the number of iterations
1 This notion of stretch will play a central role in the proof of Theorem 3.1 presented in this
monograph and more on this appears in Section 17.

98
Iterative Linear Solvers I The Kaczmarz Method
required by the Randomized Kaczmarz method to get within ε can be
seen to be bounded by ˜O(mlog 1/ε)
In order to complete the proof of Theorem 3.1 the key issues that
remain are, given T, how to choose an edge not in T with probability
proportional to the length of the cycle it forms with T and how to
update ft+1 from ft. The former is a simple exercise. For the latter,
note that the updates involve simply adding or removing ﬂow along
cycles, which can be implemented in logarithmic time using a data
structure called link-cut trees. The details of this data structure and
how it is employed are beyond the scope of this monograph.
Notes
The Kaczmarz method appears in [43]. The Randomized Kaczmarz
procedure and its analysis appear in [83]. This analysis is tied to the
alternating projections framework [19], which also has many other algo-
rithmic applications. Section 14.3 is based on a recent result due to [47]
where details about the link-cut tree data structures can be found. It
is also shown in [47] that the inverter derived from the Randomized
Kaczmarz method is linear in the sense of Section 3.4.

15
Iterative Linear Solvers II
The Gradient Method
This section phrases the problem of solving a system of linear equations
as an optimization problem and uses the technique of gradient descent
to develop an iterative linear solver. Roughly, iterative methods to solve
a system Ax = b maintain a guess xt for the solution at an iteration t,
and update to a new guess xt+1 through feedback about how good their
guess was. The update typically uses a simple vector–matrix product
and is often very fast and popular in practice due to its small space
complexity.
15.1
Optimization View of Equation Solving
In this section we assume that A is symmetric and positive deﬁnite
and, as usual, we let the eigenvalues of A be 0 < λ1 ≤ ··· ≤ λn. To start,
observe that solving Ax = b is equivalent to ﬁnding the minimum of
f(x) where f is deﬁned to be
f(x) def
= 1
2x⊤Ax − bx.
Observe that when A is positive deﬁnite, ∇2f = A ≻ 0, so f is strictly
convex and, thus, has a unique minimum x⋆. This x⋆ must satisfy
∇f(x⋆) = Ax⋆ − b = 0.
99

100
Iterative Linear Solvers II The Gradient Method
Since f is a convex function, we can use the gradient descent
approach popular in convex optimization to solve for x⋆. A typical
gradient descent algorithm starts with an initial vector x0, and at iter-
ation t moves to a new point xt+1 in the direction opposite to the
gradient of f at xt. We would like to move in this direction because f
decreases along it. Note that ∇f(xt) = Axt − b which means that the
time it takes to compute the gradient requires a single multiplication
of a vector with a matrix A. We use tA to denote the time it takes to
multiply an n × n matrix A with a vector.1 The hope is that in a small
number of iterations, each of which is geared to reducing the function
value, the process will converge to x⋆. We prove the following theorem.
Theorem 15.1. There is an algorithm GDSolve that, given an n × n
symmetric matrix A ≻ 0, a vector b, and ε > 0, ﬁnds a vector x such
that
∥x − A+b∥A ≤ ε∥A+b∥A
in time O(tA · κ(A)log 1/ε). Here the condition number of A is deﬁned
to be κ(A) def
= λn(A)/λ1(A) and, for a vector v, ∥v∥A
def
=
√
v⊤Av.
15.2
The Gradient Descent-Based Solver
Before we describe the algorithm GDSolve formally and prove Theo-
rem 15.1, we provide a bit more intuition. First, let us ﬁx some notation.
For t ≥ 0, let dt
def
= x⋆ − xt be a vector which tells us where the cur-
rent solution xt is w.r.t. x⋆, and let rt
def
= Adt = b − Axt. Observe that
rt = −∇f(xt). Think of rt as a crude approximation to dt. At itera-
tion t, there is a parameter ηt which determines how much to move in
the direction of rt. We will discuss the choice of ηt shortly, but for now,
the new point is
xt+1
def
= xt + ηtrt.
(15.1)
1 tA is at least n and, even if A has m nonzero entries, can be much less than m, e.g., if
A = vv⊤ and is speciﬁed in this form.

15.2 The Gradient Descent-Based Solver
101
How does one choose ηt? One way is to choose it greedily: Given xt
(and therefore rt) choose ηt which minimizes f(xt+1). Towards this
end, consider the function
g(η) def
= f(xt + ηrt) = 1
2(xt + ηrt)⊤A(xt + ηrt) − b⊤(xt + ηrt).
The minimum is attained when the gradient of g w.r.t. η is 0. This can
be calculated easily:
g′(η) = r⊤
t Axt + ηr⊤
t Art − r⊤
t b,
which implies
ηt = r⊤
t (b − Axt)
r⊤
t Art
= r⊤
t rt
r⊤
t Art
.
GDSolve is described formally in Algorithm 15.1.
Algorithm 15.1 GDSolve
Input: Symmetric, positive-deﬁnite matrix A ∈ Rn×n, b ∈ Rn and T
Output: xT ∈ Rn
1: x0 ← 0
2: for t = 0 → T − 1 do
3:
Set rt = b − Axt
4:
Set ηt =
r⊤
t rt
r⊤
t Art
5:
Set xt+1 = xt + ηtrt
6: end for
7: return xT
Thus, to prove Theorem 15.1 the only thing one needs to show is that
for T = O(κ(A)log 1/ε), ∥xT − A+b∥A ≤ ε∥A+b∥A. Towards this, we
prove Lemma 15.2 which shows how the A-norm of the error vector
dt = xt − x⋆ drops with t.
Lemma 15.2. ∥dt+1∥2
A ≤ (1 − 1/κ(A)) · ∥dt∥2
A
Proof. We start with the following observation: From the update rule
we get dt+1 = dt − ηtrt and rt+1 = rt − ηtArt. This gives, r⊤
t rt+1 =
r⊤
t rt −
r⊤
t rt
r⊤
t Art r⊤
t Art = 0. Therefore, two consecutive update directions

102
Iterative Linear Solvers II The Gradient Method
rt and rt+1 are orthonormal. This is expected since from xt we move
in the direction rt as far as we can to minimize f. Now,
∥dt+1∥2
A = d⊤
t+1Adt+1 = d⊤
t+1rt+1 = (dt − ηtrt)⊤rt+1 = d⊤
t rt+1.
The last equality used the orthonormality of rt and rt+1. Thus,
∥dt+1∥2
A = d⊤
t rt+1 = d⊤
t Adt+1 = d⊤
t A(dt − ηtrt).
This is the same as
∥dt∥2
A ·
�
1 − ηt
d⊤
t Art
d⊤
t Adt
�
= ∥dt∥2
A ·
�
1 − r⊤
t rt
r⊤
t Art
· d⊤
t A2dt
d⊤
t Adt
�
.
In the last inequality, we use rt = Adt. Now, the ﬁrst fraction is at least
1/λn, while the second is at least λ1 since
d⊤
t A2dt
d⊤
t Adt
= (A
1/2dt)⊤A(A
1/2dt)
(A1/2dt)⊤(A1/2dt) ≥ min
x̸=0
x⊤Ax
x⊤x = λ1.
Here we is where we have used that A ≻ 0. This gives us
∥dt+1∥2
A ≤ (1 − 1/κ(A))∥dt∥2
A,
which completes the proof of the lemma.
As a corollary we get that in T = 2κ(A)log 1/ε steps, ∥xT − x⋆∥A ≤
ε∥x0 − x⋆∥A; in fact, with a better analysis the factor 2 can be removed.
This completes the proof of Theorem 15.1.
To eﬀectively use Theorem 15.1 one needs a more tractable condi-
tion to check when to terminate. It is easy to check that ∥Axt − b∥ ≤ δ.
For this termination condition to imply the termination condition in
the theorem, one has to choose δ small enough; this seems to require an
upper bound on the condition number of A. We leave it as an exercise
to check that for graph Laplacians, an easy estimate can be obtained
in terms of the ratio of the largest to the smallest edge weight. In the
next section we will see how to reduce the number of iterations to about
�
κ(A) when A is PSD. This will require a deeper look into GDSolve.
Notes
The method GDSolve suggested here is well known and bears simi-
larity with Richardson’s iterative method [64], see also [36, 37]. More
on gradient descent can be found in the book [19].

16
Iterative Linear Solvers III
The Conjugate Gradient Method
This section presents a sophisticated iterative technique called the
conjugate gradient method which is able to approximately solve a linear
system of equations in time that depends on the square root of the con-
dition number. It introduces the notion of Krylov subspace and shows
how Chebyshev polynomials play a crucial role in obtaining this square
root saving. Finally, the Chebyshev iteration method is presented which
is used in Section 18 to construct linear Laplacian solvers.
16.1
Krylov Subspace and A-Orthonormality
Given a symmetric and positive-deﬁnite matrix A, we start where we
left oﬀ in the previous section. Recall that for a vector b, we set up the
optimization problem
min
x f(x) def
= 1
2x⊤Ax − b⊤x.
We presented an algorithm GDSolve which, in its t-th iteration, gen-
erates xt where
x1 = x0 + η0r0,
x2 = x1 + η1r1 = x1 + η1(r0 − η0Ar0) = x0 + η1r0 − η1η0Ar0,
103

104
Iterative Linear Solvers III The Conjugate Gradient Method
and so on. Here r0 = b and ri = b − Axi. Thus, it follows by
induction that xt lies in x0 + Kt where Kt is the subspace spanned
by {r0,Ar0,...,At−1r0} = {b,Ab,...,At−1b}. Kt is called the Krylov
subspace of order t generated by A and b.
In GDSolve, although at each iteration t we move greedily along
the direction rt−1, the resulting point xt which lies in x0 + Kt is not
guaranteed to be a minimizer of f over this subspace. That is, there
could be a y ∈ x0 + Kt such that f(y) < f(xt). The main idea of the
conjugate gradient method can be summarized in one sentence:
At step t, move to the f-minimizer over x0 + Kt.
Note that the problem of ﬁnding x⋆ is precisely that of ﬁnding the
f-minimizer over x0 + Kn. In this section we will see how to make this
idea work and will prove the following theorem.
Theorem 16.1. There is an algorithm CGSolve that, given an n × n
symmetric matrix A ≻ 0, a vector b, and ε > 0, ﬁnds a vector x such
that
∥x − A+b∥A ≤ ε∥A+b∥A
in time O(tA ·
�
κ(A)log 1/ε).
The ﬁrst question we would like to ask is: How can we ﬁnd the f-
minimizer over x0 + Kt quickly? Suppose we have access to a basis
{p0,p1,...,pt−1} of Kt such that for any scalars β0,...,βt−1, we have
the following decomposition:
f
�
x0 +
t−1
�
i=0
βipi
�
− f(x0) =
t−1
�
i=0
(f(x0 + βipi) − f(x0)),
(16.1)
i.e., the optimization problem becomes separable in the variables βi.
Then, it is suﬃcient to ﬁnd, for all i, αi which is the α that
minimizes f(x0 + αpi) − f(x0): Observe that xt
def
= x0 + �t−1
i=1 αipi
minimizes f(y) for all y ∈ x0 + Kt. Therefore, if we have access to
a basis as described above, we are able to move to a minimizer over
x0 + Kt iteratively.

16.2 Computing the A-Orthonormal Basis
105
Consider Equation (16.1). If f were a linear function, then equality
clearly holds. Thus, we can just look at the quadratic portion of f to
decide when this equality holds. For brevity, let vi
def
= βipi. Evaluating
the l.h.s. of Equation (16.1) gives
1
2
�
x +
�
i
vi
�⊤
A
�
x +
�
i
vi
�
− 1
2x⊤Ax
= x⊤A
��
i
vi
�
+
��
i
vi
�⊤
A
��
i
vi
�
.
Evaluating the r.h.s. gives 1
2
�
i(x⊤Avi + v⊤
i Avi). The above equations
are equal if the cross terms v⊤
i Avj are equal to 0. This is precisely the
deﬁnition of A-orthonormal vectors.
Deﬁnition 16.1.
Given a symmetric matrix A, a set of vectors
p0,p1,...,pt are A-orthonormal iﬀ for all i ̸= j, p⊤
i Apj = 0.
Now we are armed to give an overview of the conjugate gradient
algorithm. Let x0 be the initial vector, and let r0
def
= A(x⋆ − x0).
Let p0,p1,...,pt be a set of A-orthonormal vectors spanning Kt =
span{r0,Ar0,...,At−1r0}. In the next section we will see how to com-
pute this A-orthonormal basis for Kt. In fact, we will compute the vec-
tor pt itself in the (t + 1)-st iteration taking O(1) extra matrix–vector
computations; for the time being, suppose they are given.
Let αt be scalars that minimize f(x0 + αpt) − f(x0); by a simple
calculation, we get αt =
p⊤
t r0
p⊤
t Apt . The conjugate gradient algorithm
updates the vectors as follows
xt+1
def
= xt + αtpt
and
αt
def
=
p⊤
t r0
p⊤
t Apt
.
(16.2)
16.2
Computing the A-Orthonormal Basis
We now show how to compute an A-orthonormal basis of {r0,Ar0,...,
At−1r0}. One could use Gram–Schmidt orthonormalization which

106
Iterative Linear Solvers III The Conjugate Gradient Method
iteratively computes p0,...,pt, by setting
pt+1
def
= v −
�
i≤t
v⊤Api
p⊤
i Api
pi,
where v is the new vector which one is trying to A-orthonormalize.
Given that p⊤
j Api = 0 for all i ̸= j ≤ t, this ensures that p⊤
t+1Api = 0
for all i ≤ t. Note that the above can take up to O(t) matrix–vector
calculations to compute pt+1. We now show that if one is trying to A-
orthonormalize the Krylov subspace generated by A, one can get away
by performing only O(1) matrix–vector computations. This relies on
the fact that A is symmetric.
Let p0 = r0. Suppose we have constructed vectors {p0,p1,...,pt}
which form an A-orthonormal basis for Kt+1. Inductively assume that
the vectors satisfy
Ki+1 = span{r0,Ar0,...,Air0} = span{p0,p1,...,pi}
(16.3)
and that Api ∈ Ki+2 for all i ≤ t − 1. Note that this is true when i = 0
as p0 = r0. Let us consider the vector Apt. If Apt ∈ Kt+1, Kj = Kt+1
for all j > t + 1 and we can stop. On the other hand, if Apt ̸∈ Kt+1, we
construct pt+1 by A-orthonormalizing it w.r.t. pi for all i ≤ t. Thus,
pt+1
def
= Apt −
�
i≤t
(Apt)⊤Api
p⊤
i Api
pi.
(16.4)
This way of picking pt+1 implies, from our assumption in Equa-
tion (16.3), that
Kt+2 = span{r0,Ar0,...,At+1r0} = span{p0,p1,...,pt+1}.
It also ensures that Apt can be written as a linear combination of pis for
i ≤ t + 1. Hence, Apt ∈ Kt+2, proving our induction hypothesis. Thus,
for every i ≤ t, there are constants cjs such that
(Apt)⊤(Api) = p⊤
t A⊤(Api) = p⊤
t A(Api) =
�
j≤i+1
cjp⊤
t Apj
Hence,
(Apt)⊤(Api) = 0

16.3 Analysis via Polynomial Minimization
107
for all i < t − 1. Hence, Equation (16.4) simpliﬁes to
pt+1 = Apt − p⊤
t A2pt
p⊤
t Apt
pt − p⊤
t A2pt−1
p⊤
t−1Apt−1
pt−1.
Thus, to compute pt+1 we need only O(1) matrix–vector multiplications
with A, as promised. This completes the description of the conjugate
gradient algorithm which appears formally below.
Algorithm 16.1 CGSolve
Input: Symmetric, positive-deﬁnite matrix A ∈ Rn×n, b ∈ Rn and T
Output: xT ∈ Rn
1: x0 ← 0
2: r0 ← b
3: p0 = r0
4: for t = 0 → T − 1 do
5:
Set αt =
p⊤
t r0
p⊤
t Apt
6:
Set rt = b − Axt
7:
Set xt+1 = xt + αtpt
8:
Set pt+1 = Apt − p⊤
t A2pt
p⊤
t Apt pt − p⊤
t A2pt−1
p⊤
t−1Apt−1 pt−1.
9: end for
10: return xT
Since each step of the conjugate gradient algorithm requires O(1)
matrix–vector multiplications, to analyze its running time, it suﬃces
to bound the number of iterations. In the next section, we analyze
the convergence time of the conjugate gradient algorithm. In partic-
ular, we call a point xt an ε-approximate solution if f(xt) − f(x⋆) ≤
ε(f(x0) − f(x⋆)). We wish to ﬁnd how many iterations the algorithm
needs to get to an ε-approximate solution. We end this section with
the following observation: In n steps, the conjugate gradient method
returns x⋆ exactly. This is because x⋆ ∈ x0 + Kn.
16.3
Analysis via Polynomial Minimization
We now utilize the fact that xt minimizes f over the subspace x0 + Kt
to prove an upper bound on f(xt) − f(x⋆). The following is easily seen.
f(xt) − f(x⋆) = 1
2(xt − x⋆)⊤A(xt − x⋆) = 1
2∥xt − x⋆∥2
A.

108
Iterative Linear Solvers III The Conjugate Gradient Method
Since xt lies in x0 + Kt, we can write xt = x0 + �t−1
i=0 γiAir0 for some
scalars γi. Let p(x) be the polynomial deﬁned to be �t−1
i=0 γixi. Note
that there is a one-to-one correspondence between points in x0 + Kt
and degree t − 1 polynomials in one variable. Then, we get xt = x0 +
p(A)r0 = x0 + p(A)A(x⋆ − x0). Therefore, we get
xt − x⋆ = (I − p(A)A)(x0 − x⋆) = q(A)(x0 − x⋆),
where q(x) def
= 1 − xp(x). Now, note that there is a one-to-one corre-
spondence between degree t − 1 polynomials and degree t polynomials
that evaluate to 1 at 0. Let this latter set of polynomials be Qt. Since
xt minimizes ∥xt − x⋆∥2
A over Kt, we get that f(xt) − f(x⋆) equals
1
2∥xt − x⋆∥2
A = min
q∈Qt (q(A)(x0 − x⋆))⊤ A(q(A)(x0 − x⋆)).
(16.5)
Before we proceed with the proof of Theorem 16.1, we need the follow-
ing lemma.
Lemma 16.2.
Let A be a symmetric matrix with eigenvalues
λ1,...,λn. Then, for any polynomial p(·) and vector v,
(p(A)v)⊤A(p(A)v) ≤ v⊤Av ·
n
max
i=1 |p(λi)|2.
Proof. Write A = UΓU⊤ where Γ is the diagonal matrix consisting of
eigenvalues and the columns of U are the corresponding orthonormal
eigenvectors. Note that p(A) = Up(Γ)U⊤. Now for any vector v, we get
v⊤p(A)⊤Ap(A)v = v⊤Up(Γ)U⊤UΓU⊤Up(Γ)U⊤v = v⊤UΓp2(Γ)U⊤v.
Thus, if v = �
j ζjuj where uj is the eigenvector of A corresponding
to λj, we get that the l.h.s. of the above equality is �
j ζ2
j λjp2(λj).
Similarly, v⊤Av = �
j ζ2
j λj. The lemma follows.
Plugging the above claim into Equation (16.5) we get the following
upper bound on f(xt) which is crux in the analysis of the conjugate
gradient algorithm:
f(xt) − f(x⋆) ≤ min
q∈Qt
n
max
i=1 |q(λi)|2 · (f(x0) − f(x⋆)).
(16.6)

16.3 Analysis via Polynomial Minimization
109
Note that, since the eigenvalues of A satisfy λ1 ≤ ··· ≤ λn, the r.h.s. of
Equation (16.6) can be further approximated by
f(xt) − f(x⋆) ≤ min
q∈Qt
max
x∈[λ1,λn]|q(x)|2 · (f(x0) − f(x⋆)).
(16.7)
Thus, since f(x0) = f(0) = 0 and f(x⋆) = −1/2 · ∥x⋆∥2
A, we have proved
the following lemma about CGSolve.
Lemma 16.3. Let A ≻ 0, λ1,λn be the smallest and the largest eigen-
values of A, respectively, and let Qt be the set of polynomials of degree
at most t which take the value 1 at 0. Then,
∥xt − x⋆∥2
A ≤ ∥x⋆∥2
A · min
q∈Qt
max
x∈[λ1,λn]|q(x)|2.
Therefore, any polynomial in Qt gives an upper bound on f(xt). In
particular, the polynomial q(x) def
= (1 −
2x
λ1+λn )t gives
f(xt) − f(x⋆) ≤
�κ − 1
κ + 1
�t
(f(x0) − f(x⋆)),
where κ = λn/λ1. This is slightly better than what we obtained in
Lemma 15.2. As a corollary of the discussion above, we again obtain
that after n iterations the solution computed is exact.
Corollary 16.4.
After n steps of CGSolve, xn = x⋆. Hence, in
O(tAn) time, one can compute x⋆ = A+b for a positive deﬁnite A.
Proof. Let λ1 ≤ ··· ≤ λn be the eigenvalues of A. Consider the polyno-
mial q(x) def
= �n
i=1(1 − x/λi). Note that q(0) = 1, and q(λi) = 0 for all
eigenvalues of A. From Lemma 16.3,
∥xn − x⋆∥2
A ≤ ∥x⋆∥2
A ·
n
max
i=1 |q(λi)|2 = 0.

110
Iterative Linear Solvers III The Conjugate Gradient Method
16.4
Chebyshev Polynomials — Why Conjugate
Gradient Works
In the previous section we reduced the problem of bounding the error
after t iterations to a problem about polynomials. In particular, the goal
reduced to ﬁnding a polynomial q(·) of degree at most t which takes the
value 1 at 0 and minimizes maxn
i=1 |q(λi)| where λis are eigenvalues of A.
In this section we present a family of polynomials, called Chebyshev
polynomials, and use them to prove Theorem 16.1.
For a nonnegative integer t, we will let Tt(x) denote the degree t
Chebyshev polynomial of the ﬁrst kind, which are deﬁned recursively
as follows: T0(x) def
= 1,T1(x) def
= x, and for t ≥ 2,
Tt(x) def
= 2xTt−1(x) − Tt−2(x).
The following is a simple consequence of the recursive deﬁnition above.
Proposition 16.5. If x ∈ [−1,1], then we can deﬁne θ def
= arccos(x),
and the degree t Chebyshev polynomial is given by Tt(cosθ) = cos(tθ).
Hence, Tt(x) ∈ [−1,1] for all x ∈ [−1,1].
Proof. First, note that T0(cosθ) = cos0 = 1 and T1(cosθ) = cosθ = x.
Additionally, cos((t + 1)θ) = cos(tθ)cos(θ) − sin(tθ)sin(θ) and simi-
larly cos((t − 1)θ) = cos(tθ)cos(θ) − sin(tθ)sin(θ). Hence, it is clear
that the recursive deﬁnition for Chebyshev polynomials, namely
cos((t + 1)θ) = 2cosθcos(tθ) + cos((t − 1)θ), applies.
1Thus, letting x = cosθ and using de Moivre’s formula, Tt(x) = cos(tθ) =
2(exp(itθ) + exp(−itθ)) can be written as
Tt(x) = 1
2((x +
�
x2 − 1)t + (x −
�
x2 − 1)t),
which is a polynomial of degree t. For 0 < a < b, we deﬁne the polyno-
mial Qa,b,t as follows:
Qa,b,t(x) def
=
Tt
�
a+b−2x
b−a
�
Tt
�
a+b
b−a
�
.

16.5 The Chebyshev Iteration
111
Note that Qa,b,t is of degree t and evaluates to 1 at x = 0 and hence
lies in Qt. Furthermore, for x ∈ [a,b], the numerator takes a value of
at most 1 (by the deﬁnition of Tt). Therefore, if we plug in a = λ1 and
b = λn, the smallest and the largest eigenvalues of A, respectively, we
obtain
∀x ∈ [λ1,λn],
Qλ1,λn,t(x) ≤ Tt
�λ1 + λn
λn − λ1
�−1
= Tt
� λn/λ1 + 1
λn/λ1 − 1
�−1
.
Since κ(A) = λn/λ1, and Tt( κ+1
κ−1) = 1
2((
√
k+1
√
k−1)t + (
√κ−1
√κ+1)t), we get
∀x ∈ [λ1,λn],
Qλ1,λn,t(x) ≤ 2
��
λn/λ1 − 1
�
λn/λ1 + 1
�t
.
(16.8)
Thus, by Lemma 16.3, for any t ≥ Ω (
�
κ(A)log 1/ε), after t steps of
CGSolve we get xt satisfying f(xt) − f(x⋆) ≤ ε2 · (f(x0) − f(x⋆)).
Thus, for this value of t,
∥xt − A+b∥A ≤ ε∥A+b∥A.
This completes the proof of Theorem 16.1.
16.5
The Chebyshev Iteration
In this section we consider the possibility of attaining a bound of
�
κ(A)log 1/ε iterations when xt = pt(A)b where pt is a sequence of
polynomials. When compared with CGSolve, this will have the advan-
tage that xt is a linear operator applied to b. As in the proof of
Lemma 16.3, it is suﬃcient to deﬁne pts such that
max
x∈[λ1,λn]|xpt(x) − 1| ≤ O(1 −
�
λ1/λn)t,
(16.9)
which gives us
∥xt − A+b∥2
A ≤ O(1 −
�
λ1/λn)t · ∥A+b∥2
A.
Thus, in O(
�
κ(A)log 1/ε) iterations xt is ε close to A+b. Note that
Equation (16.9) implies that, if we let Zt
def
= pt(A), then for all b,
∥Ztb − A+b∥A ≤ O(1 −
�
λ1/λn)t.

112
Iterative Linear Solvers III The Conjugate Gradient Method
Hence, if ∥Ztb − A+b∥A ≤ δ, then ∥Zt − A∥ ≤ δ · ∥A+∥
1/2. In fact, we
can set pt to be Qλ1,λn,t, deﬁned using the Chebyshev polynomials
in the previous section, and use Equation (16.8) to obtain the bound
in Equation (16.9). The only issue that remains is how to compute
Qλ1,λn,t(A)b in ˜O(tAt)-time. This can be done using the recursive deﬁ-
nition of the Chebyshev polynomials in the previous section and is left
as an exercise. The update rule starts by setting
x0
def
= 0
and
x1
def
= b,
and for t ≥ 2,
xt
def
= α2Axt−1 + α1xt−2 + α0b
for ﬁxed scalars α0,α1,α2 which depend on λ1 and λn. This is called
the Chebyshev iteration and, unlike CGSolve, requires the knowledge
of the smallest and the largest eigenvalues of A.1 We summarize the
discussion in this section in the following theorem.
Theorem 16.6. There is an algorithm which given an n × n sym-
metric positive-deﬁnite matrix A, a vector b, numbers λl ≤ λ1(A) and
λu ≥ λn(A) and an error parameter ε > 0, returns an x such that
(1) ∥x − A+b∥A ≤ ε · ∥A+b∥A,
(2) x = Zb where Z depends only on A and ε, and
(3) ∥Z − A+∥ ≤ ε,
where we have absorbed the ∥A+∥
1/2 term in the error. The algorithm
runs in (tA ·
�
λu/λl log 1/(ελl)) time.
16.6
Matrices with Clustered Eigenvalues
As another corollary of the characterization of Lemma 16.3 we show
that the rate of convergence of CGSlove is better if the eigenvalues
of A are clustered. This partly explains why CGSolve is attractive in
practice; it is used in Section 17 to construct fast Laplacian solvers.
1 For our application in Section 18, we will be able to provide estimates of these eigenvalues.

16.6 Matrices with Clustered Eigenvalues
113
Corollary 16.7. For a matrix A, suppose all but c eigenvalues are
contained in a range [a,b]. Then, after t ≥ c + O(
�
b/alog 1/ε) iterations,
∥xt − x⋆∥A ≤ ε∥x⋆∥A.
Proof. Let q(x) def
= Qa,b,i(x) · Πc
i=1(1 − x/λi) where λ1,...,λc are the c
eigenvalues outside of the interval [a,b], and Qa,b,i deﬁned earlier. From
Lemma 16.3, since the value of q(λi) = 0 for all i = 1,...,c, we only
need to consider the maximum value of |q(x)|2 in the range [a,b]. By the
properties of Qa,b,i described above, if we pick i = Θ(
�
b/alog 1/ε), for all
x ∈ [a,b], |q(x)|2 ≤ ε. Therefore, CGSolve returns an ε-approximation
in c + O(
�
b/alog 1/ε) steps; note that this could be much better than
�
κ(A) since no restriction is put on the c eigenvalues.
Notes
The conjugate gradient method was ﬁrst proposed in [39]. There is a
vast amount of literature on this algorithm and the reader is directed to
the work in [36, 37, 68, 72]. More on Chebyshev polynomials and their
centrality in approximation theory can be found in the books [21, 65].
An interested reader can check that a Chebyshev-like iterative method
with similar convergence guarantees can also be derived by applying
Nesterov’s accelerated gradient descent method to the convex function
in the beginning of this section.

17
Preconditioning for Laplacian Systems
In this section, the notion of preconditioning is introduced. Instead
of solving Ax = b, here one tries to solve PAx = Pb for a matrix P
such that κ(PA) ≪ κ(A) where it is not much slower to compute Pv
than Av, thus speeding up iterative methods such as the conjugate
gradient method. As an application, preconditioners are constructed
for Laplacian systems from low-stretch spanning trees that result in
a ˜O(m
4/3) time Laplacian solver. Preconditioning plays an important
role in the proof of Theorem 3.1 presented in Section 18. Low-stretch
spanning trees have also been used in Section 4.
17.1
Preconditioning
We saw in the previous section that using the conjugate gradient
method for a symmetric matrix A ≻ 0, one can solve a system of equa-
tions Ax = b in time O(tA ·
�
κ(A)log 1/ε) up to an error of ε. Let us
focus on the two quantities in this running time bound: tA which is the
time it takes to multiply a vector with A, and κ(A), the condition num-
ber of A. Note that the algorithm requires only restricted access to A:
Given a vector v, output Av. Thus, one strategy to reduce this running
114

17.1 Preconditioning
115
time is to ﬁnd a matrix P s.t. κ(PA) ≪ κ(A) and tP ∼ tA. Indeed, if
we had access to such a matrix, we could solve the equivalent system
of equations PAx = Pb.1 Such a matrix P is called a preconditioner
for A. One choice for P is A+. This reduces the condition number to
1 but reduces the problem of computing A+b to itself, rendering it
useless. Surprisingly, as we will see in this section, for a Laplacian sys-
tem one can often ﬁnd preconditioners by using the graph structure,
thereby reducing the running time signiﬁcantly. The following theorem
is the main result of this section.
Theorem 17.1. For any undirected, unweighted graph G with m
edges, a vector b with ⟨b,1⟩ = 0, and ε > 0, one can ﬁnd an x such
that ∥x − L+
Gb∥LG ≤ ε∥L+
Gb∥LG in ˜O(m
4/3 log 1/ε) time.
There are two crucial ingredients to the proof. The ﬁrst is the following
simple property of CGSolve.
Lemma 17.2. Suppose A is a symmetric positive-deﬁnite matrix with
minimum eigenvalue λ1 ≥ 1 and trace Tr(A) ≤ τ. Then CGSolve con-
verges to an ε-approximation in O(τ
1/3 log 1/ε) iterations.
Proof. Let Λ be the set of eigenvalues larger than τ/γ, where γ is
a parameter to be set later. Note that |Λ| ≤ γ. Therefore, apart
from these γ eigenvalues, all the rest lie in the range [1, τ/γ]. From
Corollary 16.7, we get that CGSolve ﬁnds an ε-approximation in
γ + O(
�
τ/γ log 1/ε) iterations. Choosing γ def
= τ
1/3 completes the proof
of the lemma.
The second ingredient, and the focus of this section, is a construction
of a combinatorial preconditioner for LG. The choice of preconditioner
is L+
T where T is a spanning tree of G. Note that from Corollary 13.4,
this preconditioner satisﬁes the property that L+
T v can be computed in
O(n) time. Thus, the thing we need to worry about is how to construct
a spanning tree T of G such that κ(L+
T LG) is as small as possible.
1 One might worry that the matrix PA may not be symmetric; one normally gets around
this by preconditioning by P 1/2AP 1/2. This requires P ≻ 0.

116
Preconditioning for Laplacian Systems
17.2
Combinatorial Preconditioning via Trees
Let G be an unweighted graph with an arbitrary orientation ﬁxed for
the edges giving rise to the vectors be which are the rows of the corre-
sponding incidence matrix. We start by trying to understand why, for
a spanning tree T of a graph G, L+
T might be a natural candidate for
preconditioning LG. Note that
LT =
�
e∈T
beb⊤
e ⪯
�
e∈G
beb⊤
e = LG.
Thus, I ≺ L+
T LG, which implies that λ1(L+
T LG) ≥ 1. Thus, to bound
the condition number of κ(L+
T LG), it suﬃces to bound λn(L+
Unfortunately, there is no easy way to bound this. Since L+T LG).
T LG is
PSD, an upper bound on its largest eigenvalue is its trace, Tr(L+
T LG).
If this upper bound is τ, Lemma 17.2 would imply that the con-
jugate gradient method applied to L+
T LG takes time approximately
τ
1/3tL+
T LG ∼ O(τ
1/3(m + n)). Thus, even though it sounds wasteful to
bound the trace by τ rather than by the largest eigenvalue by τ,
Lemma 17.2 allows us to improve the dependency on τ from τ
1/2 to
τ
1/3. We proceed to bound the trace of L+
T LG:
Tr(L+
T LG) = Tr
�
L+
T
�
e∈G
beb⊤
e
�
=
�
e
b⊤
e L+
T be,
where we use Tr(A + B) = Tr(A) + Tr(B) and Tr(ABC) = Tr(CAB).
Note that b⊤
e L+
T be is a scalar and is precisely the eﬀective resistance
across the endpoints of e in the tree T where each edge of G has a unit
resistance. The eﬀective resistance across two nodes i,j in a tree is the
sum of eﬀective resistances along the unique path P(i,j) on the tree.
Thus, we get
Tr(L+
T LG) =
�
e∈G
|P(e)|.
A trivial upper bound on Tr(L+
T LG), thus, is nm. This can also be
shown to hold when G is weighted. This leads us to the following
deﬁnition.

17.3 An ˜O(m
4/3)-Time Laplacian Solver
117
Deﬁnition 17.1. For an unweighted graph G, the stretch of a spanning
T is deﬁned to be strT (G) def
= Tr(L+
T LG).
Thus, to obtain the best possible bound on the number of iterations of
CGSolve, we would like a spanning tree T of G which minimizes the
average length of the path an edge of G has to travel in T. The time it
takes to construct T is also important.
17.3
An ˜
O(m
4/3)-Time Laplacian Solver
In this section we complete the proof of Theorem 17.1. We start by
stating the following nontrivial structural result about the existence
and construction of low-stretch spanning trees whose proof is graph-
theoretic and outside the scope of this monograph. The theorem applies
to weighted graphs as well.
Theorem 17.3. For any undirected graph G, a spanning tree T can
be constructed in ˜O(mlogn + nlognloglogn) time such that strT (G) =
˜O(mlogn). Here ˜O(·) hides loglogn factors.
This immediately allows us to conclude the proof of Theorem 17.1 using
Lemma 17.2 and the discussion in the previous section. The only thing
that remains is to address the issue that the matrix L+
T LG is symmetric.
The following trick is used to circumvent this diﬃculty. Recall from
Theorem 13.3 that the Cholesky decomposition of a tree can be done in
O(n) time. In particular, let LT = EE⊤, where E is a lower triangular
matrix with at most O(n) nonzero entries. The idea is to look at the
system of equations E+LGE+⊤y = E+b instead of LGx = b. If we can
solve for y then we can ﬁnd x = E+⊤y, which is computationally fast
since E is lower triangular with at most O(n) nonzero entries. Also, for
the same reason, E+b can be computed quickly.
Now we are in good shape. Let A def
= E+LGE+⊤ and b′ def
= E+b;
note that A is symmetric. Also, the eigenvalues of A are the same as
those of E⊤+AE⊤ = L+
T LG. Thus, the minimum eigenvalue of A is 1

118
Preconditioning for Laplacian Systems
and the trace is ˜O(m) by Theorem 17.3. Thus, using the conjugate gra-
dient method, we ﬁnd an ε-approximate solution to Ay = b′ in ˜O(m
1/3)
iterations.
In each iteration, we do O(1) matrix–vector multiplications. Note
that for any vector v, Av can be computed in O(n + m) operations
since E+v takes O(n) operations (since E is a lower triangular matrix
with at most O(n) nonzero entries) and LGv′ takes O(m) operations
(since LG has at most O(m) nonzero entries).
Notes
While preconditioning is a general technique for Laplacian systems,
its use originates in the work of Vaidya [86]. Using preconditioners,
Vaidya obtained an
˜O((∆n)1.75 log 1/ε) time Laplacian solver for
graphs with maximum degree ∆. Vaidya’s paper was never published
and his ideas and their derivatives appear in the thesis [42]. Boman
and Hendrickson [18] use low-stretch spanning trees constructed
by Alon et al. [6] to obtain ε-approximate solutions to Laplacian
systems roughly in m
3/2 log 1/ε time. The main result of this section,
Theorem 17.1, is from [81]. The ﬁrst polylog n stretch trees in
near-linear-time were constructed by Elkin et al. [27]. Theorem 17.3
is originally from [1], and improvements can be found in [2]. An
important open problem is to ﬁnd a simple proof of Theorem 17.3
(potentially with worse log factors).

18
Solving a Laplacian System in ˜
O(m) Time
Building on the techniques developed hitherto, this section presents an
algorithm which solves Lx = b in ˜O(m) time.
18.1
Main Result and Overview
In Section 17 we saw how preconditioning a symmetric PSD matrix
A by another symmetric PSD matrix P reduces the running time of
computing A+b for a vector b to about O((tA + tP )
�
κ(PA)), where
tA and tP are the times required to compute matrix–vector product
with A and P, respectively. We saw that if A = LG for a graph G, then
a natural choice for P is L+
T where T is a spanning tree of G with small
total stretch.1 For a graph G with edge weights given by wG and a
tree T, the stretch of an edge e ∈ E(G) in T is deﬁned to be
strT (e) def
= wG(e)
�
f∈Pe
w−1
G (f),
where Pe is the set of edges on the unique path that connects the
endpoints of e in T. Thus, if e ∈ T, then strT (e) = 1. We deﬁne
1 Unless speciﬁed, when we talk about a spanning tree T of a weighted graph, the edges of
T inherit the weights from G.
119

120
Solving a Laplacian System in ˜O(m) Time
strT (G) def
= �
e∈E strT (e). With this deﬁnition, it is easy to see that
strT (G) = Tr(L+
T LG).
This is the same as the deﬁnition in Section 17 where Theorem 17.3
asserted a remarkable result: A spanning tree T such that strT (G) =
˜O(m) can be constructed in time ˜O(m). Note that choosing a spanning
tree is convenient since we can compute L+
T v exactly in O(n) time using
Cholesky decomposition, see Theorem 13.3. If T is such a spanning
tree and LT is its Laplacian, then, using L+
T as a preconditioner, we
showed in Theorem 17.1 how the conjugate gradient method can be
used to compute a vector x such that ∥x − L+
Gb∥LG ≤ ε∥L+
Gb∥LG in
˜O(m
4/3 log 1/ε) time. In this section we improve this result and prove
the following theorem; the key result of this monograph.
Theorem 18.1. For any undirected, unweighted graph G with m
edges, a vector b with ⟨b,1⟩ = 0, and ε > 0, one can ﬁnd an x such
that ∥x − L+
Gb∥LG ≤ ε∥L+
Gb∥LG in ˜O(mlog 1/ε) time.
This theorem is a restatement of Theorem 3.1 where the algorithm
which achieves this bound is referred to as LSolve. In this section we
present LSolve and show that it runs in ˜O(mlog 1/ε) time. Toward
the end we discuss the linearity of LSolve: Namely, the output x of
LSolve on input LG,b and ε is a vector Zx, where Z is a n × n matrix
that depends only on G and ε, and satisﬁes ∥Z − L+∥ ≤ ε.
Proof Overview
Unlike the proof of Theorem 17.1, it is not clear how to prove
Theorem 18.1 by restricting to tree preconditioners. The reason is that,
while for a tree T of G we can compute L+
T v in O(n) time, we do
not know how to improve upon the upper bound on κ(L+
T LG) beyond
what was presented in Section 17. A natural question is whether we
can reduce the condition number by allowing more edges than those
contained in a tree. After all, the Cholesky decomposition-based algo-
rithm to solve tree systems in Section 13 works if, at every step during
the elimination process, there is always a degree 1 or a degree 2 vertex.

18.1 Main Result and Overview
121
The ﬁrst observation we make, proved in Section 18.2, is that if we
have a graph on n vertices and n − 1 + k edges (i.e., k more edges
than in a tree), after we are done eliminating all degree 1 and 2 ver-
tices, we are left with a graph that has at most 2(k − 1) vertices and
3(k − 1) edges. Thus, if k is not too large, there is a serious reduc-
tion in the size of the linear system that is left to solve. Now, can
we ﬁnd a subgraph H of G (whose edges are allowed to be weighted)
that has at most n − 1 + k edges and the condition number of L+
HLG
is much smaller than n? The answer turns out to be yes. We achieve
this by a combination of the spectral sparsiﬁcation technique from Sec-
tion 10 with low-stretch spanning trees. Speciﬁcally, for a graph G with
n vertices and m edges, in ˜O(m) time we can construct a graph H with
n − 1 + k edges such that κ(L+
HLG) ≤ O((mlog2 n)/k). Thus, if we choose
k = m/(100log2 n), apply crude sparsiﬁcation followed by eliminating all
degree 1 and 2 vertices, we are left with a Laplacian system of size
O(m/(100log2 n)), call it ˜G. The details of how to ﬁnd such an H appear
in Section 18.3.
We now need to make about
�
κ(L+
HLG) ≤ 10log2 n calls to the con-
jugate gradient method to solve for L+
Gb. This immediately leads us to
the problem of computing approximately 10log2 n products of the form
L+
Hv for some vector v. This, in turn, requires us to compute the same
number of L+
˜Gu products. ˜G is neither a tree nor does it contain any
degree 1 or 2 vertices. Thus, to proceed, we recurse on ˜G. Fortunately,
the choice of parameters ensures that
�
κ(L+
HLG)/(100log2 n) ≤ 1/10. This
shrinkage ensures that the work done at any level remains bounded by
˜O(m). We stop the recursion when the size of the instance becomes
polylog n. This means that the depth of the recursion is bounded by
O(logn/loglogn). The details of how the recursion is employed and how
the parameters are chosen to ensure that the total work remains ˜O(m)
are presented in Section 18.4.
There is an important issue regarding the fact that the conjugate
gradient is error-prone. While in practice this may be ﬁne, in the-
ory, this can cause serious problems. To control the error, one has to
replace the conjugate gradient with its linear version, Chebyshev itera-
tion, presented in Section 16.5. This results in an algorithm that makes

122
Solving a Laplacian System in ˜O(m) Time
LSolve a linear operator as claimed above. The details are presented
in Section 18.5 and can be omitted.
If we were to present all the details that go into the proof of
Theorem 18.1, including a precise description of LSolve, it would
be overwhelming to the reader and make the presentation quite long.
Instead, we bring out the salient features and important ideas in the
algorithm and proof, and show how it marries ideas between graph the-
ory and linear algebra that we have already seen in this monograph.
The goal is not to convince the reader about the proof of Theorem 18.1
to the last constants, but to give enough detail to allow a keen reader to
reconstruct the full argument and adapt it to their application. Finally,
in what follows, we will ignore poly(loglogn) factors.
18.2
Eliminating Degree 1,2 Vertices
Suppose H is a graph with n′ vertices and m′ = n′ − 1 + k edges. As in
Section 13.2, we greedily eliminate all degree 1 and 2 vertices to obtain
a graph G′. If m′ ≤ n′ − 1, then we can compute L+
Hv in O(n′) time
via Theorem 13.3. Hence, we may assume that k ≥ 1. This reduces the
computation of L+
Hv to that of computing L+
G′v in O(m′ + n′) time.
How many edges and vertices does G′ have? Let k1 and k2 be the
number of vertices of degree 1 and 2 eliminated from H, respectively.
Then |V (G′)| = n′ − k1 − k2 and |E(G′)| ≤ m′ − k1 − k2. The latter
occurs because we removed two edges adjacent to a degree 2 vertex and
added one edge between its neighbors. In G′, however, every vertex has
degree at least 3. Hence,
3(n′ − k1 − k2) ≤ 2(m′ − k1 − k2) = 2(n′ − 1 + k − k1 − k2).
Hence, |V (G′)| = n′ − k1 − k2 ≤ 2(k − 1), and consequently,
|E(G′)| ≤ m′ − k1 − k2 ≤ 2(k − 1) + k − 1 = 3(k − 1).
Thus, we have proved the following lemma.
Lemma 18.2. Given a graph H with n′ vertices and m′ = n′ − 1 + k
edges for k ≥ 1, the computation of L+
Hv can be reduced to computing
L+
G′v in time O(n′ + m′) where G′ has at most 2(k − 1) vertices and
at most 3(k − 1) edges.

18.3 Crude Sparsiﬁcation Using Low-Stretch Spanning Trees
123
18.3
Crude Sparsiﬁcation Using Low-Stretch
Spanning Trees
We now present the following crude spectral sparsiﬁcation theorem,
which is a rephrasing of Theorem 10.4.
Theorem 18.3. There is an algorithm that, given a graph G with edge
weights wG, γ > 0, and numbers qe such that qe ≥ wG(e)Re for all e,
outputs a weighted graph H s.t.
LG ⪯ 2LH ⪯ 3LG
and O(W logW log 1/γ) edges. The algorithm succeeds with probability
at least 1 − γ and runs in ˜O(W logW log 1/γ) time. Here W def
= �
e qe
and Re is the eﬀective resistance of e.
We show how one can compute qes that meet the condition of this the-
orem and have small �
e qe in time O(mlogn + nlog2 n). Thus, we do
not rely on the Laplacian solver as in the proof of Theorem 10.2. First
notice that if T is a spanning tree of G with weight function wG and
e ∈ G, then Re ≤ �
f∈Pe w−1
G (f), where Pe is the path in T with end-
points the same as e. This is because adding more edges only reduces the
eﬀective resistance between the endpoints of e. Hence, weRe ≤ strT (e).
Thus, strT (e) satisﬁes one property required by qe in the theorem above.
Moreover, we leave it as an exercise to show that, given T, one can com-
pute strT (e) for all e ∈ G in O(mlogn) time using elementary methods.
The thing we need to worry about is how to quickly compute a T with
small �
e strT (e). This is exactly where the low-stretch spanning trees
from Theorem 17.3, and mentioned in the introduction to this section,
come into play. Recall that, for a graph G with weight function wG,
the tree T from Theorem 17.3 satisﬁes �
e∈G strT (e) = O(mlogn) and
can be computed in time O(nlog2 n + mlogn). Hence, the number of
edges in the crude sparsiﬁer from Theorem 18.3 is O(mlog2 n) if we set
γ = 1/logn. This is not good as we have increased the number of edges
in the sampled graph.
There is a trick to get around this. Let 1 ≪ κ < m be an addi-
tional parameter and let T be the low-stretch spanning tree from
Theorem 17.3 for the graph G. Consider the graph ˆG = (V,E) with

124
Solving a Laplacian System in ˜O(m) Time
weight function w ˆG(e) def
= κwG(e) if e ∈ T and w ˆG(e) def
= wG(e) if e ̸∈ T.
Thus, in ˆG we have scaled the edges of T by a factor of κ. This trivially
implies that
LG ⪯ L ˆG ⪯ κLG.
We now let qe
def
= strT (e) = 1 if e ∈ T and qe
def
= 1/κ · strT (e) if e ̸∈ T.
Thus, again it can be checked that the qes satisfy the conditions for
Theorem 18.3 for ˆG with weights w ˆG. First note that
W =
�
e
qe =
�
e∈T
1 +
�
e̸∈T
strT (e)
κ
= n − 1 + O(mlogn/κ)
as strT (G) = O(mlogn). Now, let us estimate the number of edges we
obtain when we sample from the distribution {qe}e∈E approximately
W logW times as in Theorem 10.4. Note that even if an edge is chosen
multiple times in the sampling process, there is exactly one edge in H
corresponding to it with a weight summed up over multiple selections.
Thus, in H, we account for edges from T separately. These are exactly
n − 1. On the other hand, if an edge e /∈ T, a simple Chernoﬀ bound
argument implies that with probability at least 1 − 1/n0.1 ≫ 1 − 1/logn,
the number of such edges chosen is at most O(mlog2 n/κ). Since it takes
O(nlog2 n + mlogn) time to ﬁnd T and approximately W logW addi-
tional time to sample H, in
O(nlog2 n + mlogn + mlog2 n/κ)
time, with probability at least 1 − 1/log2 n, we obtain a graph H such
that
LG ⪯ 2LH ⪯ 3κLG.
Moreover, the number of edges in H is n − 1 + O(mlog2 n/κ). Impor-
tantly, κ(L+
HLG) = O(κ). Thus, to compute L+
Gv, we need about O(√κ)
computations of the form L+
Hu if we deploy the conjugate gradient
method. To summarize, we have proved the following lemma.
Lemma 18.4. There is an algorithm that, given a graph G on n ver-
tices with m edges, a vector v, an ε > 0 and a parameter κ, constructs

18.4 Recursive Preconditioning — Proof of the Main Theorem
125
a graph H on n vertices such that, with probability at least 1 − 1/logn,
(1) κ(L+
HLG) = O(κ),
(2) the number of edges in H is n − 1 + O(mlog2 n/κ), and
(3) the time it takes to construct H is O(nlog2 n + mlogn +
mlog2 n/κ).
Now, if we apply the greedy degree 1,2 elimination procedure on H, we
obtain G′ with O(mlog2 n/κ) edges and vertices. Thus, we have reduced
our problem to computing L+
G′u for vectors u where the size of G′ has
gone down by a factor of κ from that of G.
18.4
Recursive Preconditioning — Proof of the
Main Theorem
Now we show how the ideas we developed in the last two sections can
be used recursively. Our starting graph G = G1 has m1 = m edges and
n1 = n vertices. We use a parameter κ < m which will be determined
later. We then crudely sparsify G1 using Lemma 18.4 to obtain H1
such that, with probability 1 − 1/logn, (this probability is ﬁxed for all
iterations), the number of edges in H1 is at most n1 − 1 + O(m1 log2 n1/κ)
and κ(L+
H1LG1) = O(κ). If H1 does not satisfy these properties, then we
abort and restart from G1. Otherwise, we apply greedy elimination, see
Lemma 18.2, to H1 and obtain G2; this is a deterministic procedure. If
n2,m2 are the number of vertices and edges of G2, then we know that
m2 = O(m1 log2 n1/κ) and n2 = O(m1 log2 n1/κ). If n2 ≤ nf (for some nf
to be determined later), then we stop, otherwise we iterate and crude
sparsify G2. We follow this with greedy elimination to get H2 and
proceed to ﬁnd G3 and so on. Once we terminate, we have a sequence
G = G1,H1,G2,H2,...,Hd−1,Gd,
such that
(1) nd = O(nf),
(2) For all 1 ≤ i ≤ d − 1, κ(L+
HiLGi) = O(κ), and
(3) For all 2 ≤ i ≤ d − 1, ni,mi ≤ mi−1 log2 ni−1
κ
.

126
Solving a Laplacian System in ˜O(m) Time
Note that ni,mi are the number of vertices and edges of Gi, and
not Hi. The probability that this process is never restarted is at least
1 − d/logn. For this discussion we assume that we use the conjugate
gradient method which, when given H and G, can compute L+
Gu for a
given vector u with about
�
κ(L+
HLG) computations of the form L+
Hv
for some vector v. The (incorrect) assumption here is that there is no
error in computation and we address this in the next section. The depth
of the recursion is
d def
=
log(n/nf)
log(κ/log2 n).
Now to compute L+
G1u for a given vector u, the computation tree is
of degree O(√κ) and depth d. At the bottom, we have to compute
roughly (√κ)d problems of the type L+
Gdv. We can do this exactly in
time O(n3
f) using Gaussian elimination. Hence, the total work done at
the last level is
O(n3
f) · (√κ)d.
At level i, the amount of work is
(
√
k)i · ˜O(mi + ni).
We want the sum over all except the top level to be O(m). This is
achieved if
√κ · log2 n
κ
≪ 1.
Finally, note that the work done in building the sequence of precondi-
tioners is
�
i
O(mi logni + ni log2 ni + mi log2 ni/κ) = O(nlog2 n + mlogn)
if κ ≥ log3 n. The depth of the recursion is d =
log n/nf
log κ/log2 n. Hence, the
choice of parameters is dictated by the following constraints:
(1) O(n3
f) · (√κ)d = O(mlog2 n),
(2) O(√κ · log2 n
κ
) ≤ 1
2, and
(3) κ ≥ log3 n.

18.5 Error Analysis and Linearity of the Inverse
127
Under these conditions, the running time of the algorithm is ˜O(m√κ)
at the top level and O(mlog2 n) in the remaining levels. We set κ def
=
100log4 n and nf
def
= logn. This choice of κ and nf also ensures that d is
at most logn/(2loglogn) and, hence, condition (2) above is also satisﬁed.
Finally, the probability that we never restart is at least 1 − d/logn ≫ 1 −
1/100 for large enough n. The total running time is bounded by O((m +
n)log2 n). In the calculations above, note that as long as there is an
algorithm that reduces computing A+b to computing at most κ(A)1−ε
products Au, one can obtain an algorithm that runs in ˜O(mlogO(1/ε) n)
time. This concludes the proof of Theorem 18.1 assuming that there
is no error in the application of the conjugate gradient method. In the
following section we the intuition behind managing this error.
18.5
Error Analysis and Linearity of the Inverse
As discussed above, the conjugate gradient is not error-free. Since our
algorithm uses conjugate gradient recursively, the error could cascade
in a complicated manner and it is not clear how to analyze this eﬀect.
Here, we use the Chebyshev iteration, see Section 16.5, which achieves
the same running time as the conjugate gradient method but has the
additional property that on input a matrix A, a vector b, and an
ε > 0, outputs x = Zb where Z is a matrix such that (1 − ε)Z ⪯ A+ ⪯
(1 + ε)Z. This extra property comes at a price; it requires the knowl-
edge of the smallest and the largest eigenvalues of A. A bit more for-
mally, if we are given a lower bound λl on the smallest eigenvalue
and an upper bound λu on the largest eigenvalue of A, then, after
O(
�
λu/λl log 1/ε) iterations, the Chebyshev iteration-based method out-
puts an x = Zb such that (1 − ε)Z ⪯ A+ ⪯ (1 + ε)Z.
Let us see how to use this for our application. To illustrate the main
idea, consider the case when the chain consists of
G = G1,H1,G2,H2,G3,
where we compute L+
G3u exactly. In addition, we know that all the
eigenvalues of L+
HiLGi lie in the interval [1,κ], where we ﬁxed κ ∼ log4 n.
Given L+
G3, it is easy to obtain L+
H2 since G3 is obtained by elimi-
nating variables from H2; hence there is no error. Thus, if we want

128
Solving a Laplacian System in ˜O(m) Time
to compute L+
G2u, we use the Chebyshev iteration-based solver from
Theorem 16.6 with error ε and the guarantee that all eigenvalues of
L+
H2LG2 lie in the interval [1,κ]. Thus, the linear operator that tries to
approximate L+
G2, Z2, is such that (1 − ε)Z2 ⪯ L+
G2 ⪯ (1 + ε)Z2. This
is where the error creeps in and linearity is used. From Z2 we can
easily construct ˜Z1 which is supposed to approximate L+
H1; namely,
(1 − ε) ˜Z1 ⪯ L+
H1 ⪯ (1 + ε) ˜Z1. This implies that the eigenvalues for our
approximator ˜Z1LG1 of L+
H1LG1 which were supposed to lie in the
interval [1,κ] may spill out. However, if we enlarge the interval to
[1/1+δ,(1 + δ)κ] for a small constant δ suﬃciently bigger than ε, (at the
expense of increasing the number of iterations by a factor of 1 + δ) we
ensure that (1 − ε)Z1 ⪯ L+
G1 ⪯ (1 + ε)Z1. This argument can be made
formal via induction by noting that κ,ε, and δ remain ﬁxed through out.
Notes
Theorem 18.1 was ﬁrst proved by Spielman and Teng [77, 78, 79, 80] and
the proof presented in this section, using crude sparsiﬁers, draws sig-
niﬁcantly from [49, 50]. The idea of eliminating degree 1 and 2 vertices
was implicit in [86]. The idea of recursive preconditioning appears in
the thesis [42]. Theorem 18.3 is in [49].

19
Beyond Ax = b
The Lanczos Method
This section looks beyond solving linear equations and considers the
more general problem of computing f(A)v for a given PSD matrix A,
vector v and speciﬁed function f(·). A variant of the conjugate gradient
method, called the Lanczos method, is introduced which uses Krylov
subspace techniques to approximately compute f(A)v. The results of
this section can also be used to give an alternative proof of the main
result of Section 9 on computing the matrix exponential.
19.1
From Scalars to Matrices
Suppose one is given a symmetric PSD matrix A and a function
f : R �→ R. Then one can deﬁne f(A) as follows: Let u1,...,un be eigen-
vectors of A with eigenvalues λ1,...,λn, then f(A) def
= �
i f(λi)uiu⊤
i .
Given a vector v, we wish to compute f(A)v. One way to do this
exactly is to implement the deﬁnition of f(A) as above. This requires
the diagonalization of A, which is costly. Hence, in the interest of speed,
we are happy with an approximation to f(A)v. The need for such
primitive often arises in theory and practice. While the conjugate gra-
dient method allows us to do this for f(x) = 1/x, it seems speciﬁc to
this function. For instance, it is not clear how to adapt the conjugate
129

130
Beyond Ax = b The Lanczos Method
gradient method to compute exp(A)v, a primitive central in several
areas of optimization and mathematics. See Section 9 for the deﬁni-
tion of matrix exponential. In this section we present a meta-method,
called the Lanczos method, to compute approximations to f(A)v. The
method has a parameter k that provides better and better approxima-
tions to f(A)v as it increases: The error in the approximation after k
rounds is bounded by the maximum distance between the best degree k
polynomial and f in the interval corresponding to the smallest and the
largest eigenvalues of A. The linear algebra problem is, thus, reduced
to a problem in approximation theory. The following is the main result
of this section.
Theorem 19.1. There is an algorithm that, given a symmetric PSD
matrix A, a vector v with ∥v∥ = 1, a function f, and a positive integer
parameter k, computes a vector u such that,
∥f(A)v − u∥ ≤ 2 · min
pk∈Σk
max
λ∈Λ(A)|f(λ) − pk(λ)|.
Here Σk denotes the set of all degree k polynomials and Λ(A) denotes
the interval containing all the eigenvalues of of A. The time taken by
the algorithm is O((n + tA)k + k2).
We remark that this theorem can be readily applied to the computation
of exp(A)v; see the notes.
19.2
Working with Krylov Subspace
For a given positive integer k, the Lanczos method looks for an approxi-
mation to f(A)v of the form p(A)v where p is a polynomial of degree k.
Note that for any polynomial p of degree at most k, the vector p(A)v is
a linear combination of the vectors {v,Av,...,Akv}. The span of these
vectors is referred to as the Krylov subspace of order k of A w.r.t. v
and is deﬁned below.
Deﬁnition 19.1. Given a matrix A and a vector v, the Krylov sub-
space of order k, denoted by Kk, is deﬁned as the subspace that is
spanned by the vectors {v,Av,...,Akv}.

19.2 Working with Krylov Subspace
131
Since A and v are ﬁxed, we denote this subspace by Kk. (This
deﬁnition diﬀers slightly from the one in Section 16 where Kk+1 was
used to denote this subspace.) Note that any vector in Kk has to be
of the form p(A)v, where p is a degree k polynomial. The Lanczos
method to compute f(A)v starts by generating an orthonormal basis
for Kk. Let v0,...,vk be any orthonormal basis for Kk, and let Vk be
the n × (k + 1) matrix with {vi}k
i=0 as its columns. Thus, V ⊤
k Vk = Ik+1
and VkV ⊤
k
denotes the projection onto the subspace. Also, let Tk be
the operator A in the basis {vi}k
i=0 restricted to this subspace, i.e.,
Tk
def
= V ⊤
k AVk. Since all the vectors v,Av,...,Akv are in the subspace,
any of these vectors (or their linear combination) can be obtained by
applying Tk to v (after a change of basis), instead of A. The following
lemma states this formally.
Lemma 19.2. Let Vk be the orthonormal basis and Tk be the oper-
ator A restricted to Kk where ∥v∥ = 1, i.e., Tk = V ⊤
k AVk. Let p be a
polynomial of degree at most k. Then,
p(A)v = Vkp(Tk)V ⊤
k v.
Proof. Recall that VkV ⊤
k is the orthogonal projection onto the subspace
Kk. By linearity, it suﬃces to prove this for p = xt for all t ≤ k. This
is true for t = 0 since VkV ⊤
k v = v. For all j ≤ k, Ajv lies in Kk, thus,
VkV ⊤
k Ajv = Ajv. Hence,
Atv = (VkV ⊤
k )A(VkV ⊤
k )A···A(VkV ⊤
k )v
= Vk(V ⊤
k AVk)(V ⊤
k AVk)···(V ⊤
k AVk)V ⊤
k v = VkT t
kV ⊤
k v.
The next lemma shows that Vkf(Tk)V ⊤
k v approximates f(A)v as
well as the best degree k polynomial that uniformly approximates f.
The proof is based on the observation that if we express f as a
sum of any degree k polynomial and an error function, the above
lemma shows that the polynomial part is computed exactly in this
approximation.

132
Beyond Ax = b The Lanczos Method
Lemma 19.3. Let Vk be the orthonormal basis, and Tk be the opera-
tor A restricted to Kk where ∥v∥ = 1, i.e., Tk = V ⊤
k AVk. Let f : R → R
be any function such that f(A) and f(Tk) are well deﬁned. Then,
��f(A)v − Vkf(Tk)V ⊤
k v
�� is at most
min
pk∈Σk
�
max
λ∈Λ(A)|f(λ) − pk(λ)| +
max
λ∈Λ(Tk)|f(λ) − pk(λ)|
�
.
Proof. Let pk be any degree k polynomial. Let rk
def
= f − pk. Then,
���f(A)v − Vkf(Tk)V ⊤
k v
���
≤
���pk(A)v − Vkpk(Tk)V ⊤
k v
���
+
���rk(A)v − Vkrk(Tk)V ⊤
k v
���
(Lemma 19.2)
≤
0 + ∥rk(A)∥ +
���Vkrk(Tk)V ⊤
k
���
=
max
λ∈Λ(A)|rk(λ)| +
max
λ∈Λ(Tk)|rk(λ)|.
Minimizing over pk gives us our lemma.
Observe that in order to compute this approximation, we do not need
to know the polynomial explicitly. It suﬃces to prove that there exists
a degree k polynomial that uniformly approximates f on the inter-
val containing the spectrum of A and Tk (for exact computation,
Λ(Tk) = [λmin(Tk),λmax(Tk)] ⊆ [λmin(A),λmax(A)] = Λ(A).) Moreover,
if k ≪ n, the computation is reduced to a much smaller matrix. We
now show that an orthonormal basis for the Krylov subspace, Vk, can
be computed quickly and then describe the Lanczos procedure which
underlies Theorem 19.1.
19.3
Computing a Basis for the Krylov Subspace
In this section, we show that if we construct the basis {vi}k
i=0 in a
particular way, the matrix Tk has extra structure. In particular, if A is
symmetric, we show that Tk must be tridiagonal. This helps us speed
up the construction of the basis.

19.3 Computing a Basis for the Krylov Subspace
133
Algorithm 19.1 Lanczos
Input: A symmetric, PSD matrix A, a vector v such that ∥v∥ = 1, a
positive integer k, and a function f : R → R
Output: A vector u that is an approximation to f(A)v
1: v0 ← v
2: for i = 0 → k − 1
// Construct an orthonormal basis to Krylov
subspace of order k
do
3:
if i = 0 then
4:
w0 ← Av0
5:
else
6:
wi ← Avi − βivi−1
// Orthogonalize w.r.t. vi−1
7:
end if
8:
αi ← v⊤
i wi
9:
w′
i
def
= wi − αivi
// Orthogonalize w.r.t. vi
// If w′
i = 0, compute the approximation with the matrices Ti−1
and Vi−1, instead of Tk and Vk. The error bound still holds.
10:
βi+1 ← ∥w′
i∥
11:
vi+1 ←
w′
i
βi+1
// Scaling it to norm 1
12: end for
13: Let Vk be the n × (k + 1) matrix whose columns are v0,...,vk
14: Let Tk be the (k + 1) × (k + 1) matrix such that for all i (Tk)ii =
v⊤
i Avi = αi,(Tk)i,i+1 = (Tk)i+1,i = v⊤
i+1Avi = βi+1 and all other
entries are 0
// Compute Tk
def
= V ⊤
k AVk
15: Compute A ← f (Tk) exactly via eigendecomposition
16: return VkAV ⊤
k v
Suppose we compute the orthonormal basis {vi}k
i=0 iteratively,
starting from v0 = v: For i = 0,...,k, we compute Avi and remove
the components along the vectors {v0,...,vi} to obtain a new vec-
tor that is orthogonal to the previous vectors. This vector, scaled to
norm 1, is deﬁned to be vi+1. These vectors, by construction, are such
that for all i ≤ k, Span{v0,...,vi} = Span{v,Av,...,Akv}. Note that
(Tk)ij = v⊤
i Avj.

134
Beyond Ax = b The Lanczos Method
If
we
construct
the
basis
iteratively
as
above,
Avj ∈ Span
{v0,...,vj+1} by construction, and if i > j + 1, vi is orthogonal to
this subspace and hence v⊤
i (Avj) = 0. Thus, Tk satisﬁes (Tk)ij = 0 for
i > j + 1.
Moreover, if A is symmetric, v⊤
j (Avi) = v⊤
i (Avj), and hence Tk is
symmetric and tridiagonal. This means that at most three coeﬃcients
are nonzero in each row. Thus, while constructing the basis, at step
i + 1, we need to orthonormalize Avi only w.r.t. vi−1 and vi. This fact
is used for eﬃcient computation of Tk. The algorithm Lanczos appears
in Figure 19.1.
Completing the Proof of Theorem 19.1
The algorithm Lanczos implements the Lanczos method discussed in
this section. The guarantee on u follows from Lemma 19.3 and the fact
that Λ(Tk) ⊆ Λ(A). We use the fact that (Tk)ij = v⊤
i Avj and that Tk
must be tridiagonal to reduce our work to just computing O(k) entries
in Tk. The total running time is dominated by k multiplications of A
with a vector, O(k) dot-products and the eigendecomposition of the
tridiagonal matrix Tk to compute f(Tk) (which can be done in O(k2)
time), giving a total running time of O((n + tA)k + k2).
Notes
The presentation in this section is a variation of the Lanczos method,
a term often used speciﬁcally to denote the application of this meta-
algorithm to computing eigenvalues and eigenvectors of matrices. It
has been used to compute the matrix exponential, see [60, 67, 87].
The Lanczos method was combined with a semideﬁnite programming
technique from [62], a rational approximation result from [70] and the
Spielman–Teng Laplacian solver by Orecchia et al. [60] to obtain an
˜O(m) time algorithm for the Balanced Edge-Separator problem
from Section 7. Speciﬁcally, since Theorem 19.1 does not require the
explicit knowledge the best polynomial, it was used in [60] to com-
pute an approximation to exp(−L)v and, hence, give an alternative
proof of Theorem 9.1 from Section 9. The eigendecomposition result

19.3 Computing a Basis for the Krylov Subspace
135
for tridiagonal matrices, referred to in the proof of Theorem 19.1, is
from [63]. The reader is encouraged to compare the Lanczos method
with the conjugate gradient method presented in Section 16. Con-
cretely, it is a fruitful exercise to try to explore if the conjugate
gradient method can be derived from the Lanczos method by plugging
in f(x) = x−1.

References
[1] I. Abraham, Y. Bartal, and O. Neiman, “Nearly tight low stretch spanning
trees,” in Proceedings of the IEEE Symposium on Foundations of Computer
Science (FOCS), pp. 781–790, 2008.
[2] I. Abraham and O. Neiman, “Using petal-decompositions to build a low
stretch spanning tree,” in ACM Symposium on Theory of Computing (STOC),
pp. 395–406, 2012.
[3] D. Achlioptas, “Database-friendly random projections: Johnson-Lindenstrauss
with binary coins,” Journal of Computer and Systems Sciences, vol. 66, no. 4,
pp. 671–687, 2003.
[4] R. Ahlswede and A. Winter, “Addendum to “Strong converse for identiﬁcation
via quantum channels”,” IEEE Transactions on Information Theory, vol. 49,
no. 1, p. 346, 2003.
[5] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory, Algo-
rithms, and Applications. Prentice Hall, February 1993.
[6] N. Alon, R. M. Karp, D. Peleg, and D. B. West, “A graph-theoretic game and
its application to the k-server problem,” SIAM Journal on Computing, vol. 24,
no. 1, pp. 78–100, 1995.
[7] N. Alon and V. D. Milman, “λ1, isoperimetric inequalities for graphs, and
superconcentrators,” Journal of Combinational Theory, Series B, vol. 38, no. 1,
pp. 73–88, 1985.
[8] R. Andersen, F. R. K. Chung, and K. J. Lang, “Local graph partitioning using
pagerank vectors,” in Proceedings of the IEEE Symposium on Foundations of
Computer Science (FOCS), pp. 475–486, 2006.
[9] R. Andersen and Y. Peres, “Finding sparse cuts locally using evolving sets,” in
ACM Symposium on Theory of Computing (STOC), pp. 235–244, 2009.
136

References
137
[10] S. Arora, E. Hazan, and S. Kale, “O(√logn) approximation to sparsest cut
in ˜O(n2) time,” in Proceedings of the IEEE Symposium on Foundations of
Computer Science (FOCS), pp. 238–247, 2004.
[11] S. Arora, E. Hazan, and S. Kale, “The multiplicative weights update method:
A meta-algorithm and applications,” Theory of Computing, vol. 8, no. 1,
pp. 121–164, 2012.
[12] S. Arora and S. Kale, “A combinatorial, primal-dual approach to semideﬁnite
programs,” in ACM Symposium on Theory of Computing (STOC), pp. 227–236,
2007.
[13] S. Arora, S. Rao, and U. V. Vazirani, “Expander ﬂows, geometric embeddings
and graph partitioning,” Journal of the ACM, vol. 56, no. 2, 2009.
[14] J. D. Batson, D. A. Spielman, and N. Srivastava, “Twice-Ramanujan spar-
siﬁers,” in ACM Symposium on Theory of Computing (STOC), pp. 255–262,
2009.
[15] M. Belkin, I. Matveeva, and P. Niyogi, “Regularization and semi-supervised
learning on large graphs,” in Proceedings of the Workshop on Computational
Learning Theory (COLT), pp. 624–638, 2004.
[16] A. A. Bencz´ur and D. R. Karger, “Approximating s–t minimum cuts in ˜O(n2)
time,” in ACM Symposium on Theory of Computing (STOC), pp. 47–55, 1996.
[17] R. Bhatia, Matrix Analysis (Graduate Texts in Mathematics). Springer, 1996.
[18] E. G. Boman and B. Hendrickson, “Support theory for preconditioning,” SIAM
Journal on Matrix Analysis Applications, vol. 25, no. 3, pp. 694–717, 2003.
[19] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University
Press, March 2004.
[20] J. Cheeger, “A lower bound for the smallest eigenvalue of the Laplacian,” Prob-
lems in Analysis, pp. 195–199, 1970.
[21] E. W. Cheney, Introduction to Approximation Theory/E.W. Cheney. New York:
McGraw-Hill, 1966.
[22] P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S. Teng, “Elec-
trical ﬂows, Laplacian systems, and faster approximation of maximum ﬂow in
undirected graphs,” in ACM Symposium on Theory of Computing (STOC),
pp. 273–282, 2011.
[23] F. R. K. Chung, Spectral Graph Theory (CBMS Regional Conference Series in
Mathematics, No. 92). American Mathematical Society, 1997.
[24] S. I. Daitch and D. A. Spielman, “Faster approximate lossy generalized ﬂow
via interior point algorithms,” in ACM Symposium on Theory of Computing
(STOC), pp. 451–460, 2008.
[25] P. G. Doyle and J. L. Snell, Random Walks and Electric Networks. Washington,
DC: Mathematical Association of America, 1984.
[26] P. Elias, A. Feinstein, and C. Shannon, “A note on the maximum ﬂow through a
network,” IEEE Transactions on Information Theory, vol. 2, no. 4, pp. 117–119,
December 1956.
[27] M. Elkin, Y. Emek, D. A. Spielman, and S.-H. Teng, “Lower-stretch spanning
trees,” SIAM Journal on Computing, vol. 38, no. 2, pp. 608–628, 2008.
[28] M. Fiedler, “Algebraic connectivity of graphs,” Czechoslovak Mathematical
Journal, vol. 23, pp. 298–305, 1973.

138
References
[29] L. R. Ford and D. R. Fulkerson, “Maximal ﬂow through a network,” Canadian
Journal of Mathematics, vol. 8, pp. 399–404, 1954.
[30] A. Frangioni and C. Gentile, “Prim-based support-graph preconditioners
for min-cost ﬂow problems,” Computational Optimization and Applications,
vol. 36, no. 2–3, pp. 271–287, April 2007.
[31] W. S. Fung, R. Hariharan, N. J. A. Harvey, and D. Panigrahi, “A general frame-
work for graph sparsiﬁcation,” in ACM Symposium on Theory of Computing
(STOC), pp. 71–80, 2011.
[32] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the
Theory of NP-Completeness. W.H. Freeman, 1979.
[33] A. George, “Nested dissection of a regular ﬁnite element mesh,” SIAM Journal
on Numerical Analysis, vol. 10, no. 2, pp. 345–363, 1973.
[34] C. D. Godsil and G. Royle, Algebraic Graph Theory. Springer, 2001.
[35] G. H. Golub and C. F. Van Loan, Matrix Computations. Johns Hopkins Univ.
Press, 1996.
[36] G. H. Golub and M. L. Overton, “The convergence of inexact Chebyshev and
Richardson iterative methods for solving linear systems,” Technical Report,
Stanford University, Stanford, CA, USA, 1987.
[37] G. H. Golub and R. S. Varga, “Chebyshev semi-iterative methods, successive
overrelaxation iterative methods, and second order Richardson iterative meth-
ods,” Numerische Mathematik, vol. 3, pp. 157–168, 1961.
[38] K. Gremban, “Combinatorial preconditioners for sparse, symmetric, diagonally
dominant linear systems,” PhD Thesis, Carnegie Mellon University, Pittsburgh,
CMU CS Tech Report CMU-CS-96-123, October 1996.
[39] M. R. Hestenes and E. Stiefel, “Methods of conjugate gradients for solving linear
systems,” Journal of Research of the National Bureau of Standards, vol. 49,
pp. 409–436, December 1952.
[40] G. Iyengar, D. J. Phillips, and C. Stein, “Approximating semideﬁnite packing
programs,” SIAM Journal on Optimization, vol. 21, no. 1, pp. 231–268, 2011.
[41] G. Iyengar, D. J. Phillips, and C. Stein, “Approximation algorithms for semidef-
inite packing problems with applications to maxcut and graph coloring,” in
IPCO’05: Proceedings of Conference on Integer Programming and Combinato-
rial Optimization, pp. 152–166, 2005.
[42] A. Joshi, “Topics in optimization and sparse linear systems,” PhD Thesis, Uni-
versity of Illinois at Urbana-Champaign, Champaign, IL, USA, UMI Order
No. GAX97-17289, 1997.
[43] S. Kaczmarz, “¨Angen¨aherte Auﬂ¨osung von Systemen linearer Gleichungen,”
Bulletin International del’ Acad´emie Polonaise Sciences et des Lettres,
pp. 355–356, 1937.
[44] S. Kale, “Eﬃcient algorithms using the multiplicative weights update method,”
PhD Thesis, Princeton University, Department of Computer Science, 2007.
[45] J. A. Kelner and A. Madry, “Faster generation of random spanning trees,”
in Proceedings of the IEEE Symposium on Foundations of Computer Science
(FOCS), pp. 13–21, 2009.
[46] J. A. Kelner, G. L. Miller, and R. Peng, “Faster approximate multicommod-
ity ﬂow using quadratically coupled ﬂows,” in ACM Symposium on Theory of
Computing (STOC), pp. 1–18, 2012.

References
139
[47] J. A. Kelner, L. Orecchia, A. Sidford, and Z. A. Zhu, “A simple, combinatorial
algorithm for solving SDD systems in nearly-linear time,” in ACM Symposium
on Theory of Computing (STOC), 2013.
[48] R. Khandekar, S. Rao, and U. V. Vazirani, “Graph partitioning using single
commodity ﬂows,” Journal of the ACM, vol. 56, no. 4, 2009.
[49] I. Koutis, G. L. Miller, and R. Peng, “Approaching optimality for solving SDD
linear systems,” in Proceedings of the IEEE Symposium on Foundations of Com-
puter Science (FOCS), pp. 235–244, 2010.
[50] I. Koutis, G. L. Miller, and R. Peng, “A nearly-mlogn time solver for SDD
linear systems,” in Proceedings of the IEEE Symposium on Foundations of
Computer Science (FOCS), pp. 590–598, 2011.
[51] D. A. Levin, Y. Peres, and E. L. Wilmer, Markov Chains and Mixing Times.
American Mathematical Society, 2006.
[52] R. J. Lipton, D. J. Rose, and R. E. Tarjan, “Generalized nested dissection,”
SIAM Journal on Numerical Analysis, vol. 16, no. 2, pp. 346–358, 1979.
[53] L. Lov´asz and M. Simonovits, “Random walks in a convex body and an
improved volume algorithm,” Random Structures & Algorithms, vol. 4, no. 4,
pp. 359–412, 1993.
[54] R. Lyons and Y. Peres, Probability on Trees and Networks.
To Appear in
Cambridge University Press, 2012.
[55] A. Madry, “Fast approximation algorithms for cut-based problems in undi-
rected graphs,” in Proceedings of the IEEE Symposium on Foundations of Com-
puter Science (FOCS), pp. 245–254, 2010.
[56] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A spectral algorithm for
improving graph partitions,” Journal of Machine Learning Research, vol. 13,
pp. 2339–2365, 2012.
[57] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 2057–2064, 2011.
[58] M. Mihail, “Conductance and convergence of markov chains — a combinatorial
treatment of expanders,” in Proceedings of the IEEE Symposium on Founda-
tions of Computer Science (FOCS), pp. 526–531, 1989.
[59] L. Orecchia, “Fast approximation algorithms for graph partitioning using spec-
tral and semideﬁnite-programming techniques,” PhD Thesis, EECS Depart-
ment, University of California, Berkeley, May 2011.
[60] L. Orecchia, S. Sachdeva, and N. K. Vishnoi, “Approximating the exponential,
the Lanczos method and an ˜O(m)-time spectral algorithm for balanced sepa-
rator,” in ACM Symposium on Theory of Computing (STOC), pp. 1141–1160,
2012.
[61] L. Orecchia, L. J. Schulman, U. V. Vazirani, and N. K. Vishnoi, “On parti-
tioning graphs via single commodity ﬂows,” in ACM Symposium on Theory of
Computing (STOC), pp. 461–470, 2008.
[62] L. Orecchia and N. K. Vishnoi, “Towards an SDP-based approach to spectral
methods: A nearly-linear-time algorithm for graph partitioning and decom-
position,” in Proceedings of the Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pp. 532–545, 2011.

140
References
[63] V. Y. Pan and Z. Q. Chen, “The complexity of the matrix eigenproblem,” in
ACM Symposium on Theory of Computing (STOC), pp. 507–516, 1999.
[64] L. F. Richardson, “The approximate arithmetical solution by ﬁnite diﬀerences
of physical problems involving diﬀerential equations with an application to
the stresses in a masonry dam,” Transactions of the Royal Society of London,
vol. Series A, no. 210, pp. 307–357, 1910.
[65] T. J. Rivlin, An Introduction to the Approximation of Functions. Blaisdell book
in numerical analysis and computer science. Blaisdell Pub. Co., 1969.
[66] M. Rudelson, “Random vectors in the isotropic position,” Journal of Functional
Analysis, vol. 164, no. 1, pp. 60–72, 1999.
[67] Y. Saad, “Analysis of some Krylov subspace approximations to the matrix expo-
nential operator,” SIAM Journal on Numerical Analysis, vol. 29, pp. 209–228,
February 1992.
[68] Y. Saad, Iterative Methods for Sparse Linear Systems. Society for Industrial
and Applied Mathematics. Philadelphia, PA, USA, 2nd Edition, 2003.
[69] S. Sachdeva and N. K. Vishnoi, “Inversion is as easy as exponentiation,”
manuscript, 2012.
[70] E. B. Saﬀ, A. Schonhage, and R. S. Varga, “Geometric convergence to
e−z by rational functions with real poles,” Numerische Mathematik, vol. 25,
pp. 307–322, 1975.
[71] J. Sherman, “Breaking the multicommodity ﬂow barrier for O(√logn)-
approximations to sparsest cut,” in Proceedings of the IEEE Symposium on
Foundations of Computer Science (FOCS), 2009.
[72] J. R. Shewchuk, “An introduction to the conjugate gradient method without
the agonizing pain,” Technical Report, Carnegie Mellon University, Pittsburgh,
PA, USA, 1994.
[73] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, vol. 22, pp. 888–905,
1997.
[74] D. B. Shmoys, Cut Problems and Their Application to Divide-and-Conquer.
pp. 192–235, 1997.
[75] D. A. Spielman, “Algorithms, graph theory, and linear equations in Lapla-
cian matrices,” in Proceedings of International Congress of Mathematicians
(ICM’10), 2010.
[76] D. A. Spielman and N. Srivastava, “Graph sparsiﬁcation by eﬀective resis-
tances,” SIAM Journal on Computing, vol. 40, no. 6, pp. 1913–1926, 2011.
[77] D. A. Spielman and S.-H. Teng, “Nearly-linear time algorithms for graph parti-
tioning, graph sparsiﬁcation, and solving linear systems,” in ACM Symposium
on Theory of Computing (STOC), pp. 81–90, New York, NY, USA, 2004.
[78] D. A. Spielman and S.-H. Teng, “Nearly-linear time algorithms for precondi-
tioning and solving symmetric, diagonally dominant linear systems,” CoRR,
abs/cs/0607105, 2006.
[79] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for massive
graphs and its application to nearly-linear time graph partitioning,” CoRR,
abs/0809.3232, 2008.

References
141
[80] D. A. Spielman and S.-H. Teng, “Spectral sparsiﬁcation of graphs,” SIAM
Journal on Computing, vol. 40, no. 4, pp. 981–1025, 2011.
[81] D. A. Spielman and J. Woo, “A note on preconditioning by low-stretch spanning
trees,” CoRR, abs/0903.2816, 2009.
[82] G. Strang, Linear Algebra and Its Applications. Harcourt Brace Jonanovich.
San Diego, 3rd Edition, 1988.
[83] T. Strohmer and R. Vershynin, “A randomized Kaczmarz algorithm with expo-
nential convergence,” Journal of Fourier Analysis and Applications, vol. 15,
pp. 262–278, 2009.
[84] S. Teng, “The Laplacian paradigm: Emerging algorithms for massive graphs,”
in Proceedings of the Annual Conference on Theory and Applications of Models
of Computation (TAMC), pp. 2–14, 2010.
[85] L. N. Trefethen and D. Bau, Numerical Linear Algebra. SIAM, 1997.
[86] P. M. Vaidya, “Solving linear equations with symmetric diagonally dominant
matrices by constructing good preconditioners,” Technical Report, Department
of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL,
1990.
[87] J. vanden Eshof and M. Hochbruck, “Preconditioning Lanczos approximations
to the matrix exponential,” SIAM Journal on Scientiﬁc Computing, vol. 27,
pp. 1438–1457, November 2005.
[88] J. H. Wilkinson, The Algebraic Eigenvalue Problem (Monographs on Numerical
Analysis). USA: Oxford University Press, 1st Edition, April 1988.
[89] X. Zhu, Z. Ghahramani, and J. D. Laﬀerty, “Semi-supervised learning using
Gaussian ﬁelds and harmonic functions,” in Proceedings of the International
Conference on Machine Learning (ICML), pp. 912–919, 2003.