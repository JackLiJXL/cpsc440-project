 Having too small of a learning rate in gradient descent can cause the algorithm to get stuck in a local minimum and not converge to the global minimum. This is because the gradient is too small to move the algorithm out of the local minimum.