 When the number of features, 'd', is large, it may be more efficient to use gradient descent to approximate the solution to the least squares problem instead of exactly solving it with the closed form solution. This is because the cost of computing the gradient is linear in 'n', and as 'n' gets large, gradient descent iterations become expensive.