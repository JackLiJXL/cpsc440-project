



Laplacian Solvers and Their Algorithmic Applications

By Nisheeth K. Vishnoi

II Applications

Graph Partitioning I The Normalized Laplacian

Graph Conductance

A Mathematical Program

The Normalized Laplacian and Its Second Eigenvalue

Graph Partitioning II A Spectral Algorithm for Conductance

Sparse Cuts from

Embeddings

An

Embedding from an

Embedding

Graph Partitioning III Balanced Cuts

The Balanced Edge-Separator Problem

The Algorithm and Its Analysis

Graph Partitioning IV Computing the Second Eigenvector

The Power Method

The Second Eigenvector via Powering

The Matrix Exponential and Random Walks

The Matrix Exponential

Rational Approximations to the Exponential

Simulating Continuous-Time Random Walks

11.1 Computing Voltages and Currents 11.2 Computing Eﬀective Resistances

12 Cuts and Flows

13 Cholesky Decomposition Based Linear Solvers

13.1 Cholesky Decomposition 13.2 Fast Solvers for Tree Systems

15.1 Optimization View of Equation Solving 15.2 The Gradient Descent-Based Solver

The Conjugate Gradient Method

Krylov Subspace and A -Orthonormality

Computing the A -Orthonormal Basis

Analysis via Polynomial Minimization

Chebyshev Polynomials — Why Conjugate Gradient Works

The Chebyshev Iteration

Preconditioning

Combinatorial Preconditioning via Trees

An ˜ O ( m

/

)-Time Laplacian Solver

18 Solving a Laplacian System in ˜ O ( m ) Time

Eliminating Degree

,

Vertices

Crude Sparsiﬁcation Using Low-Stretch Spanning Trees

Recursive Preconditioning — Proof of the Main Theorem

Error Analysis and Linearity of the Inverse





Laplacian Solvers and Their Algorithmic Applications

Nisheeth K. Vishnoi

Microsoft Research, India, nisheeth.vishnoi@gmail.com

Abstract

The ability to solve a system of linear equations lies at the heart of areas such as optimization, scientiﬁc computing, and computer science, and has traditionally been a central topic of research in the area of numer- ical linear algebra. An important class of instances that arise in prac- tice has the form L x = b , where L is the Laplacian of an undirected graph. After decades of sustained research and combining tools from disparate areas, we now have Laplacian solvers that run in time nearly- linear in the sparsity (that is, the number of edges in the associated graph) of the system, which is a distant goal for general systems. Sur- prisingly, and perhaps not the original motivation behind this line of research, Laplacian solvers are impacting the theory of fast algorithms for fundamental graph problems. In this monograph, the emerging paradigm of employing Laplacian solvers to design novel fast algorithms for graph problems is illustrated through a small but carefully chosen set of examples. A part of this monograph is also dedicated to develop- ing the ideas that go into the construction of near-linear-time Laplacian solvers. An understanding of these methods, which marry techniques from linear algebra and graph theory, will not only enrich the tool-set of an algorithm designer but will also provide the ability to adapt these methods to design fast algorithms for other fundamental problems.

The ability to solve a system of linear equations lies at the heart of areas such as optimization, scientiﬁc computing, and computer science and, traditionally, has been a central topic of research in numerical linear algebra. Consider a system A x = b with n equations in n variables. Broadly, solvers for such a system of equations fall into two categories. The ﬁrst is Gaussian elimination-based methods which, essentially, can be made to run in the time it takes to multiply two n × n matrices, (currently O ( n 2 . 3 ... ) time). The second consists of iterative methods, such as the conjugate gradient method. These reduce the problem to computing n matrix–vector products, and thus make the running time proportional to mn where m is the number of nonzero entries, or sparsity, of A. 1 While this bound of n in the number of iterations is tight in the worst case, it can often be improved if A has additional structure, thus, making iterative methods popular in practice.

An important class of such instances has the form L x = b , where L is the Laplacian of an undirected graph G with n vertices and m edges

Strictly speaking, this bound on the running time assumes that the numbers have bounded

Strictly speaking, this bound on the running time assumes that the numbers have bounded precision.

with m (typically) much smaller than n 2 . Perhaps the simplest setting in which such Laplacian systems arise is when one tries to compute cur- rents and voltages in a resistive electrical network. Laplacian systems are also important in practice, e.g., in areas such as scientiﬁc computing and computer vision. The fact that the system of equations comes from an underlying undirected graph made the problem of designing solvers especially attractive to theoretical computer scientists who entered the fray with tools developed in the context of graph algorithms and with the goal of bringing the running time down to O ( m ) . This eﬀort gained serious momentum in the last 15 years, perhaps in light of an explosive growth in instance sizes which means an algorithm that does not scale near-linearly is likely to be impractical.

After decades of sustained research, we now have a solver for Lapla- cian systems that runs in O ( m log n ) time. While many researchers have contributed to this line of work, Spielman and Teng spearheaded this endeavor and were the ﬁrst to bring the running time down to ˜ O ( m ) by combining tools from graph partitioning, random walks, and low- stretch spanning trees with numerical methods based on Gaussian elim- ination and the conjugate gradient. Surprisingly, and not the original motivation behind this line of research, Laplacian solvers are impacting the theory of fast algorithms for fundamental graph problems; giving back to an area that empowered this work in the ﬁrst place.

That is the story this monograph aims to tell in a comprehensive manner to researchers and aspiring students who work in algorithms or numerical linear algebra. The emerging paradigm of employing Laplacian solvers to design novel fast algorithms for graph problems is illustrated through a small but carefully chosen set of problems such as graph partitioning, computing the matrix exponential, simulat- ing random walks, graph sparsiﬁcation, and single-commodity ﬂows. A signiﬁcant part of this monograph is also dedicated to developing the algorithms and ideas that go into the proof of the Spielman–Teng Lapla- cian solver. It is a belief of the author that an understanding of these methods, which marry techniques from linear algebra and graph theory, will not only enrich the tool-set of an algorithm designer, but will also provide the ability to adapt these methods to design fast algorithms for other fundamental problems.

How to use this monograph.

How to use this monograph. This monograph can be used as the text for a graduate-level course or act as a supplement to a course on spectral graph theory or algorithms. The writing style, which deliber- ately emphasizes the presentation of key ideas over rigor, should even be accessible to advanced undergraduates. If one desires to teach a course based on this monograph, then the best order is to go through the sections linearly. Essential are Sections 1 and 2 that contain the basic linear algebra material necessary to follow this monograph and Section 3 which contains the statement and a discussion of the main theorem regarding Laplacian solvers. Parts of this monograph can also be read independently. For instance, Sections 5–7 contain the Cheeger inequality based spectral algorithm for graph partitioning. Sections 15 and 16 can be read in isolation to understand the conjugate gradient method. Section 19 looks ahead into computing more general functions than the inverse and presents the Lanczos method. A dependency dia- gram between sections appears in Figure 1. For someone solely inter- ested in a near-linear-time algorithm for solving Laplacian systems, the quick path to Section 14, where the approach of a short and new proof is presented, should suﬃce. However, the author recommends going all

1,2,3 5 8 9 4 13 15 6 11 12 10 16 14 7 17 19 18

Fig. 1 The dependency diagram among the sections in this monograph. A dotted line from i to j means that the results of Section j use some results of Section i in a black-box manner and a full understanding is not required.

the way to Section 18 where multiple techniques developed earlier in the monograph come together to give an ˜ O ( m ) Laplacian solver.

Acknowledgments.

Acknowledgments. This monograph is partly based on lectures delivered by the author in a course at the Indian Institute of Sci- ence, Bangalore. Thanks to the scribes: Deeparnab Chakrabarty, Avishek Chatterjee, Jugal Garg, T. S. Jayaram, Swaprava Nath, and Deepak R. Special thanks to Elisa Celis, Deeparnab Chakrabarty, Lorenzo Orecchia, Nikhil Srivastava, and Sushant Sachdeva for read- ing through various parts of this monograph and providing valuable feedback. Finally, thanks to the reviewer(s) for several insightful com- ments which helped improve the presentation of the material in this monograph.

Notation

• The set of real numbers is denoted by R , and R ≥

denotes the set of nonnegative reals. We only consider real numbers in this monograph. • The set of integers is denoted by Z , and Z ≥

denotes the set of nonnegative integers. • Vectors are denoted by boldface, e.g., u , v . A vector v ∈ R n is a column vector but often written as v = ( v

,...,v n ) . The transpose of a vector v is denoted by v . • For vectors u , v , their inner product is denoted by u , v or u v . • For a vector v , v denotes its

or Euclidean norm where v def = v , v . We sometimes also refer to the

or Man- hattan distance norm v

def = sal
 ni =

| v i | . • The outer product of a vector v with itself is denoted by vv . • Matrices are denoted by capitals, e.g., A,L. The transpose of A is denoted by A ‘

. • We use t A to denote the time it takes to multiply the matrix A with a vector.

• The A -norm of a vector v is denoted by v A def = v A v . • For a real symmetric matrix A, its real eigenvalues are ordered λ

( A ) ≤ λ

( A ) ≤ ··· ≤ λ n ( A ) . We let Λ( A ) def = [ λ

( A ) ,λ n ( A )] . • A positive-semideﬁnite (PSD) matrix is denoted by A =

and a positive-deﬁnite matrix A

• The norm of a symmetric matrix A is denoted by A

)
 def = max {| λ

( A ) | , | λ n ( A ) |} . For a symmetric PSD matrix A, A = λ n ( A ) . • Thinking of a matrix A as a linear operator, we denote the image of A by Im( A ) and the rank of A by rank( A ). • A graph G has a vertex set V and an edge set E. All graphs are assumed to be undirected unless stated otherwise. If the graph is weighted, there is a weight function w : E (cid:

)→ R ≥

. Typically, n is reserved for the number of vertices | V | , and m for the number of edges | E | . • E F [ · ] denotes the expectation and P F [ · ] denotes the proba- bility over a distribution F . The subscript is dropped when clear from context. • The following acronyms are used liberally, with respect to (w.r.t.), without loss of generality (w.l.o.g.), with high prob- ability (w.h.p.), if and only if (iﬀ), right-hand side (r.h.s.), left-hand side (l.h.s.), and such that (s.t.). • Standard big-o notation is used to describe the limiting behavior of a function. ˜ O denotes potential logarithmic factors which are ignored, i.e., f = ˜ O ( g ) is equivalent to f = O ( g log k ( g )) for some constant k .

This section reviews basics from linear algebra, such as eigenvalues and eigenvectors, that are relevant to this monograph. The spectral theorem for symmetric matrices and min–max characterizations of eigenvalues are derived.

Spectral Decomposition of Symmetric Matrices

One way to think of an m × n matrix A with real entries is as a linear operator from R n to R m which maps a vector v ∈ R n to A v ∈ R m . Let dim( S ) be dimension of S , i.e., the maximum number of linearly independent vectors in S . The rank of A is deﬁned to be the dimension of the image of this linear transformation. Formally, the image of A is deﬁned to be Im( A ) def = { u ∈ R m : u = A v for some v ∈ R n } , and the rank is deﬁned to be rank( A ) def = dim(Im( A )) and is at most min { m,n } . We are primarily interested in the case when A is square, i.e., m =

be complex. If A is symmetric, then one can show that the eigenvalues are real. For a complex number z = a + ib with a,b ∈ R , its conjugate is deﬁned as ¯ z = a − ib. For a vector v , its conjugate transpose v is the transpose of the vector whose entries are conjugates of those in v . Thus, v v = v 2 .

Lemma 1.1. If A is a real symmetric n × n matrix, then all of its eigenvalues are real.

Proof. Let λ be an eigenvalue of A , possibly complex, and v be the corresponding eigenvector. Then, A v = λ v . Conjugating both sides we obtain that v A = λ v , where v is the conjugate transpose of v . Hence, v A v = λ v v , since A is symmetric. Thus, λ Ally
 v 2 = λ Xs
 v 2 which implies that λ = λ. Thus, λ ∈ R .

Let λ 1 ≤ λ 2 ≤ ··· ≤ λ n be the n real eigenvalues, or the spectrum , of A with corresponding eigenvectors u 1 ,..., u n . For a symmetric matrix, its

We now study eigenvectors that correspond to distinct eigenvalues.

Lemma 1.2. Let λ i and λ j be two eigenvalues of a symmetric matrix A , and u i , u j be the corresponding eigenvectors. If λ i = λ j , then u i , u j = 0.

Proof. Given A u i = λ i u i and A u j = λ j u j , we have the following sequence of equalities. Since A is symmetric, u i A 47
 = u i A. Thus, u i A u j = λ i u i u j on the one hand, and u i A u j = λ j u i u j on the other. Therefore, λ j u i u j = λ i u i u j . This implies that u i u j = 0 since λ i = λ j .

Hence, the eigenvectors corresponding to diﬀerent eigenvalues are orthogonal. Moreover, if u i and u j correspond to the same eigen- value λ , and are linearly independent, then any linear combination

is also an eigenvector corresponding to the same eigenvalue. The maxi- mal eigenspace of an eigenvalue is the space spanned by all eigenvectors corresponding to that eigenvalue. Hence, the above lemma implies that one can decompose R n into maximal eigenspaces U i , each of which cor- responds to an eigenvalue of A, and the eigenspaces corresponding to distinct eigenvalues are orthogonal. Thus, if λ 1 < λ 2 < ··· < λ k are the set of distinct eigenvalues of a real symmetric matrix A , and U i is the eigenspace associated with λ i , then, from the discussion above,

Hence, given that we can pick an orthonormal basis for each U i , we may assume that the eigenvectors of A form an orthonormal basis for R n . Thus, we have the following spectral decomposition theorem.

Theorem 1.3. Let λ 1 ≤ ··· ≤ λ n be the spectrum of A with corre- sponding eigenvalues u 1 ,..., u n . Then, A = ni =1 λ i u i u i .

The above is true for all j . Since u j s are orthonormal basis of R n , we have for all v ∈ R n , A v = B v . This implies A = B .

Thus, when A is a real and symmetric matrix, Im( A ) is spanned by the eigenvectors with nonzero eigenvalues. From a computational perspec- tive, such a decomposition can be computed in time polynomial in the bits needed to represent the entries of A. 1

To be very precise, one can only compute eigenvalues and eigenvectors to a high enough precision in polynomial time. We will ignore this distinction for this monograph as we do not need to know the exact values.

Basic Linear Algebra

Min–Max Characterizations of Eigenvalues Now we present a variational characterization of eigenvalues which is very useful.

Min–Max Characterizations of Eigenvalues

Lemma 1.4. If A is an n × n real symmetric matrix, then the largest eigenvalue of A is

Proof. Let λ 1 ≤ λ 2 ≤ ··· ≤ λ n be the eigenvalues of A , and let u 1 , u 2 ,..., u n be the corresponding orthonormal eigenvectors which span R n . Hence, for all v ∈ R n , there exist c 1 ,...,c n ∈ R such that v = i c i u i . Thus,

v , v = i c i u i , i c i u i = i c 2 i . Moreover, v A v = i c i u i  (cid:3) j λ j u j u j   k c k u k = i,j,k c i c k λ j ( u i u j ) · ( u j u k ) = i c 2 i λ i ≤ λ n i c 2 i = λ n tv
 v , v .

Hence, ∀ v = 0, v A v v v ≤ λ n . This implies,

Now note that setting v = u n achieves this maximum. Hence, the lemma follows.

If one inspects the proof above, one can deduce the following lemma just as easily.

Lemma 1.5. If A is an n × n real symmetric matrix, then the smallest eigenvalue of A is

More generally, one can extend the proof of the lemma above to the following. We leave it as a simple exercise.

Theorem 1.6. If A is an n × n real symmetric matrix, then for all 1 ≤ k ≤ n, we have

Some good texts to review basic linear algebra are [35, 82, 85]. Theo- rem 1.6 is also called the Courant–Fischer–Weyl min–max principle.

This section introduces the graph Laplacian and connects the second eigenvalue to the connectivity of the graph.

The Graph Laplacian and Its Eigenvalues

Consider an undirected graph G = ( V,E ) with n def = | V | and m def = | E | . We assume that G is unweighted; this assumption is made to simplify the presentation but the content of this section readily generalizes to the weighted setting. Two basic matrices associated with G, indexed by its vertices, are its adjacency matrix A and its degree matrix D . Let d i denote the degree of vertex i.

The graph Laplacian of G is deﬁned to be L def = D − A . We often refer to this as the Laplacian. In Section 5, we introduce the normalized

Laplacian which is diﬀerent from L. To investigate the spectral proper- ties of L, it is useful to ﬁrst deﬁne the n × n matrices L e as follows: For every e = ij ∈ E, let L e ( i,j ) = L e ( j,i ) def = − 1 , let L e ( i,i ) = L e ( j,j ) = 1 , and let L e ( i ,j ) = 0 , if i / ∈ { i,j } or j / ∈ { i,j } . The following then fol- lows from the deﬁnition of the Laplacian and L e s.

Lemma 2.1. Let L be the Laplacian of a graph G = ( V,E ). Then, L = e ∈ E L e .

This can be used to show that the smallest eigenvalue of a Laplacian is always nonnegative. Such matrices are called positive semideﬁnite, and are deﬁned as follows.

Deﬁnition 2.1. A symmetric matrix A is called positive semideﬁnite (PSD) if λ 1 ( A ) ≥ 0. A PSD matrix is denoted by A 0. Further, A is said to be positive deﬁnite if λ 1 ( A ) > 0, denoted A >
 0.

Note that the Laplacian is a PSD matrix.

Lemma 2.2. Let L be the Laplacian of a graph G = ( V,E ). Then, L 0 .

0 . Proof. For any v = ( v 1 ,...,v n ), v 7]
 L v = v e ∈ E L e v = e ∈ E v Ty
 L e v = e = ij ∈ E ( v i − v j ) 2 ≥ 0 .

Hence, min v vy
 = 0 v oF
 L v ≥ 0 . Thus, appealing to Theorem 1.6, we obtain that L 0 .

Is L

? The answer to this is no: Let

denote the vector with all coordinates

. Then, it follows from Lemma

that L

=

·

. Hence, λ

( L ) =

. For weighted a graph G = ( V,E ) with edge weights given by a weight function w G : E (cid:

)→ R ≥ , one can deﬁne

Then, the deﬁnition of the Laplacian remains the same, namely, L = D − A.

The Second Eigenvalue and Connectivity

The Second Eigenvalue and Connectivity

What about the second eigenvalue, λ 2 ( L ), of the Laplacian? We will see in later sections on graph partitioning that the second eigenvalue of the Laplacian is intimately related to the conductance of the graph, which is a way to measure how well connected the graph is. In this section, we establish that λ 2 ( L ) determines if G is connected or not. This is the ﬁrst result where we make a formal connection between the spectrum of the Laplacian and a property of the graph.

Theorem 2.3. Let L be the Laplacian of a graph G = ( V,E ). Then, λ 2 ( L ) > 0 iﬀ G is connected.

Proof. If G is disconnected, then L has a block diagonal structure. It suﬃces to consider only two disconnected components. Assume the dis- connected components of the graph are G 1 and G 2 , and the correspond- ing vertex sets are V 1 and V 2 . The Laplacian can then be rearranged as follows:

dimension | V i | , respectively. This is an eigenvector corresponding to the 77a


smallest eigenvalue, which is zero. Now consider x 2 def = 1

@
 x 2 , x 1 = 0 , the smallest eigenvalue, which is 0 has multiplicity at 2 . Hence, λ 2 ( L ) = 0.

For the other direction, assume that λ 2 ( L ) = 0 and that G is con- nected. Let u 2 be the second eigenvector normalized to have length 1 . Then, λ 2 ( L ) = u 2 L u 2 . Thus, eige
 e = ij ∈ E ( u 2 ( i ) − u 2 ( j )) 2 = 0 . Hence, for all e = ij ∈ E, u 2 ( i ) = u 2 ( j ) .

Since G is connected, there is a path from vertex 1 to every vertex j = 1, and for each intermediate edge e = ik , u 2 ( i ) = u 2 ( k ). Hence, u 2 (1) = u 2 ( j ) , ∀ j . Hence, u 2 = ( c,c,...,c ). However, we also know u 2 , 1 = 0 which implies that c = 0 . This contradicts the fact that u 2 is a nonzero eigenvector and establishes the theorem.

Notes

There are several books on spectral graph theory which contain numer- ous properties of graphs, their Laplacians and their eigenvalues; see [23, 34, 88]. Due to the connection of the second eigenvalue of the Laplacian to the connectivity of the graph, it is sometimes called its algebraic connectivity or its Fiedler value, and is attributed to [28].

This section introduces Laplacian systems of linear equations and notes the properties of the near-linear-time Laplacian solver relevant for the applications presented in this monograph. Sections 5–12 cover several applications that reduce fundamental graph problems to solving a small number of Laplacian systems.

3.1 System of Linear Equations

An n × n matrix A and vector b ∈ R n together deﬁne a system of linear equations A x = b , where x = ( x 1 ,...,x n ) are variables. By deﬁnition, a solution to this linear system exists if and only if (iﬀ) b lies in the image of A. A is said to be invertible, i.e., a solution exists for all b , if its image is R n , the entire space. In this case, the inverse of A is denoted by A − 1 . The inverse of the linear operator A, when b is restricted to the image of A, is also well deﬁned and is denoted by A + . This is called the pseudo-inverse of A.

3.2 Laplacian Systems

Now consider the case when, in a system of equations A x = b , A = L is the graph Laplacian of an undirected graph. Note that this sys- tem is not invertible unless b ∈ Im( L ) . It follows from Theorem 2.3 that for a connected graph, Im( L ) consists of all vectors orthogonal to the vector 1 . Hence, we can solve the system of equations L x = b if (t
 b , 1 = 0 . Such a system will be referred to as a Laplacian system of linear equations or, in short, a Laplacian system. Hence, we can deﬁne the pseudo-inverse of the Laplacian as the linear operator which takes a vector b orthogonal to 1 to its pre-image.

3.3 An Approximate, Linear-Time Laplacian Solver

In this section we summarize the near-linear algorithm known for solving a Laplacian system L x = b . This result is the basis of the appli- cations and a part of the latter half of this monograph is devoted to its proof.

Theorem 3.1. There is an algorithm LSolve which takes as input a graph Laplacian L , a vector b , and an error parameter ε > 0, and returns x satisfying

where ik
 b Dll
 L def = b T L b . The algorithm runs in O ( m log 1 / ε ) time, where m is the number of nonzero entries in L .

Let us ﬁrst relate the norm in the theorem above to the Euclidean norm. For two vectors v , w and a symmetric PSD matrix A,

In other words,

A + 1 / 2 v − w ≤ v − w A ≤ A |)
 1 / 2 v − w

Hence, the distortion in distances due to the A -norm is at most λ n ( A ) / λ 1 ( A ) .

For the graph Laplacian of an unweighted and connected graph, when all vectors involved are orthogonal to 1 , λ 2 ( L ) replaces λ 1 ( L ) . It can be checked that when G is unweighted, λ 2 ( L ) ≥ 1 / poly( n ) and λ n ( L ) ≤ Tr( L ) = m ≤ n 2 . When G is weighted the ratio between the L -norm and the Euclidean norm scales polynomially with the ratio of the largest to the smallest weight edge. Finally, note that for any two vectors, v − w ∞ ≤ v − w . Hence, by a choice of ε def = δ / poly( n ) in Theorem 3.1, we ensure that the approximate vector output by LSolve is δ close in every coordinate to the actual vector. Note that since the dependence on the tolerance on the running time of Theorem 3.1 is logarithmic, the running time of LSolve remains ˜ O ( m log 1 / ε ) .

3.4 Linearity of the Laplacian Solver

While the running time of LSolve is ˜ O ( m log 1 / ε ) , the algorithm pro- duces an approximate solution that is oﬀ by a little from the exact solution. This creates the problem of estimating the error accumula- tion as one iterates this solver. To get around this, we note an impor- tant feature of LSolve : On input L, b , and ε, it returns the vector x = Z b , where Z is an n × n matrix and depends only on L and ε. Z is a symmetric linear operator satisfying

and has the same image as L. Note that in applications, such as that in Section 9, for Z to satisfy Z − L + ≤ ε, the running time is increased to ˜ O ( m log( 1 / ελ 2 ( L ) )) . Since in most of our applications λ 2 ( L ) is at least / poly( n ) , we ignore this distinction.

Notes

A good text to learn about matrices, their norms, and the inequalities that relate them is [17]. The pseudo-inverse is sometimes also referred to as the Moore–Penrose pseudo-inverse. Laplacian systems arise in many areas such as machine learning [15, 56, 89], computer vision [57, 73], partial diﬀerential equations and interior point methods [24, 30], and solvers are naturally needed; see also surveys [75] and [84].

We will see several applications that reduce fundamental graph problems to solving a small number of Laplacian systems in Sections 5–12. Sections 8 and 9 (a result of which is assumed in Sections 5 and 6) require the Laplacian solver to be linear. The notable applications we will not be covering are an algorithm to sample a ran- dom spanning tree [45] and computing multicommodity ﬂows [46].

Theorem 3.1 was ﬁrst proved in [77] and the full proof appears in a series of papers [78, 79, 80]. The original running time contained a very large power of the log n term (hidden in ˜ O ( · )). This power has since been reduced in a series of work [8, 9, 49, 62] and, ﬁnally, brought down to log n in [50]. We provide a proof of Theorem 3.1 in Section 18 along with its linearity property mentioned in this section. Recently, a simpler proof of Theorem 3.1 was presented in [47]. This proof does not require many of the ingredients such as spectral sparsiﬁers (Section 10), pre- conditioning (Section 17), and conjugate gradient (Section 16). While we present a sketch of this proof in Section 14, we recommend that the reader go through the proof in Section 18 and, in the process, familiar- ize themselves with this wide array of techniques which may be useful in general.

This section introduces how a graph can be viewed as an electrical network composed of resistors. It is shown how voltages and currents can be computed by solving a Laplacian system. The notions of eﬀec- tive resistance, electrical ﬂows, and their energy are presented. Viewing graphs as electrical networks will play an important role in applications presented in Sections 10–12 and in the approach to a simple proof of Theorem 3.1 presented in Section 14.

4.1 Incidence Matrices and Electrical Networks

Given an undirected, unweighted graph G = ( V,E ) , consider an arbi- trary orientation of its edges. Let B ∈ {− 1 , 0 , 1 } m × n be the matrix whose rows are indexed by the edges and columns by the vertices of G where the entry corresponding to ( e,i ) is 1 if a vertex i is the tail of the directed edge corresponding to e, is − 1 if i is the head of the directed edge e , and is zero otherwise. B is called the (edge-vertex) incidence matrix of G . The Laplacian can now be expressed in terms of B. While B depends on the choice of the directions to the edges, the Laplacian does not.

4.1 Incidence Matrices and Electrical Networks

Lemma 4.1. Let G be a graph with (arbitrarily chosen) incidence matrix B and Laplacian L . Then, B aT
 B = L .

Proof. For the diagonal elements of B a7
 B , i.e., ( B a7]
 B ) i,i = e B i,e B e,i . The terms are nonzero only for those edges e which are incident to i , in which case the product is 1, and hence, this sum gives the degree of ver- tex i in the undirected graph. For other entries, ( B 27]
 B ) i,j = degr
 e B i,e B e,j . The product terms are nonzero only when the edge e is shared by i and j. In either case the product is − 1. Hence, ( B a7
 B ) i,j = − 1 , ∀ i = j , ij ∈ E . Hence, B 37
 B = L .

Now we associate an electrical network to G. Replace each edge with a resistor of value 1 . To make this circuit interesting, we need to add power sources to its vertices. Suppose c ext ∈ R n is a vector which indicates how much current is going in at each vertex. This will induce voltages at each vertex and a current across each edge. We capture these by vectors v ∈ R n and i ∈ R m , respectively. Kirchoﬀ’s law asserts that, for every vertex, the diﬀerence of the outgoing current and the incoming current on the edges adjacent to it equals the external current input at that vertex. Thus,

On the other hand, Ohm’s law asserts that the current in an edge equals the voltage diﬀerence divided by the resistance of that edge. Since in our case all resistances are one, this gives us the following relation.

Combining these last two equalities with Lemma 4.1 we obtain that

If €
 c ext , 1 1)
 = 0, which means there is no current accumulation inside the electrical network, we can solve for v = L + c ext . The voltage vector is not unique since we can add the same constant to each of its entries and it still satisﬁes Ohm’s law. The currents across every edge, however,

are unique. Deﬁne vectors e i ∈ R n for i ∈ V which have a 1 at the i -th location and zero elsewhere. Then the current through the edge e = ij , taking into account the sign, is ( e i − e j ) \T,
 v = ( e i − e j ) 17]
 L + c ext . Thus, computing voltages and currents in such a network is equivalent to solving a Laplacian system.

Eﬀective Resistance and the Π Matrix

Now we consider a speciﬁc vector c ext and introduce an important quantity related to each edge, the eﬀective resistance. Consider c ext def = e i − e j . The voltage diﬀerence between vertices i and j is then ( e i − e j ) L + ( e i − e j ). When e = ij is an edge, this quantity is called the eﬀective resistance of e.

Deﬁnition 4.1. Given a graph G = ( V,E ) with Laplacian L and edge e = ij ∈ E,

R eﬀ ( e ) is the potential diﬀerence across e when a unit current is inducted at i and taken out at j .

We can also consider the current through an edge f when a unit current is inducted and taken out at the endpoints of a possibly dif- ferent edge e ∈ E. We capture this by the Π matrix which we now deﬁne formally. Let us denote the rows from the B matrix b e and b f respectively. Then,

Note also that Π( e,e ) is the eﬀective resistance of the edge e. The matrix Π has several interesting properties. The ﬁrst is trivial and follows from the fact the Laplacian and, hence, its pseudo-inverse is symmetric.

The third equality comes from the fact that the rows of B are orthog- onal to 1 .

Proposition 4.4. The eigenvalues of Π are all either 0 or 1.

Hence, it is an easy exercise to prove that if G is connected, then rank(Π) = n − 1 . In this case, Π has exactly n − 1 eigenvalues which are 1 and m − n + 1 eigenvalues which are 0 .

Finally, we note the following theorem which establishes a connec- tion between spanning trees and eﬀective resistances. While this theo- rem is not explicitly used in this monograph, the intuition arising from it is employed in sparsifying graphs.

Theorem 4.5. Let T be a spanning tree chosen uniformly at random from all spanning trees in G . Then, the probability that an edge e belongs to T is given by

Given a graph G = ( V,E ) , we ﬁx an orientation of its edges thus resulting in an incidence matrix B. Moreover, we associate unit

resistance with each edge of the graph. Now suppose that for vertices s and t one unit of current is injected at vertex s and taken out at t. The electrical ﬂow on each edge is then captured by the vector

In general, an s,t - ﬂow is an assignment of values to directed edges such that the total incoming ﬂow is the same as the total outgoing ﬂow at all vertices except s and t. For a ﬂow vector f , the energy it consumes is deﬁned to be

By taking the summation inside and noting that L = B a7
 B = e b e b e , Equation (4.1) is equivalent to

( e s − e t ) L + LL + ( e s − e t ) = ( e s − e t ) 7]
 L + ( e s − e t ) .

Thus we have proved the following proposition.

Proposition 4.6. If f is the unit s,t -ﬂow, then its energy is

The following is an important property of electrical s,t -ﬂows and will be useful in ﬁnding applications related to combinatorial ﬂows in graphs.

Theorem 4.7. Given a graph G = ( V,E ) with unit resistances across all edges, if f def = BL + ( e s − e t ), then f minimizes the energy con- sumed E ( f ) def = e f 2 e among all unit ﬂows from s to t .

Proof. Let Π = BL + B 37
 be as before. Proposition 4.3 implies that Π 2 = Π and, hence, for all vectors g , g 2 ≥ 7
 Π g Z|
 2 where equality holds

E ( f ) = f f\?
 2 ≥ ld
 Π f f |?
 2 = f sTy
 ΠΠ f = f Π f = f BL + B f .

Hence, using the fact that B aT
 f = e s − e t , one obtains that

Hence, for any unit s,t -ﬂow f , E ( f ) ≥ E ( f ) .

4.4 Weighted Graphs

Our results also extend to weighted graphs. Suppose the graph G = ( V,E ) has weights given by w : E (cid:9)→ R ≥ 0 . Let W be the m × m diagonal matrix such that W ( e,e ) = w ( e ) . Then, for an incidence matrix B given an orientation of G, the Laplacian is L def = B 27]
 W B. The Π matrix in this setting is W 1 / 2 BL + B a7]
 W 1 / 2 . To set up a cor- responding electrical network, we associate a resistance r e def = 1 / w ( e ) with each edge. Thus, for a given voltage vector v , the current vector, by Ohm’s Law, is i = W B v . The eﬀective resistance remains R eﬀ ( e ) = ( e i − e j ) L + ( e i − e j ) where L + is the pseudo-inverse of the Laplacian which involves W. For vertices s and t, the unit s,t -ﬂow is f ee
 def = W BL + ( e s − e t ) and its energy is deﬁned to be and
 e r e ( f (cid:1)e ) 2 , which turns out to be ( e s − e t ) L + ( e s − e t ) . It is an easy exercise to check that all the results in this section hold for this weighted setting.

Notes

The connection between graphs, random walks, and electrical networks is an important one, and its study has yielded many surprising results. The books [25] and [54] are good pointers for readers intending to explore this connection.

While we do not need Theorem 4.5 for this monograph, an interested reader can try to prove it using the Matrix-Tree Theorem, or refer to [34]. It forms the basis for another application that uses Laplacian solvers to develop fast algorithms: Generating a random spanning tree in time ˜ O ( mn 1 / 2 ) , see [45].

Applications

This section introduces the fundamental problem of ﬁnding a cut of least conductance in a graph, called Sparsest Cut . A quadratic pro- gram is presented which captures the Sparsest Cut problem exactly. Subsequently, a relaxation of this program is considered where the optimal value is essentially the second eigenvalue of the normalized Laplacian; this provides a lower bound on the conductance of the graph. In Sections 6 and 7 this connection is used to come up with approxima- tion algorithms for the Sparsest Cut and related Balanced Edge- Separator problems. Finally, in Section 8, it is shown how Laplacian solvers can be used to compute the second eigenvector and the associ- ated second eigenvector in ˜ O ( m ) time.

Graph Conductance

Given an undirected, unweighted graph G = ( V,E ) with n vertices and m edges, we are interested in vertex cuts in the graph. A vertex cut is a partition of V into two parts, S ⊆ V and ¯ S def = V \ S, which we denote by ( S, ¯ S ) . Before we go on to deﬁne conductance, we need a way to measure the size of a cut given by S ⊆ V. One measure is its

Graph Partitioning I The Normalized Laplacian

cardinality | S | . Another is the sum of degrees of all the vertices in S. If the graph is regular, i.e., all vertices have the same degree, then these two are the same up to a factor of this ﬁxed degree. Otherwise, they are diﬀerent and a part of the latter is called the volume of the set. Formally, for S ⊆ V, the volume of S is vol( S ) def = i ∈ S d i , where d i is the degree of vertex i. By a slight abuse of notation, we deﬁne vol( G ) def = vol( V ) = i ∈ V d i = 2 m. The number of edges that cross the cut ( S, ¯ S ), i.e., have one end point in S and the other in ¯ S, is denoted | E ( S, ¯ S ) | . Now we deﬁne the conductance of a cut.

Deﬁnition 5.1. The conductance of a cut ( S, ¯ S ) (also referred to as its normalized cut value or cut ratio) is deﬁned to be

The conductance of a cut measures the ratio of edges going out of a cut to the total edges incident to the smaller side of the cut. This is always a number between 0 and 1 . Some authors deﬁne the conductance to be

where | S | denotes the cardinality of S. The former deﬁnition is preferred to the latter one as φ ( S ) lies between 0 and 1 in case of the former while there is no such bound on φ ( S ) in the latter. Speciﬁcally, the latter is not a dimension-less quantity: if we replace each edge by k copies of itself, this value will change, while the value given by the former deﬁnition remains invariant. The graph conductance problem is to compute the conductance of the graph which is deﬁned to be

This problem is often referred to as the Sparsest Cut problem and is NP-hard. This, and its cousin, the Balanced Edge-Separator problem (to be introduced in Section 7) are intensely studied, both in theory and practice, and have far reaching connections to spectral graph theory, the study of random walks, and metric embeddings. Besides

being theoretically rich, they are of great practical importance as they play a central role in the design of recursive algorithms, image segmen- tation, community detection, and clustering.

Another quantity which is closely related to the conductance and is often easier to manipulate is the following.

Deﬁnition 5.2. For a cut ( S, ¯ S ) , the h -value of a set is deﬁned to be

We ﬁrst observe the relation between h and φ.

Thus, the h -value captures the conductance of a graph up to a factor of 2. Computing the h -value of a graph is not any easier than comput- ing its conductance. However, as we see next, it can be formulated as a mathematical program which can then be relaxed to an eigenvalue problem involving the Laplacian.

5.2 A Mathematical Program

We will write down a mathematical program which captures the h

value of the conductance. First, we introduce some notation which will be useful.

Graph Partitioning I The Normalized Laplacian

Probability Measures on Vertices and Cuts as Vectors Let ν : E → [

,

] be the uniform probability measure deﬁned on the of edges E ,

Graph Partitioning I The Normalized Laplacian

Probability Measures on Vertices and Cuts as Vectors

Let ν : E → [0 , 1] be the uniform probability measure deﬁned on the set of edges E ,

For a subset of edges F ⊆ E, ν ( F ) def = e ∈ F ν ( e ) . Next, we consider a measure on the vertices induced by the degrees. For i ∈ V,

Note that µ is a probability measure on V. We extend µ to subsets

Note that µ is a probability measure on V. We extend µ to subsets S ⊆ V ,

With these deﬁnitions it follows that

Given S ⊆ V , let 1 S : V (cid:9)→ { 0 , 1 } denote the indicator function for the set S by

Therefore,

( i,j ) ← µ × µ [(

S ( i )

S ( j )) ] = P ( i,j ) ← µ × µ [(

S ( i ) −

S ( j ))

=

] = P ( i,j ) ← µ × µ [ i ∈ S,j ∈ ¯ S or i ∈ ¯ S,j ∈ S ] = P i ← µ [ i ∈ S ] P j ← µ [ j ∈ ¯ S ] + P i ← µ [ i ∈ ¯ S ] P j ← µ [ j ∈ S (since i and j are chosen independently) = µ ( S ) µ ( ¯ S ) + µ ( ¯ S ) µ ( S ) =

µ ( S ) µ ( ¯ S ) . Therefore, h ( S ) = ν ( E ( S, ¯ S )) ¯ = E ij ← ν [(

S ( i ) −

S ( j ))

] E [(

( i ) −

( j ))

] .

Hence, noting the one-to-one correspondence between sets S ⊆ V and functions x ∈ { 0 , 1 } n , we have proved the following mathematical pro- gramming characterization of the conductance.

As noted before, this quantity in the right-hand side (r.h.s.) of the lemma above is hard to compute. Let us now try to relax the condition that x ∈ { 0 , 1 } n to x ∈ R n . We will refer to this as the real conductance; this notation, as we will see shortly, will go away.

Deﬁnition 5.3. The real conductance of a graph G is

Since we are relaxing from a 0 / 1 embedding of the vertices to a real embedding, it immediately follows from the deﬁnition that h R ( G ) is at

Graph Partitioning I The Normalized Laplacian

most h ( G ) . The optimal solution of the optimization problem above provides a real embedding of the graph. Two questions need to be addressed: Is h R ( G ) eﬃciently computable? How small can h R ( G ) get when compared to h ( G ); in other words, how good an approximation is h R ( G ) to h ( G )? In the remainder of the section we address the ﬁrst problem. We show that computing h R ( G ) reduces to computing an eigenvalue of a matrix, in fact closely related to the graph Laplacian. In the next section we lower bound h R ( G ) by a function in h ( G ) and present an algorithm that ﬁnds a reasonable cut using h R .

The Normalized Laplacian and Its Second Eigenvalue

Recall that h R ( G ) def = min x ∈ R n E ij ← ν [( x i − x j ) 2 ] E ( i,j ) ← µ × µ [( x i − x j ) 2 ] . Note that the r.h.s. above remains unchanged if we add or subtract the same quantity to every x i . One can thereby subtract E i ← µ [ x i ] from every x i , and assume that we optimize with an additional constraint on x , namely, E i ← µ [ x i ] = 0 . This condition can be written as x ,D 1 = 0 . First note the following simple series of equalities for any x ∈ R n based on simple properties of expectations.

Therefore, by our assumption, E ( i,j ) ← µ × µ [( x i − x j ) 2 ] = 2 E i ← µ [ x i 2 ] . Hence, we obtain

5.3 The Normalized Laplacian and Its Second Eigenvalue 35

Moreover, from the deﬁnition of L it follows that

E ij ← ν ( x i − x j ) 2 = e = ij ∈ E ( x i − x j ) 2 m = x 7]
 L x m = 2 x L x vol( G ) . Therefore, we can write h R ( G ) = min x ∈ R n , x ,D 1 =0 x TY
 L x x D x .

This is not quite an eigenvalue problem. We will now reduce it to one. Substitute y def = D 1 / 2 x in the equation above. Then,

This is an eigenvalue problem for the following matrix which we refer to as the normalized Laplacian.

Deﬁnition 5.4. The normalized Laplacian of a graph G is deﬁned to be

Deﬁnition 5.4. The normalized Laplacian of a graph G is deﬁned

where D is the degree matrix and L is the graph Laplacian as in Section 2.

Note that L is symmetric and, hence, has a set of orthonormal eigenvec- tors which we denote D 1 / 2 1 = u 1 ,..., u n with eigenvalues 0 = λ 1 ( L ) ≤ ··· ≤ λ n ( L ) . Hence,

We summarize what we have proved in the following theorem.

Graph Partitioning I The Normalized Laplacian

Thus, we have a handle on the quantity λ 2 ( L ) , which we can compute eﬃciently. Can we use this information to recover a cut in G of conduc- tance close to λ 2 ( L )? It turns out that the eigenvector corresponding to λ 2 ( L ) can be used to ﬁnd a cut of small conductance, which results in an approximation algorithm for Sparsest Cut . This is the content of the next section.

Notes

For a good, though out-dated, survey on the graph partitioning problem and its applications see [74]. There has been a lot of activity on and around this problem in the last decade. In fact, an important compo- nent of the original proof of Theorem 3.1 by Spielman and Teng relied on being able to partition graphs in near-linear-time, see [79]. The proof of NP-hardness of Sparsest Cut can be found in [32]. Theorem 5.3 and Corollary 5.4 are folklore and more on them can be found in the book [23].

This section shows that the conductance of a graph can be roughly upper bounded by the square root of the second eigenvalue of the nor- malized Laplacian. The proof that is presented implies an algorithm to ﬁnd such a cut from the second eigenvector.

Sparse Cuts from b
 1 Embeddings

Recall the 22 problem which arose from the relaxation of h ( G ) and Theorem 5.3 1

We will relate the mathematical program that captures graph conduc- tance with an 1 -minimization program (Theorem 6.1) and then relate that 4
 1 -minimization to the 22 program (Theorem 6.3). In particular,

Here by 22 we mean that it is an optimization problem where both the numerator and the denominator are squared-Euclidean distances. This is not to be confused with an 22 metric space.

Graph Partitioning II A Spectral Algorithm for Conductance

Here, µ 1 / 2 ( y ) is deﬁned to be a t such that µ ( { i : y i < t } ) ≤ 1 / 2 and µ ( { i : y i > t } ) ≤ 1 / 2 . Note that µ 1 / 2 ( · ) is not uniquely deﬁned.

Theorem 6.1. For any graph G on n vertices, the graph conductance

Proof. Let ( S, ¯ S ) be such that φ ( S ) = φ ( G ). Without loss of general- ity (W.l.o.g.) assume that µ ( S ) ≤ µ ( ¯ S ). Then E ij ← ν [ | 1 S ( i ) − 1 S ( j ) | ] = ν ( E ( S, ¯ S )) and E i ← µ [ | 1 S ( i ) | ] = µ ( S ) = min { µ ( S ) ,µ ( ¯ S ) } . Since { i : 1 S ( i ) ≤ t } is ∅ for t < 0 and ¯ S for t = 0, we have µ 1 / 2 ( 1 S ) = 0. Combin- ing, we obtain

It remains to show that ij ← ν [ i j ] E i ← µ [ | y i | ] ≥ φ ( G ) for every y ∈ R n such that µ 1 / 2 ( y ) = 0. Fix an arbitrary y ∈ R with µ 1 / 2 ( y ) = 0. For convenience we assume that all entries of y are distinct. Re-index the vertices of G such that y 1 < y 2 < ··· < y n . This gives an embedding of the vertices of G on R . The natural cuts on this embedding, called sweep cuts , are given by S i def = { 1 ,...,i } , 1 ≤ i ≤ n − 1. Let S be the sweep cut with minimum conductance. We show that the conductance φ ( (3)
 S ) is a lower bound on E ij ← ν [ | y i − y j | ] E i ← µ [ | y i | ] . This completes the proof since φ ( S ) itself is bounded below by φ ( G ). First, note that

ij ← ν [ | y i − y j | ] = 1 m ij ∈ E | y i − y j | = 1 m ij ∈ E max { i,j }− 1 wtih
 l =min { i,j } ( y l +1 − y l ) = n − 1 l =1 ν ( E ( S l , ¯ S l ))( y l +1 − y l ) .

The third equality above follows since, in the double summation, the term ( y l +1 − y l ) is counted once for every edge that crosses ( S l , ¯ S l ). For sake of notational convenience we assign S 0 def = ∅ and S n def = V as two limiting sweep cuts so that { i } = S i − S i − 1 = ¯ S i − 1 − ¯ S i for every i ∈ [ n ]. Hence µ i = µ ( S i ) − µ ( S i − 1 ) = µ ( ¯ S i − 1 ) − µ ( ¯ S i ). We use this to express E i ← µ [ | y i | ] below. Assume that there is a k such that y k = 0 . Thus, since µ 1 / 2 ( y ) = 0 , µ ( ¯ S k ) ≤ µ ( S k ) . If no such k exists, the proof is only simpler.

← µ [ | y i | ] = n i =1 µ i | y i | = k i =1 µ i ( − y i ) + n i = k +1 µ i ( y i ) = k i =1 ( µ ( S i ) − µ ( S i − 1 ))( − y i ) + n i = k +1 ( µ ( ¯ S i − 1 ) − µ ( ¯ S i ))( = µ ( S 0 ) y 1 + k − 1 i =1 µ ( S i )( y i +1 − y i ) + µ ( S k )( − y k ) + µ ( ¯ S k ) y k +1 + n − 1 i = k +1 µ ( ¯ S i )( y i +1 − y i ) + µ ( ¯ S n )( − y n ) = k − 1 i =1 µ ( S i )( y i +1 − y i ) + n − 1 i = k µ ( ¯ S i )( y i +1 − y i ) (since µ ( S 0 ) = µ ( ¯ S n ) = 0 , y k = 0 and µ ( ¯ S k ) ≤ µ ( S k )) = n − 1 i =1 min { µ ( S i ) ,µ ( ¯ S i ) } ( y i +1 − y i ) .

Where the last equality follows since µ ( ¯ S i ) ≥ µ ( S i ) for all i ≤ k and µ ( ¯ S i ) ≤ µ ( S i ) for all i ≥ k . Putting Equations (6.1) and (6.2) together, we get the desired inequality:

40 Graph Partitioning II A Spectral Algorithm for Conductance

In the proof we have used the following simple proposition.

Proof. Let min i a i / b i = θ. Then, since b i ≥ 0 , for all i, a i ≥ θ · b i . Hence, i a i ≥ θ · i b i . Thus, i a i / i b i ≥ θ.

The next theorem permits us to go from an a:
 22 solution to an 1 solution which satisﬁes the conditions of Theorem 6.1 with a quadratic loss in the objective value.

Theorem 6.3. If there is an x ∈ R n such that E ij ← ν [( x i − x j ) ] E ( i,j ) ← µ × µ [( x i − x j ) 2 ] = ε then there exists a y ∈ R n with µ 1 / 2 ( y ) = 0 such that E ij ← ν [ | y i − y j | ] E i ← µ [ | y i | ] ≤ 2 √ ε .

Before we prove this theorem, we will prove a couple simple propositions which are used in its proof. For y ∈ R , let sgn( y ) = 1 if y ≥ 0 , and − 1 otherwise.

(

) If sgn( y ) = sgn( z ) , then | sgn( y ) · y

− sgn( z ) · z

| = | y

− z

| = ( y − z ) · | y + z | = ( y − z )( | y | + | z | ) as y ≥ z. (

) If sgn( y ) = sgn( y ) , then y ≥ z, ( y − z ) = | y | + | z | . Hence, | sgn( y ) · y

− sgn( z ) · z

| = y

+ z

≤ ( | y | + | z | )

= ( y − z ) ( | y | + | z | ) .

Proof. Observe that a 2 + b 2 − 2 ab ≥ 0 . Hence, 2 a 2 + 2 b 2 − 2 ab ≥ b 2 . Hence,

Proof. [of Theorem 6.3] Since the left-hand side (l.h.s.) of the hypothe- sis is shift invariant for x , we can assume w.l.o.g. that µ 1 / 2 ( x ) = 0. Let y = ( y 1 ,...,y n ) be deﬁned such that

Hence, µ 1 / 2 ( y ) = 0. Further, E ij ← ν [ | y i − y j | ] Prop. 6.4 ≤ E ij ← ν [ | x i − x j | ( | x i | + | x j | )] Cauchy-Schwarz ≤ E ij ← ν [ | x i − x j | 2 ] E ij ← ν [( | x i | + | x j | ) 2 ] Prop. 6.5 ≤ E ij ← ν [( x i − x j ) 2 ] 2 E ij ← ν [ x i 2 + x j 2 ] double counting = E ij ← ν [( x i − x j ) 2 ] 2 E i ← µ [ x 2 i ] = E ij ← ν [( x i − x j ) 2 ] 2 E i ← µ [ | y i | ] . Hence, E ij ← ν [ | y i − y j | ] E i ← µ [ | y i | ] ≤ 2 E ij ← ν [( x i − x j ) 2 ] E i ← µ [ | y i | ]

42 Graph Partitioning II A Spectral Algorithm for Conductance

= 2 E ij ← ν [( x i − x j ) 2 ] E i ← µ [ x 2 i ] ≤ 4 E ij ← ν [( x i − x j ) 2 ] E ( i,j ) ← µ × µ [( x i − x j ) 2 ] (since E ( i,j ) ← µ × µ [( x i − x j ) = 2 √ ε.

As a corollary we obtain the following theorem known as Cheeger’s inequality.

Proof. The ﬁrst inequality was proved in the last section, see Corol- lary 5.4. The second inequality follows from Theorems 6.1 and 6.3 by choosing x to be the vector that minimizes E ij ← ν [( x i − x j ) 2 ] E ( i,j ) ← µ × µ [( x i − x j ) 2 ] .

It is an easy exercise to show that Cheeger’s inequality is tight up to constant factors for the cycle on n vertices.

Notes

Theorem 6.6 is attributed to [7, 20]. There are several ways to prove Theorem 6.3 and the proof presented here is inﬂuenced by the work of [58].

In terms of approximation algorithms for Sparsest Cut , the best result is due to [13] who gave an O ( √ log n )-factor approximation algo- rithm. Building on a sequence of work [10, 12, 48, 61], Sherman [71] showed how to obtain this approximation ratio of √ log n in essentially single-commodity ﬂow time. Now there are improved algorithms for this ﬂow problem, see Section 12, resulting in an algorithm that runs in time ˜ O ( m 4 / 3 ) . Obtaining a √ log n approximation in ˜ O ( m ) time, pos- sibly bypassing the reduction to single-commodity ﬂows, remains open. Toward this, getting a log n factor approximation in ˜ O ( m ) seems like a challenging problem. Finally, it should be noted that Madry [55] gave an algorithm that, for every integer k ≥ 1 , achieves, roughly, an O ((log n ) ( k + 1 / 2 ) ) approximation in ˜ O ( m + 2 k · n 1+2 − k ) time.

This section builds up on Section 6 to introduce the problem of ﬁnding sparse cuts that are also balanced. Here, a balance parameter b ∈ (0 , 1 / 2 ] is given and the goal is to ﬁnd the cut of least conductance among cuts whose smaller side has a fractional volume of at least b. We show how one can recursively apply Algorithm 6.1 to solve this problem.

The Balanced Edge-Separator Problem

In many applications, we not only want to ﬁnd the cut that minimizes conductance, but would also like the two sides of the cut to be comparable in volume. Formally, given a graph G and a balance parameter b ∈ (0 , 1 / 2 ], the goal is to ﬁnd a cut ( S, ¯ S ) with minimum conductance such that min { µ ( S ) ,µ ( ¯ S ) } ≥ b . This problem, referred to as Balanced Edge-Separator , is also NP-hard and we present an approximation algorithm for it in this section. In fact, we recur- sively use the algorithm SpectralCut presented in the previous section to give a pseudo -approximation algorithm to this problem: We present an algorithm ( BalancedCut , see Algorithm 7.1) that accepts an undirected graph G , a balance requirement b ∈ (0 , 1 / 2 ], and a conductance requirement γ ∈ (0 , 1) as input. If the graph contains

7.1 The Balanced Edge-Separator Problem

parameter b ∈ (

,

/

]. A target conductance parameter γ ∈ (

,

). Output: Either a b /

-balanced cut ( S, ¯ S ) with φ ( S ) ≤

√ γ or certify that every b -balanced cut in G has conductance at least γ .

: µ G ← degree measure of G

: H ← G

: S ← ∅

: repeat

: L H ← normalized Laplacian of H

: if λ

( L H ) >

γ then

: return NO

: end if

: S ← Spectra

ut ( H )

: S ← argmin { µ G ( S ) ,µ G ( V ( H ) \ S ) } ∪ S

: if min { µ G ( S ) ,µ G ( V ( G ) \ S ) } ≥ b /

then

: return ( S , ¯ S ) and ( S, ¯ S ) // At least one of the cuts will be shown to satisfy the output requirement of being b /

balanced and conductance at most

√ γ.

: else // Restrict the problem to the induced subgraph of H on V \ S with every edge that crosses V \ S to S replaced by a self-loop at the V \ S end

: V ”
 ← V ( H ) \ S

: E ← { ij : i,j ∈ V ”
 } ∪ { ii : ij ∈ E ( V ”
 ,S ) } // Note that E is multiset

: H = ( V m
 ,E )

: end if

: until

a b -balanced cut of conductance at most γ, then BalancedCut will return a b / 2 -balanced cut of conductance O ( √ γ ) , otherwise it will certify that every b -balanced cut in G has conductance at least γ. This b / 2 can be improved to (1 − ε ) b for any ε > 0 . Assuming this, even if the algorithm outputs a (1 − ε ) b balanced cut, the cut with least conductance with this volume could be signiﬁcantly smaller than γ. In this sense, the algorithm is a pseudo-approximation.

Graph Partitioning III Balanced Cuts

The Algorithm and Its Analysis

The Algorithm and Its Analysis

The following is the main theorem of this section.

Theorem 7.1. Given an undirected graph G, a balance parameter b ∈ (0 , 1 / 2 ] and a target conductance γ ∈ (0 , 1) , BalancedCut either ﬁnds a b / 2 -balanced cut in G of conductance at most O ( √ γ ) , or certiﬁes that every b -balanced cut in G has conductance more than γ.

The algorithm BalancedCut , which appears below, starts by check- ing if the second smallest eigenvalue of the normalized Laplacian of G, L is at most O ( γ ) , the target conductance. If not, then it outputs NO. On the other hand, if λ 2 ( L ) ≤ O ( γ ) , it makes a call to SpectralCut which returns a cut ( S 0 , ¯ S 0 ) of conductance at most O ( √ γ ) by Theo- rem 6.3. Assume that µ ( S 0 ) ≤ µ ( ¯ S 0 ). If ( S 0 , ¯ S 0 ) is already b / 2 -balanced, we return ( S 0 , ¯ S 0 ). Otherwise, we construct a smaller graph G 1 by removing S 0 from G and replacing each edge that had crossed the cut with a self-loop at its endpoint in ¯ S 0 . We always remove the smaller side of the cut output by SpectralCut from the current graph. This ensures that the total number of edges incident on any subset of ver- tices of G 1 is the same as it was in G, i.e., the volume of a subset does not change in subsequent iterations.

BalancedCut recurses on G 1 and does so until the relative volume of the union of all the deleted vertices exceeds b / 2 for the ﬁrst time. In this case, we are able to recover a b / 2 -balanced cut of conductance at most O ( √ γ ) . It is obvious that BalancedCut either outputs an NO or stops with an output of pair of cuts ( S, ¯ S ) and ( S , ¯ S ) . Since the graph is changing in every iteration, in the description of the algorithm and the analysis we keep track of the degree measure µ and the normalized Laplacian L by subscripting it with the graph involved. The following two lemmata imply Theorem 7.1.

Lemma 7.2. If BalancedCut deletes a set of degree measure less than b / 2 before it returns an NO, then every b -balanced cut in G has conductance at least γ.

Proof. When BalancedCut stops by returning an NO, we know that

(

) λ

( L H ) >

γ, and (

) vol( V ( H )) ≥ (

− b /

)vol( G ) .

Suppose for sake of contradiction that there is a b -balanced cut in G of conductance at most γ. Then, such a cut when restricted to H has relative volume at least b / 2 , and the number of edges going out of it in H is at most those going out in G. Thus, since the volume has shrunk by a factor of at most 2 and the edges have not increased, the conductance of the cut in H is at most 2 times that in G. Hence, such a cut will have conductance at most 2 γ in H. But λ 2 ( L H ) > 4 γ and hence, from the easy side of Cheeger’s inequality (Corollary 5.4), we can infer that φ ( H ) , even with degrees from G, is at least λ 2 ( L H ) / 2 > 2 γ. Thus, H does not have a cut of conductance at most 2 γ.

Lemma 7.3. When BalancedCut terminates by returning a pair of cuts, one of the two cuts it outputs has a relative volume at least b / 2 and conductance at most 4 √ γ.

Proof. There are two cases at the time of termination: If ( S H
 , ¯ S 1)
 ) and ( S, ¯ S ) are returned, either

(

) vol( S s")
 ) / vol( G ) ≥

/

, or (

) b /

≤ vol( S ) / vol( G ) ≤

/

In the ﬁrst case, since we stopped the ﬁrst time the relative volume exceeded b / 2 (and in fact went above 1 / 2 ) , it must be the case that output ( S, ¯ S ) of SpectralCut in that iteration is b / 2 -balanced. The conductance of this cut in H is at most 2 √ 4 γ = 4 √ γ by the guarantee of SpectralCut . Hence, the lemma is satisﬁed.

In the second case, we have that ( S , ¯ S ) is b / 2 -balanced. What about its conductance? S 3
 , the smaller side, consists of the union of disjoint cuts S 0 ,S 1 ,...,S t for some t and each is of conductance at most 4 √ γ in the respective G i by the guarantee of SpectralCut . We argue that the conductance of S is also at most 4 √ γ in G. First note that

| E G ( S , ¯ S ) | ≤ | E G ( S 0 , ¯ S 0 ) | + | E G 1 ( S 1 , ¯ S 1 ) | + ··· + | E G t ( S t , ¯ S t ) | .

This is because every edge contributing to the l.h.s. also contributes to the r.h.s. The inequality occurs because there could be more, e.g., if there is an edge going between S

and S

. Now, the conductance of each S i in G i is at most

√ γ. Hence,

| E G ( S , ¯ S ) | ≤ 4 γ · vol G ( S 0 )+4 γ · vol G 1 ( S 1 )+ ··· +4 γ · vol G t ( S t )

Finally, note that vol G i ( S i ) = vol G ( S i ) for all i due to the way in which we split the graph but retain the edges adjacent to the removed edges as self-loops. Hence, vol G ( S 3)
 ) = he ac
 ti =0 vol G i ( S i ) . Thus,

This concludes the proof of Theorem 7.1.

Notes

Theorem 7.1 is folklore and appears implicitly in [79] and [13]. There are other nearly-linear time algorithms, where we allow the running time to depend on 1 / γ for the BalancedEdge-Separator problem which achieve a γ versus √ γ polylog n bound, see for instance [8, 9, 79]. These algorithms are local in the sense that they instead bound the total work by showing that each iteration runs in time proportional to the smaller side of the cut. This approach was pioneered in [79] which relied on mixing time results in [53]. The ﬁrst near-linear algorithm for BalancedEdge-Separator that removes polylog n factors in the approximation was obtained by [62]. Recently, this dependence of the running time on γ has also removed and an ˜ O ( m ) time algorithm, which achieves a γ versus √ γ bound, has been obtained by Orecchia et al. [60]. Their proof relies crucially on Laplacian solvers and highlights of their algorithm are presented in Sections 9 and 19.

Graph Partitioning IV Computing the Second Eigenvector

This ﬁnal section on graph partitioning addresses the problem of how to compute an approximation to the second eigenvector, using the near- linear-time Laplacian solver, a primitive that was needed in Sections 6

8.1 The Power Method

Before we show how to compute the second smallest eigenvector of the normalized Laplacian of a graph, we present the power method. This is a well-known method to compute an approximation to the largest eigenvector of a matrix A. It start with a random unit vector and repeatedly applies A to it while normalizing it at every step.

Lemma 8.1. Given a symmetric matrix A ∈ R n × n , an error parameter ε > 0 and a positive integer k > 1 / 2 ε log( 9 n / 4 ) , the following holds with probability at least 1 / 2 over a vector v chosen uniformly at random from the n -dimensional unit sphere,

where λ n ( A ) is the eigenvalue with the largest magnitude of A.

Proof. Let u

,..., u n be the eigenvectors of A corresponding to the eigenvalues | λ

| ≤ ··· ≤ | λ n | . We can write the random unit vector v in the basis { u i } i ∈ [ n ] as ni =

α i u i . From a standard calculation involving Gaussian distributions we deduce that, with probability at least

/

, | α n | ≥

√ n . Using H¨older’s inequality,

i ≤ i α 2 i λ 2 k +2 i k / k +1 i α 2 i 1 / k +1 = i α 2 i λ 2 k +2 i k / k +1 = A k +1 v 2 k / k +1 .

Note that A k +1 v 2 / k +1 ≥ α 2 / k +1 n λ 2 n . Thus, it follows that

Substituting k +

≥

/

ε log(

n /

) in the r.h.s. above gives | α n |

/ k +

≥ e − ε ≥

− ε, completing the proof.

The Second Eigenvector via Powering The following is the main theorem of this section.

Substituting k + 1 ≥ 1 / 2 ε log( 9 n / 4 ) in the r.h.s. above gives | α n | 1 / k +1 ≥ e − ε ≥ 1 − ε, completing the proof.

The following is the main theorem of this section.

Theorem 8.2. Given an undirected, unweighted graph G on n vertices and m edges, there is an algorithm that outputs a vector x such that

and runs in ˜ O ( m + n ) time. Here L is the normalized Laplacian of G.

1 For vectors u , v , and p,q ≥ 0 such that 1 / p + 1 / q = 1 , |(cid:11) u , v (cid:12)| ≤ u p v q . In this applica- tion we choose p def = k + 1 ,q def = k +1 / k and u i def = α 2 / k +1 i ,v i def = α 2 k / k +1 i λ 2 ki .

8.2.1 The Trivial Application

Let us see what the power method in Lemma 8.1 gives us when we apply it to I − L , where L is the normalized Laplacian of a graph G. Recall that L = I − D − 1 / 2 AD − 1 / 2 , where D is the degree matrix of G and A its adjacency matrix. Since we know the largest eigenvector for D − 1 / 2 AD − 1 / 2 explicitly, i.e., D 1 / 2 1 , we can work orthogonal to it by removing the component along it from the random starting vector. Thus, the power method converges to the second largest eigenvector of D − 1 / 2 AD − 1 / 2 , which is the second smallest eigenvector of L . Hence, if we apply the power method to estimate λ 2 ( L ) , we need ε ∼ λ 2 ( L ) / 2 in order to be able to approximate λ 2 ( L ) up to a factor of 2. This requires the computation of ( D − 1 / 2 AD − 1 / 2 ) k v for k = Θ( log n / λ 2 ( L ) ) , which gives a time bound of O ( m log n / λ 2 ( L ) ) since λ 2 ( L ) can be as small as 1 / m and, hence, k may be large. In the next section we see how to remove this dependency on 1 / λ 2 ( L ) and obtain an algorithm which runs in time ˜ O ( m ) and computes a vector that approximates λ 2 ( L ) to within a factor of 2 .

8.2.2 Invoking the Laplacian Solver

Start by observing that λ 2 def = λ 2 ( L ) is the largest eigenvalue of L + , the pseudo-inverse of L . Thus, if we could compute L + u , for a vector u which is orthogonal to D 1 / 2 1 , it would suﬃce to use the power method with ε = 1 / 2 in order to approximate λ 2 up to a factor of 2 . Hence, we would only require k = Θ(log n ) . Unfortunately, computing L + exactly is an expensive operation. Theorem 3.1 from Section 3 implies that there is an algorithm, LSolve , that given v can approximate L + v in time O ( m log n / √ λ 2 ) . Roughly, this implies that the total time required to estimate λ 2 via this method is O ( m (log n ) 2 / √ λ 2 ) . This is the limit of meth- ods which do not use the structure of L . Note that L + = D 1 / 2 L + D 1 / 2 hence, it is suﬃcient to compute L + v when v , 1 = 0 .

Recall the procedure LSolve in Theorem 3.1 and its linearity in Section 3.4. An immediate corollary of these results is that there is a randomized procedure which, given a positive integer k, a graph Laplacian L, a vector v ∈ R n , and an ε > 0 , returns the vector Z k v , where Z is the symmetric linear operator implicit in LSolve satisfying

(

− ε /

) Z + L (

+ ε /

) Z + with the same image as L. Moreover, this algorithm can be implemented in time O ( mk log n log

/ ε ) .

(1 − ε / 4 ) Z + L (1 + ε / 4 ) Z + with the same image as L. Moreover, this algorithm can be implemented in time O ( mk log n log 1 / ε ) .

Coming back to our application, suppose we are given ε < 1 / 5 . We choose k so that the Rayleigh quotient of Z k v , which is supposed to be an approximation to ( L + ) k v , w.r.t. L + becomes at most (1 + ε ) λ 2 ( L ) for a random vector v . Thus, k is O ( 1 / ε log n / ε ) . The important point is that k does not depend on λ 2 ( L ) (or in the normalized case, λ 2 ( L )).

Let u 1 ,..., u n be the eigenvectors of Z + corresponding to the eigen- values λ 1 ≥ ··· ≥ λ n = 0 . (For this proof it turns out to be more con- venient to label the eigenvalues in decreasing order.) We express v in the basis { u i } i as v = nval
 i α i u i . We know that with probability at least 1 / 2 , we have, | α 1 | ≥ 2 / 3 √ n . Let j be the smallest index such that λ j ≤ (1 + ε / 8 ) λ n − 1 . Then,

) Z + ( Z k v ) v ) ( Z k v ) = n − 1 i =1 α 2 i λ − (2 k − 1) i n − 1 i =1 α 2 i λ − 2 k i ≤ ji =1 α 2 i λ − (2 k − 1) i n − 1 i =1 α 2 i λ − 2 k i + n − 1 i>j α 2 i λ − (2 k − 1) i n − 1 i>j α 2 i λ − 2 k i ≤ ji =1 α 2 i λ − (2 k − 1) i α 2 n − 1 λ − 2 k n − 1 + λ j ≤ λ n − 1 ji =1 α 2 i (1 + ε / 8 ) − (2 k − 1) α 2 n − 1 + (1 + ε / 8 ) λ n ≤ λ n − 1 · 9 n 4 j i =1 α 2 i e − ε (2 k − 1) 16 + (1 + ε / 8 ) λ n − 1 ≤ λ n − 1 ε 8 + λ n − 1 (1 + ε / 8 ) ≤ λ n − 1 (1 + ε / 4 ) ,

where the third last line has used (1 + x ) − 1 ≤ e − x / 2 for x ≤ 1 and k ≥ 8 / ε log 18 n / ε + 1 . Let ˆ v be the unit vector Z k v / Z k v . Thus, ˆ v L ˆ v ≤ (1 + ε / 4 )ˆ v Z + ˆ v ≤ (1 + ε / 4 ) 2 λ n − 1 ( Z + ) ≤ (1+ ε / 4 ) 2 1 − ε / 4 λ 2 ( L ) ≤ (1 + ε ) λ 2 ( L ) , where the last inequality uses ε < 1 / 5 . This, when combined with Lemma 8.1, completes the proof of Theorem 8.2.

Notes

Lemma 8.1 is folklore. Theorem 8.2 is from [78]. It is important to note that there is no known proof of Theorem 8.2 without invoking a Laplacian solver; it seems important to understand why.

The Matrix Exponential and Random Walks

This section considers the problem of computing the matrix exponen- tial, exp( − tL ) v , for a graph Laplacian L and a vector v . The matrix exponential is fundamental in several areas of mathematics and sci- ence, and of particular importance in random walks and optimization. Combining results from approximation theory that provide rational approximations to the exponential function with the Laplacian solver, an algorithm that approximates exp( − tL ) v up to ε -error is obtained. This algorithm runs in ˜ O ( m log t log 1 / ε ) time.

9.1 The Matrix Exponential

Suppose A is a symmetric n × n matrix. The matrix exponential of A is deﬁned to be

Since A is symmetric, an equivalent way to deﬁne exp( A ) is to ﬁrst write the spectral decomposition of A = y to
 ni =1 λ i u i u i , where λ i s are

the eigenvalues of A and u i s are its orthonormal eigenvectors. Then,

Thus, exp( A ) is a symmetric PSD matrix. This matrix plays a funda- mental role in mathematics and science and, recently, has been used to design fast algorithms to solve semideﬁnite programming formulations for several graph problems. In particular, it has been used in an intricate algorithm for Balanced Edge-Separator that runs in time ˜ O ( m ) and achieves the spectral bound as in Theorem 7.1. In such settings, the primitive that is often required is exp( − L ) v , where L is the graph Laplacian of a weighted graph. One way to compute an approximation to exp( − L ) v is to truncate the power series of exp( − L ) after t terms and output

If we want u − exp( − L ) v ≤ ε v , then one may have to choose t ∼ At
 L a
 + log 1 / ε . Thus, the time to compute u could be as large as O ( m ( L Li
 + log 1 / ε )) . In matrix terms, this algorithm uses the fact that the matrix polynomial ti =0 ( − 1) i L i i ! approximates exp( − L ) up to an error of ε when t ∼ L Li
 + log 1 / ε . This dependence on |
 L aT
 is prohibitive for many applications and in this section we present an algorithm where the dependence is brought down to log 7
 L |
 as in the following theorem.

Theorem 9.1. There is an algorithm that, given the graph Laplacian L of a weighted graph with n vertices and m edges, a vector v , and a parameter 0 < δ ≤ 1, outputs a vector u such that exp( − L ) v − u ≤ δ il
 v in time ˜ O (( m + n )log(1 + L a
 )polylog 1 / δ ).

The proof of this theorem essentially reduces computing exp( − L ) v to solving a small number of Laplacian systems by employing a powerful result from approximation theory. Before we give the details of this latter result, we mention a generalization of the Laplacian solver to

Notice that a graph Laplacian is SDD. Moreover, for any γ > 0 , I + γL is also SDD. While it is not straightforward, it can be shown that if one has black-box access to a solver for Laplacian systems, such as Theorem 3.1, one can solve SDD systems. We omit the proof.

Theorem 9.2. Given an n × n SDD matrix A with m nonzero entries, a vector b , and an error parameter ε > 0, it is possible to obtain a vector u such that

The time required for this computation is ˜ O ( m log n log( 1 / ( ε A + ) )) . Moreover, u = Z b where Z depends on A and ε, and is such that Z − A + ≤ ε.

Rational Approximations to the Exponential

The starting point for the proof of Theorem 9.2 is the following result which shows that simple rational functions provide uniform approxima- tions to exp( − x ) over [0 , ∞ ) where the error term decays exponentially with the degree. The proof of this theorem is beyond the scope of this monograph.

Theorem 9.3. There exists constants c ≥ 1 and k 0 such that, for any integer k ≥ k 0 , there exists a polynomial P k ( x ) of degree k such that,

A corollary of this theorem is that for any unit vector v ,

9.2 Rational Approximations to the Exponential

To see this, ﬁrst write the spectral decomposition L = i λ i u i u i where { u } ni =1 form an orthonormal basis. Then, note that

Here, we have used the spectral decomposition theorem from Section 1 on pseudo-inverses which can be readily justiﬁed. Thus, if v = i β i u i with i β 2 i = 1 , then

Thus, using orthonormality of { u i } ni =1 we obtain that

where Λ( L ) def = [ λ 1 ( L ) ,λ n ( L )]. Thus, by the choice of P k as in Theo- rem 9.3, since all eigenvalues of the Laplacian are nonnegative, we have proved the following lemma.

Lemma 9.4. There exists constants c ≥ 1 and k 0 such that, for any integer k ≥ k 0 , there exists a polynomial P k ( x ) of degree k such that, for any graph Laplacian L and a vector v ,

exp( − L ) v − P k (( I + L / k ) + ) v ≤ O ( k 2 − k ) YIIy
 v

Proof of Theorem 9.1

To use P k (( I + L / k ) + ) v as an approximation to exp( − L ) v , let us assume that P k is given to us as P k ( x ) = ki =0 c i x i with | c i | = O ( k k ) . This is not obvious and a justiﬁcation is needed to assume this. We omit the details. Thus, we need to compute

Here, we assume that on input v and ( I + L / k ) , the output of Theorem 9.2 is Z v where Z − ( I + L / k ) + ≤ ε. Since ( I + L / k ) + ≤ 1 and Z is ε -close to it, we obtain that Z ≤ (1 + ε ) . Thus, using the simple inequality that for a,b ∈ R ,

and we obtain that

Z j v − (( I + L / k ) + ) j v ≤ 2 εj (1 + ε ) j v

Therefore, by the triangle inequality,

Since u = ki =0 c i Z i v , combining the inequality above with Lemma 9.4, we get that

exp( − L ) v − u ≤ εk 2 (1 + ε ) k k O ( k ) Ny
 v + O ( k 2 − k ) YI
 v

9.3 Simulating Continuous-Time Random Walks

where we have used that max i | c i | = k O ( k ) . For a parameter δ, choose k = log 2 / δ and ε = δ (1+ L ) 2 k O ( k ) to obtain that

This completes the proof of Theorem 9.1.

Simulating Continuous-Time Random Walks

In this section we show how one can simulate continuous-time random walks on an undirected graph with m edges for time t in ˜ O ( m log t ) time. Not only are random walks of fundamental importance, the ability to simulate them in near-linear-time results in near-linear-time algorithms for the Balanced Edge-Separator problem. From an applications point of view, the bound on the running time is essentially independent of t. Now we phrase the problem and show how this simulation result is an immediate corollary of Theorem 9.1. For an undirected graph G, let D denote its degree matrix and A its adjacency matrix. Then, the tran- sition matrix of a one-step random walk in G is W def = AD − 1 . Starting with a distribution v at time 0 , the t -step transition probability of the discrete-time random walk on G is given by W t v . The continuous-time random walk on G for time t, also called the heat-kernel walk, is deﬁned by the following probability matrix:

Note that this can be interpreted as a the discrete-time random walk after a Poisson-distributed number of steps with rate t. Here, given a starting vector v , the goal is to compute We
 W t v . Before we show how to do this, note that W is not symmetric. However, it can be symmetrized by considering the spectrally equivalent matrix D − 1 / 2 W D 1 / 2 which is I − L where L is the normalized Laplacian of G. Thus, W = D 1 / 2 ( I − L ) D − 1 / 2 , and hence,

Now, it is easy to see that Theorem 9.1 applies since t L 0 and t L(cid:4) ≤ O ( t ) . Thus, given δ > 0 , we can use the algorithm in the proof

The Matrix Exponential and Random Walks of Theorem

to obtain a vector u such that

The approximation to W t v is then D 1 / 2 u . It follows that

iW,
 W t v − D 1 / 2 u ≤ δ sill
 D 1 / 2 · iy
 D − 1 / 2 v .

This proves the following theorem.

Theorem 9.5. There is an algorithm that, given an undirected graph G with m edges, a vector v , a time t ≥ 0 , and a δ > 0 , outputs a vector ˆ u such that

The time taken by the algorithm is ˜ O ( m log(1 + t )polylog 1 / δ ) . Here, d max is the largest degree of G and d min the smallest.

Notes

For more on the matrix exponential and the role it plays in the design of fast algorithms for semideﬁnite programs the reader is referred to sample applications as in [12, 40, 41, 60, 61, 62]. Two thesis on this topic are [44] and [59].

The reduction from SDD systems to Laplacian systems appears in [38]. Theorems 9.1 and 9.5 are implicit in the work of Orecchia et al. [60]. Theorem 9.3 on rational approximations was proved in [70]. The fact that the coeﬃcients can be computed and are bounded is not present in this paper, but can be proved. While this section shows how to reduce the computation of exp( − L v ) to polylog n computations of the form L + v , one may ask if the converse is true. Recently, Sachdeva and Vishnoi [69] answered this question by presenting a simple reduc- tion in the other direction: The inverse of a positive-deﬁnite matrix can be approximated by a weighted-sum of a small number of matrix expo- nentials. This proves that the problems of exponentiating and inverting are essentially equivalent up to polylog factors. This reduction makes

no use of the fact that L is a Laplacian, but only that it is a symmetric positive-semideﬁnite matrix.

positive-semideﬁnite matrix. For more on continuous-time random walks, the reader is referred to the book [

]. It is a challenging open problem to prove an analog of Theorem

for discrete-time (lazy) random walks: Find a vector u s.t. W t v − u ≤ ε v in ˜ O ( m log(

+ t )log

/ ε ) time.

Graph Sparsiﬁcation I Sparsiﬁcation via Eﬀective Resistances

In this section a way to spectrally sparsify graphs is introduced that uses the connection to electrical networks presented in Section 4. In the next section we show how sparse spectral sparsiﬁers can be computed in ˜ O ( m ) time using Laplacians. Spectral sparsiﬁers introduced in this section play a crucial role in the proof of Theorem 3.1 presented in Section 18.

10.1 Graph Sparsiﬁcation

In the cut sparsiﬁcation problem, one is given an undirected unweighted graph G = ( V,E ) and a parameter ε > 0 , and the goal is to ﬁnd a weighted graph H = ( V,E ) such that for every cut ( S, ¯ S ) of V, the weight of the edges that cross this cut in H is within a multiplicative 1 ± ε factor of the number of edges in G that cross this cut. The goal is to keep the number of edges (not counting weights) of H small. Moreover, ﬁnding H quickly is also important for applications. Benczur and Karger gave a remarkable algorithm which produces cut sparsiﬁers of size (i.e., number of edges) ˜ O ( n / ε 2 ) in time ˜ O ( m ) . Such a procedure has found use in many fundamental graph algorithms where it allows the running time to be brought down from its dependency on m to n

(note that m can be as big as Ω( n 2 )). In particular, cut sparsiﬁers can be used in the algorithms presented in Sections 6, 7, and 9.

A stronger notion than cut sparsiﬁcation is that of spectral sparsiﬁcation. This will play an important role in constructing Laplacian solvers in Section 18. However, in this section and the next, we show how to construct spectral sparsiﬁers in ˜ O ( m ) time using Lapla- cian solvers.

Deﬁnition 10.1. Given an undirected graph G = ( V,E ) with a param- eter ε > 0 , a weighted graph H = ( V,E ) is said to be an ε -spectral sparsiﬁer of G if

Where L G and L H denote the graph Laplacians for G and H , respec- tively.

The goal is to minimize the number of edges of H and construct it as quickly as possible. In particular, we consider whether spectral sparsi- ﬁers with ˜ O ( n / poly( ε ) ) edges exist and can be constructed in ˜ O ( m ) time. Before we go on, notice that if H is an ε -spectral sparsiﬁer for G then it is an ε -cut sparsiﬁer for G as well. To see this, plug the vectors x = 1 S , (the indicator vector for a cut ( S, ¯ S )), into the deﬁnition above. There are also easy-to-construct examples that show these notions are not the same. An ε -cut sparsiﬁer of G can be an arbitrarily bad spectral sparsiﬁer for G.

We show the existence of ε -spectral sparsiﬁers of G of size O ( n log n / ε 2 ) . In the next section, we show how to construct such a spar- siﬁer in time ˜ O ( m log 1 / ε ) . 1 Formally, we prove the following theorem.

Theorem 10.1. There exists a randomized algorithm that, given a graph G and an approximation parameter ε > 0, constructs a spectral sparisifer H of size O ( n log n / ε 2 ) with probability 1 − 1 / n .

If G is weighted, then there is also a dependency on the log of the ratio of the largest to the smallest weight in G in the running time. The running time crucially relies on the Spielman–Teng Laplacian solver.

In the next section we reﬁne this theorem to prove the following, which also includes a bound on the running time.

Theorem 10.2. There exists a randomized algorithm that, given a graph G and an approximation parameter ε > 0, constructs a spectral sparisifer H of size O ( n log n / ε 2 ) in time ˜ O ( m log 1 / ε ) with probability 1 − 1 / n .

Spectral Sparsiﬁcation Using Eﬀective Resistances

The proof of Theorem 10.1 relies on an edge sampling algorithm: Repeatedly sample (with replacement) edges from G according to a carefully chosen probability function over the edges and weight each sampled edge inversely to this probability. The resulting multiset of weighted edges, normalized by the number of samples, is the output of the algorithm. 2 Formally, let p e be the probability that edge e is cho- sen by the sampling algorithm. Let Y be the random variable such that P [ Y = e ] = p e . Let T be a parameter denoting the number of samples that are taken by the algorithm. Let Y 1 ,Y 2 ,...,Y T be i.i.d. copies of Y . Then the weighted multi-set of edges equals

What is the Laplacian of the associated graph H ? Let B be an inci- dence matrix for G and let b e be the column vector corresponding to edge e in B . Then, L G = e b e b e = B aT
 B . For notational conve- def √

nience, deﬁne u e def = b e / p e . Then, a simple calculation shows that

To analyze the eﬀectiveness of this scheme, ﬁrst note that

2 This multi-set can be converted to a weighted graph by additively combining weights of repeated edges.

10.2 Spectral Sparsiﬁcation Using Eﬀective Resistances 65

So far this is abstract because we have not speciﬁed the p e s. We want to choose p e so that L H behaves similar to L G w.h.p. This is where eﬀective resistances and the intuition from Theorem 4.5 come into play. We let

where R e = R eﬀ ( e ) is the eﬀective resistance of the edge e deﬁned to be b AT Y
 e L + G b e . 3 The normalization factor of n − 1 is due to the fact e R e = n − 1 . What is the intuition behind choosing an edge with probability proportional to its eﬀective resistance? Unfortunately, there does not exist a very satisfactory answer to this question. We give a rough idea here. By Theorem 4.5, R e is proportional to the probability of an edge being in a random spanning tree and choosing a small num- ber of random spanning trees from G independently seems like a good strategy to spectrally sparsify G. We move on to analyze this strategy.

Proof of Theorem 10.1

For a ﬁxed orientation of the edges of G, recall that the matrix Π def = BL + G B introduced in Section 4 satisﬁes the following properties:

(

) Π

= Π, (

) Let Π e denote the column of Π corresponding to edge e . Then R e = \l
 Π e

, (

)

e R e = n −

, and (

) Π is unitarily equivalent to n −

j =

e j e j , where e j is the j -th standard basis vector in R m . This is because Π is symmetric and PSD, and all its eigenvalues are

or

. Thus, if G is connected, the multiplicity of eigenvalue

is n −

and that of

is m − n +

If the graph G is weighted, then we choose p e proportional to w G ( e ) R e

Deﬁne v e def = Π e / √ p e . Let M def = v Y v Y and M i def = v Y i v Y i , i =

,

,...,T be i.i.d. copies of M . The following theorem is required. It is an analog of Chernoﬀ bound for sums of matrix-valued random variables.

Deﬁne v e def = Π e / √ p e . Let M def = v Y v Y and M i def = v Y i v Y i , i = 1 , 2 ,...,T be i.i.d. copies of M . The following theorem is required. It is an analog of Chernoﬀ bound for sums of matrix-valued random variables.

Theorem 10.3. Let ε > 0 be a small enough constant. Let M be a random, symmetric PSD matrix such that E [ M ] = I d , where I d is the d -dimensional identity matrix. Let ρ def = sup M At,
 M I.
 . Let T be a non- negative integer and let M 1 ,...,M T be independent copies of M. Then,

Theorem 10.3 also holds when E [ M ] = d j =1 e j e j for some d ≤ d , in which case the above bound holds with d in place of d . A slightly more general result, which is needed for our application, is that the bound also holds when E [ M ] is unitarily equivalent to the above matrix. This follows because the operator norm is unitarily invariant. As an application of Theorem 10.3, we ﬁrst calculate

which by Property (4) above is unitarily equivalent to n − 1 j =1 e j e j . Note that Property (2) implies that

Therefore 7,
 M i ≤ n − 1 for all i . Applying Theorem 10.3, we obtain

where Π def = 1 T v Y i v Y i . Setting T = O ( n log n / ε 2 ) ensures that this fail- ure probability is n − Ω(1) . How do we relate this to L H ? Observe that since Π = BL + G B , it follows that for any e :

Therefore, Π = 1 T v Y i v Y i = 1 T BL + G u Y i u Y i L + G B = BL + G L H L + G B , and Π = BL + G B 37
 = BL + G L G L + G B aT
 . Now, Π − Π 1)
 = sup x = 0 x ( Π − Π) x x x = sup x = 0 x BL + G ( L H − L G ) L + G B aT
 x x x .

Because G is connected, if z is a vector such that B z = 0 , then z must be parallel to the all 1s vector. Hence, for any z = 0 such that z , 1 1)
 = 0 , B z = 0 . Therefore, we can substitute x = B z in the above equation. Using the property that L G L + G z = z for any such z , we obtain

Π − Π il|
 ≥ sup z = 0 z , 1 =0 z B a7 y
 BL + G ( L H − L G ) L + G B a7
 B z z B B z = sup z = 0 z , 1 =0 z L G L + G ( L H − L G ) L + G L G z z L G z = sup z = 0 z , 1 =0 z ( L H − L G ) z z L G z . Combining with Equation (10.1) yields P    sup z = 0 z , 1 =0 z a7
 ( L H − L G ) z z L G z > ε    ≤ P [ (li
 si
 Π − Π 1
 > ε ] = n −

as desired. This completes the proof of Theorem 10.1.

Crude Spectral Sparsﬁcation

In this section we note that the algorithm and the proof underlying Theorem 10.1 can be modiﬁed to obtain a crude spectral sparsiﬁer.

Theorem 10.4. Consider graph G = ( V,E ) with edge weights w G , γ > 0 , and numbers q e such that q e ≥ w G ( e ) R e for all e . If W def = pats
 e q e , then the spectral sparsiﬁer in Theorem 10.1 that takes O ( W log W log 1 / γ ) samples from the probability distribution induced by the q e s produces a sampled graph H that satisﬁes

with probability 1 −

Notes

The notion of cut sparsiﬁcation was introduced in [16]. The state of the art on cut sparsiﬁcation can be found in [31].

Spectral sparsiﬁcation was introduced in [80] and plays a crucial role in the construction of Laplacian solvers, and the ﬁrst algorithm for a spectral sparsiﬁer was given in this paper. Their sparsiﬁer had size O ( n polylog n ) and could be constructed in ˜ O ( m ) time. Theorem 10.1 was proved in [76] where it is showed how to construct ε -spectral sparsiﬁers of size O ( n log n / ε 2 ) . This has been improved to O ( n / ε 2 ) in [14], coming close to the so-called Ramanujan bound.

Theorem 10.4 was observed in [49] and will be used in Section 18. Theorem 10.3, which is used in the proof of Theorem 10.1 was proved in [66] using a non-commutative Khinchine inequality. Subsequently, it has been proved in an elementary manner using the matrix exponential (introduced in Section 9) independently by several researchers, see [4].

Graph Sparsiﬁcation II Computing Electrical Quantities

In this section the near-linear-time Laplacian solver is used to approx- imately compute eﬀective resistances for all edges in ˜ O ( m ) time. This was needed in Section 10. This section starts by presenting the sim- pler task of approximately computing currents and voltages when a unit current is input at a vertex and taken out at another, again using Laplacian solvers. These primitives are also used in Section 12.

11.1 Computing Voltages and Currents

Given s,t ∈ V with one unit of current ﬂowing into s and one unit ﬂowing out of t, the goal is to compute the vector L + ( e s − e t ); the vector of voltages at the vertices of the graph. We employ Theorem 3.1 to compute this vector in an approximate sense in ˜ O ( m ) time. Recall that this theorem says there is an algorithm LSolve which takes a graph Laplacian L , a vector y , and an error parameter δ > 0, and returns an x satisfying

Note that for any two vectors v − w ∞ ≤ v − w . Hence, by a choice of δ def = ε / poly( n ) in Theorem 3.1, we can ensure the approximate vector of

Graph Sparsiﬁcation II Computing Electrical Quantities

voltages we produce is ε -close in every coordinate to the actual voltage vector. Since the dependence of the running time in Theorem 3.1 on the error tolerance is logarithmic, the running time remains ˜ O ( m log 1 / ε ) .

Thus, for any small enough constant ε > 0, we obtain a vector u of voltages such that u − L + ( e s − e t ) We
 ∞ ≤ ε. Further, if u is the vector of voltages, then the vector of electrical currents, assuming all resis- tances are 1 , is B u , and can be computed in additional O ( m + n ) time. Note that since u − L + ( e s − e t ) ∞ ≤ ε,

\
 B u − BL + ( e s − e t ) Mle
 ∞ ≤ 2 ε.

u
 B aT
 B u − B aT
 BL + ( e s − e t ) We
 ∞ = Wu
 L u − ( e s − e t ) We
 ∞ ≤ 2 εn.

We may assume that our current vector satisﬁes

Thus, L u may not be a unit s,t -ﬂow, but very close to it. If one desires, one can compute in time O ( m + n ) a vector ˜ v from u such that L ˜ v = e s − e t which is indeed a unit s,t -ﬂow, and fu
 u − ˜ v ∞ ≤ ε. We leave it as an exercise for the reader. We summarize the results of this section in the following theorem which states the result for a weighted graph; the extension is straightforward.

Theorem 11.1. There is an O ( m (log r )log 1 / ε ) time algorithm which on input ε > 0 , G = ( V,E,w ) with r def = w max / w min , and s,t ∈ V, ﬁnds vectors ˜ v ∈ R V and ˜ f ∈ R E such that if v def = L + ( e s − e t ) and f def = W BL + ( e s − e t ) ,

(

) L ˜ v = e s − e t and ˜ f = W B ˜ v , (

) v − ˜ v ∞ ≤ ε, (

) if
 f − ˜ f ∞ ≤ ε, and (

) | lf -
 e f

e / w e − e ˜ f

e / w e | ≤ ε.

Here W is an m × m diagonal matrix with W ( e,e ) = w e .

11.2 Computing Eﬀective Resistances

For an edge e = ij, let R e denote the eﬀective resistance of the edge e which, recall, is deﬁned to be ( e i − e j ) 17]
 L + ( e i − e j ) . For our spec- tral sparsiﬁcation application we required a way to compute R e for all edges. As noted in the previous section, if we are interested in near- linear time algorithms, we have to use an approximation ˜ R e of R e . It can be shown that any O (1) approximation suﬃces for our spectral sparsiﬁcation application; it just increases the sample size by a con- stant factor. We saw in the last section that for a given ε > 0 and edge e, we can compute ˜ R e such that

in time ˜ O ( m log 1 / ε ) using the LSolve . A naive extension that com- putes ˜ R e for all edges takes ˜ O ( m 2 log 1 / ε ) time. Can we reduce this to ˜ O ( m log 1 / ε )? The answer, as we will see shortly, is yes. The key obser- vation is that for an edge e = ij,

for all e. The w i s live in R m . Is it necessary to compute w i s if all we are interested in the bs
 2 distance of m pairs among them? The answer is no. The Johnson–Lindenstrauss Lemma postulates that computing the projections of these vectors on a random log m / ε 2 dimensional space preserves distances up to a multiplicative factor of 1 ± ε. Hence, our approach is to choose a random matrix A which is roughly log m × m and compute A w i = ABL + e i for all i. Several ways to choose A work and, in particular, we appeal to the following theorem where entries of A are ± 1 up to normalization. This random matrix A corresponds to the random subspace.

Theorem 11.2. Given ﬁxed vectors w 1 ,..., w n and ε > 0 , let A ∈ { − 1 / √ k , 1 / √ k } k × m be a random matrix with k ≥ c log n / ε 2 for some con- stant c > 0 . Then, with probability at least 1 − 1 / n , for all 1 ≤ i,j ≤ n (1 − ε ) Mh
 w i − w j 2 ≤ A w i − A w j 2 ≤ (1 + ε ) Ih
 w i − w j 2 .

Getting back to our problem of computing R e for all e, let Z def = ABL + where A is the random matrix promised in the theorem above. We compute an approximation Z by using LSolve to approximately com- pute the rows of Z . Let the vectors z i and ˜ z i denote the i th rows of Z and ˜ Z , respectively (so that z i is the i -th column of Z ). Now we can construct the matrix mie
 Z in the following three steps:

(

) Let A be a random ±

/ k matrix of dimension k × m where k = O ( log n / ε

). (

) Compute Y = AB . Note that this takes

m × O ( log n / ε

) + m = Ol
 O ( m / ε

) time since B has

m entries. (

) Let y i , for

≤ i ≤ k , denote the rows of Y , and compute ˜ z i def = LSolve ( L, y i ,δ ) for each i .

We now prove that, for our purposes, it suﬃces to call LSolve with δ def = ε poly( n ) . We do not explicitly specify the poly( n ), but it can be recovered from the proof below. First, we claim that if z i − ˜ z i ≤ ε for all i, then for any vector u with u = O (1) ,

i ( a i + b i ) 2 ≤ 2 i ( a 2 i + b 2 i ) = 2 Z u 2 + 2 2\\2
 ˜ Z u 2 = poly( n ) .

This is because Z u 2 = ABL + ( e i − e j ) yy
 2 for some i,j. From Johnson–Lindenstrauss, we know that w.h.p. this is within a constant factor of a
 BL + ( e i − e j ) iI
 2 = ( e i − e j ) \7]
 L + ( e i − e j ) = poly( n ) as noted before. We need to estimate ˜ Z u 2 and show that it is poly( n ) as well. Note that

| a i − b i | = |(cid:2) z i − ˜ z i , u (cid:3)| ≤ z i − ˜ z i u = ε u = O ( ε ) .

Since Z u 2 is the eﬀective resistance of some edge, it is at least 1 / poly( n ) when the graph is unweighted. Hence,

To summarize, we have proved the following lemma.

for every pair i,j ∈ V . If for all i , z i − ˜ z i L ≤ ε z i L where ε ≤ δ poly( n ) , then

(1 − δ ) R ij ≤ 12,
 Z ( e i − e j ) ye
 2 ≤ (1 + δ ) R ij

Thus, the construction of Z takes iy
 O ( m log( 1 / δ ) /ε 2 ) = O ( m / ε 2 ) time. We can then ﬁnd the approximate resistance 12
 Z ( e i − e j ) i\/?
 2 ≈ R ij for any i,j ∈ V in O (log n / ε 2 ) time. When the edges are weighted, one can extend this analysis to prove the following theorem.

Graph Sparsiﬁcation II Computing Electrical Quantities

(1 − ε ) R ij ≤ 12,
 Z ( e i − e j ) i,
 2 ≤ (1 + ε ) R

for every pair of vertices i,j ∈ V

Notes

More details concerning Theorem 11.1 can be found in [22]. Theo- rem 11.2 was proved in [3]. Theorem 11.4 is from [76].

This section presents an algorithm which reduces the s,t -max-ﬂow/ min-cut problems in undirected graphs to their corresponding electri- cal analogs. Since electrical ﬂows/voltages can be computed in ˜ O ( m ) time using Laplacian solvers as shown in Section 11, this section is ded- icated to showing how, using the multiplicative weight update method, ˜ O ( √ m )-electrical ﬂow computations suﬃce to approximately compute the max-ﬂow and min-cut between s and t.

12.1 Maximum Flows, Minimum Cuts

Given an undirected graph G = ( V,E ) with edge capacities c : E (cid:9)→ R ≥ 0 , the s,t - MaxFlow problem is to compute a ﬂow of maximum value from a source s ∈ V to a sink t ∈ V. To talk about a ﬂow on G, it is convenient to direct its edges arbitrarily, captured by an incidence matrix B, and allow the ﬂow value on an edge to be positive or negative. A ﬂow f = ( f e ) e ∈ E is then a (combinatorial) s,t -ﬂow if

(

) f

F
 B e v =

for all v ∈ V \{ s,t } , (

) f

B e s + f B e t =

, and (

) | f e | ≤ c e for all e.

The goal of the s,t - MaxFlow problem is to compute a ﬂow which maximizes the ﬂow out of s, i.e., | f eT
 B e s | . It is well known that the maximum s,t -ﬂow is the same as the minimum capacity cut that sep- arates s and t. The capacity of a cut ( S, ¯ S ) is the sum of the capacities of all the edges with one endpoint in S and another in ¯ S. This is the Max-Flow–Min-Cut theorem. The corresponding problem of ﬁnding the minimum cut separating s and t is called the s,t - MinCut problem.

In this section we give approximation algorithms for both the s,t - MaxFlow and the s,t - MinCut problem. Departing from traditional approaches, the algorithms presented in this section compute combi- natorial ﬂows and cuts by combining the information obtained from electrical ﬂows and potentials which can be computed quickly to a good accuracy, see Section 11. We prove the following theorems.

Theorem 12.1. There is an algorithm such that, given an undi- rected, capacitated graph G = ( V,E,c ) , two vertices s,t , and an ε > 0 , such that the maximum value from s to t is F , the algorithm out- puts a (combinatorial) ﬂow of value at least (1 − ε ) F and runs in ˜ O ( m 3 / 2 poly( 1 / ε )) time.

Theorem 12.2. There is an algorithm such that, given an undirected, capacitated graph G = ( V,E,c ) , two vertices s,t , and an ε > 0 , such that there is a cut separating s and t of value F , the algorithm outputs an s,t -cut of value at most (1 + ε ) F and runs in ˜ O ( m 3 / 2 poly( 1 / ε )) time.

It is important to note that these algorithms work only for undirected graphs. Since we only compute a 1 ± ε -approximation to the max-ﬂow– min-cut problems, there is a simple trick to ensure that the ratio of the largest to the smallest capacity in the input graph is at most O ( m 2 / ε ) . Hence, the running times do not depend on the capacities. Proving this is left as an exercise. Finally, the algorithms in these theorems can be modiﬁed to obtain a running time which depends on m 4 / 3 rather than m 3 / 2 ; refer to the notes at the end of this section. At this point the reader may review the connection between graphs and electrical networks in Section 4.

12.2 Combinatorial versus Electrical Flows

Recall that given resistances r e for edges in G captured by an m × m diagonal matrix R where R ( e,e ) def = r e and 0 otherwise, if F units of current is injected at s and taken out at t, then the electrical ﬂow vector f can be written as f def = R − 1 BL rT
 F ( e s − e t ) . Here, the Laplacian is deﬁned to be L def = B 27]
 R − 1 B . Using Theorem 11.1 from Section 11, we know that f can be computed approximately to a precision of ε in ˜ O ( m log 1 / ε ) time. How good a substitute is f for a combinatorial s , t -ﬂow? First note that in the input to the s,t - MaxFlow problem there are no resistances. We will have to determine how to set them. The problem with electrical ﬂows is that they could violate edge capacities. There does not seem to be an easy way to ﬁnd resistances such that the electrical ﬂow obtained by the electrical network satisfies the edge capacities and also maximizes the s,t -ﬂow. In fact, it is not immediately clear that such resistances exist; we leave showing that they do as an exercise to the reader. The algorithm in the proof of Theorem 12.1 is iterative and starts with a set of resistances. At each iteration it computes the s,t -electrical ﬂow and updates the resistances of the edges based on the congestion due to this ﬂow. The update rule is governed by the intuition that the higher the resistance of an edge, the less the electrical ﬂow will run through it. The ingenuity is in deﬁning an update rule such that the number of iterations is small and the average of the electrical ﬂows computed satisfies the capacity constraints.

Energy of Flows

In Section 4 we proved (see Theorem 4.7) that for graph G = ( V,E ) with resistances r e given by a diagonal matrix R, if f def = R − 1 BL + ( e s − e t ), then f minimizes E r ( f ) def = agon
 e r e f 2 e among all unit ﬂows f from s to t . The same also holds for ﬂows of magnitude F. Thus, any s,t ﬂow that respects capacity and pumps the same ﬂow as the electrical ﬂow from s to t has at least the energy of the corresponding electrical ﬂow. Let g be such a combinatorial ﬂow, i.e., | g e | ≤ 1. Then we have that

The last inequality follows from the fact that | g e | ≤ c e . Thus, if E r ( f ) > ‘Th
 e r e , we are certain there is no s,t

max

ﬂow of value corresponding to that of f . We use this observation crucially in the algorithm.

In this section we prove Theorem 12.1. Let F be the value of the s,t - max-ﬂow in G. We present an algorithm that ﬁnds a (1 − ε ) F s,t -ﬂow in time ˜ O ( m 3 / 2 poly( 1 / ε )). For convenience, we assume that c e = 1 , ∀ e. Refer to Section 12.3.1 for general c e .

It suﬃces to present an algorithm which, given a value F , either outputs an s,t -ﬂow of value at least (1 − O ( ε )) F satisfying the capac- ity constraints or certiﬁes that F > F . The algorithm is iterative and appeals to the multiplicative weight update (MWU) method. The MWU technique is very general and here we present it only from the point of view of our application. At any given time t, the resistances for the edges are given by a vector r t . The corresponding m × m diag- onal matrix is denoted by R t and the Laplacian L t def = B a7
 R − 1 t B. The algorithm ElecFlow appears below. At any time t, the algorithm maintains weights w te which are positive numbers. To start with, w 0 e are set to one for all e. These weights are used to compute the resistances r te at time t. The natural choice for r te is w te itself. Unfortunately, we do not know how to show that using these resistances, the number of iterations is bounded via the MWU method. The issue is that one needs to be able to control the width in the MWU method. In our case the width is the maximum ﬂow across an edge at any time. Instead, we consider the update where the resistances are set using the following equation:

It suﬃces to present an algorithm which, given a value F , either outputs an s,t -ﬂow of value at least (1 − O ( ε )) F satisfying the capac- ity constraints or certiﬁes that F > F . The algorithm is iterative and appeals to the multiplicative weight update (MWU) method. The MWU technique is very general and here we present it only from the point of view of our application. At any given time t, the resistances for the edges are given by a vector r t . The corresponding m × m diag- onal matrix is denoted by R t and the Laplacian L t def = B a7
 R − 1 t B. The algorithm ElecFlow appears below.

At any time t, the algorithm maintains weights w te which are positive numbers. To start with, w 0 e are set to one for all e. These weights are used to compute the resistances r te at time t. The natural choice for r te is w te itself. Unfortunately, we do not know how to show that using these resistances, the number of iterations is bounded via the MWU method. The issue is that one needs to be able to control the width in the MWU method. In our case the width is the maximum ﬂow across an edge at any time. Instead, we consider the update where the resistances are set using the following equation:

Thus, the resistance of an edge at time t has a local component ( w te ) and a global component (roughly ε times the average weight). Intu- itively, the global component helps reduce the gap between the local resistance and the average global resistance, thus, reducing the possi- bility of a large current ﬂowing one edge compared to the rest of the

Input: G ( V,E ), source s, sink t, a target ﬂow value F and 0 < ε < 1 Output: Either an s,t -ﬂow f of value at least (1 − O ( ε )) F or FAIL indicating that F > F 1: w 0 e ← 1 for all e ∈ E 2: ρ ← 2 “if
 mε 3: T ← ρ log m ε 2 4: for t = 0 → T − 1 do 5: ∀ e ∈ E, r te ← w te + ε 3 m e w te 6: f t def = R − 1 t BL + t F ( e s − e t ) 7: if E r t ( f t ) > (1 + ε 3 ) e r te then 8: return FAIL 9: else 10: ∀ e ∈ E, w t +1 e ← w te (1 + ε | f te | ρ ) 11: end if 12: end for 13: return f def = (1 − ε ) (1+2 ε ) · 1 T · T − 1 t =0 f t

Input: G ( V,E ), source s, sink t, a target ﬂow value F and 0 < ε < 1 Output: Either an s,t -ﬂow f of value at least (1 − O ( ε )) F or FAIL

graph. This allows us to bound the width to ρ def = O ( m / ε ) and the average congestion w.r.t. w te s of the ﬂow to (1 + ε ) . Hence, the num- ber of iterations is ˜ O ( √ m / ε 5 / 2 ) . Concretely, the upper bound on the width allows one to almost reverse the inequality 1 + x ≤ e x , and its use is demonstrated in Lemma 12.4. The key to this algorithm is the update step and the width calculation. Once the resistances are set, the corresponding electrical ﬂow f t is computed and the weights then increase by a multiplicative factor of (1 + ε | f t e | / ρ ) . Thus, the higher the congestion, the higher the increase in resistance. Finally, the ﬂow f t is computed using the Laplacian solver, see Theorem 11.1. It is an approximate ﬂow, but we ignore this issue in the interest of readability and leave it as exercise to adapt the proof in the approximate set- ting. Assuming we can do so in ˜ O ( m ) time, the overall running time is ˜ O ( m 3 / 2 /ε 5 / 2 ) . We also show in Section 12.4 that if the algorithm outputs FAIL, we can recover an s,t cut of value at most F in additional linear time.

For general c e s, the r te update rule is changed to r te ← 1 c 2 e ( w te + ε 3 m e w te ) and | f te | is replaced by | f te | / c e . Note that the new bound for the energy is E r ( f ) ≤ e r e f 2 e / c 2 e . It is easy to verify that these changes make the algorithm work for general capacities.

Since we do not know F , the maximum ﬂow from s to t, we ﬁrst need to show that the number of F s we may need to run is bounded by poly(log m, 1 / ε ). This can be done by binary search and is left as an exercise. Next, we need to show the following for any F for which the algorithm terminates but does not output FAIL:

• The ﬂow value from s to t is at least (

− O ( ε )) F ; and • the output ﬂow does not violate any capacity constraints, i.e., ∀ e, | f e | ≤ (

− ε ) (

+

ε ) ·

T · T −

t =

| f te | ≤ c e =

The ﬁrst is easily seen as the ﬂow output by the algorithm is essentially an average of T ﬂows, each pushing F units of ﬂow from s to t. Recall that we showed in Equation (12.1) that when the algorithm ﬁnds a ﬂow f t such that E r t ( f t ) > (1 + ε 3 ) 1) tt
 e r te , it implies that the s,t -max- ﬂow is less than F. Hence, when ElecFlow returns FAIL, F < F. On the other hand, when E r t ( f t ) ≤ (1 + ε 3 ) e r te we can show that, in f t , the maximum congestion on an edge is bounded above and average congestion is small.

Proof. For convenience, we drop the superscript t in this proof. The hypothesis then implies that

Hence, using the update rule for r e from the algorithm, we get that Her
 e r e f 2 e is at most

where the last inequality follows from the fact that ε < 1. On the other hand,

e r e f 2 e = e w e + ε 3 m e w e f 2 e . (12.2) Combining the two, we obtain that ∀ e, εf 2 e 3 m e w e ≤ ( 1 + ε 3 )(cid:3) e w e , which implies that | f e | ≤ 3(1+ ε / 3 ) m ε ≤ 2 m / ε for all e. Using the Cauchy–Schwarz inequality on √ w e and √ w e | f e | , we get e w e | f e | 2 ≤ e w e e w e f 2 e , which implies that ( e w e | f e | ) 2 e w e ≤ e w e f 2 e ≤ (1 + ε ) e w e .

which implies that | f e | ≤ 3(1+ ε / 3 ) m ε ≤ 2 m / ε for all e. Using the Cauchy–Schwarz inequality on √ w e and √ w e | f e | , we get

The last inequality follows from Equation (12.2) by taking the ﬁrst term of summation. Hence,

To track progress, we use the potential function Φ t def = e w te . Clearly, Φ = m and

where ρ = 2 m / ε . Thus, using Lemma 12.3 we get that Φ t +1 is at most

e w te + ε (1 + ε ) e w te ρ = Φ t 1 + ε (1 + ε ) ρ ≤ m 1 + ε (1 + ε ) ρ t +1 ,

by repeating this argument. Hence, we obtain the following lemma.

In the other direction,

Using the inequality 1 + εx ≥ e ε (1 − ε ) x for all x ∈ [0 , 1] , and 0 < ε < 1 , we get that

Combining the last two inequalities we obtain that for all e,

Φ T − 1 ≥ w T − 1 e ≥ Π T − 1 t =0 e (1 − ε ) ε | fte | ρ = e (1 − ε ) ε t | fte | ρ . Now, using Lemma 12.4, we get that e (1 − ε ) ε t | fte | ρ ≤ me ε (1+ ε ) T ρ = e ε (1+ ε ) T ρ +log m , which implies that (1 − ε ) ε t f te ρ ≤ ε (1 + ε ) T ρ + log m. Dividing by T ε and multiplying by ρ , we get (1 − ε ) t | f te | T − 1 ≤ (1 + ε ) + ρ log m T ε ,

and by setting ρ log m Tε ≤ ε , we get that for T ≥ ρ log m ε 2 ,

Note that the number of iterations of the algorithm depends on the value of ρ . An immediate question is: Can it be decreased? The following example shows that the width ρ = √ m is tight.

Example 12.5. Consider a graph composed of k + 1 disjoint paths between s and t ; k paths of length k and one path of length 1. Hence, the total number of edges is m = k 2 + 1 and the max-ﬂow in this graph is k + 1. The electrical ﬂow of value k + 1 sends k +12 ﬂow through the edge of the length 1 path, which is approximately √ m .

12.4 s, t - Min Cut

In this section we show how to complete the proof of Theorem 12.2. The algorithm is the same as ElecFlow except now if a valid ﬂow is returned for some F, we know that the minimum cut separating s and t is more than F. On the other hand, if for some value F ElecFlow returns FAIL, we show how to recover an s,t -cut of value (1 + O ( ε )) F , thus completing the proof of Theorem 12.2.

Note that the vector of potentials used for setting r when ElecFlow outputs FAIL is given by v def = L + F ( e s − v t ) . Here, L + corresponds to the resistances r e . We again ignore the issue that v is known only approximately. Hence, the cut we recover is in fact less than F. The vector v gives an embedding of the vertex set V on the real line. First, note that s and t are the two endpoints of this embedding.

Proposition 12.6. Given the embedding of V into R given by v as above, max i v i = v s and min i v i = v t .

Proposition 12.6. Given the embedding of V into R given by v as

This is an easy exercise and can be proved using the fact that a har- monic function achieves its maxima and minima at the boundary. If we

pick a value uniformly at random from the interval [ v t ,v s ], then the probability that an edge e = ij is cut is | v i − v j | | v s − v t | . Let X e be indicator random variable that is 1 if e is cut, and 0 otherwise. The expectation of X e is E [ X e ] = P [ X e = 1] = | v i − v j | | v s − v t | . Using linearity of expectation, we get that

E e X e ! = e E [ X e ] = e = ij | v i − v j | | v s − v t | . Thus, using Cauchy–Schwarz inequality on | v i − v j | √ r e and √ r e , we get e = ij | v i − v j | | v s − v t | ≤ 1 | v s − v t | * ++,(cid:3) e = ij ( v i − v j ) 2 r e e r e . Now using the fact that E r ( f ) = e = ij ( v i − v j ) 2 r e and also E r ( f ) = F | v s − v t | , we get that the above is at most F E r ( f ) E r ( f ) e r e = F e r e E r ( f ) .

Since f is such that E r ( f ) > e r e , ﬁnally we get E [ e X e ] < F ; i.e., the expected value of cut is at most F . Therefore, one of the sweep cuts that separates s and t has to be less than the expected value. Given the vector v , the running time of this algorithm is ˜ O ( m ). Thus, for a given value of F, either ElecFlow returns a valid s,t -ﬂow of value at least (1 − O ( ε )) F or the algorithm in this section ﬁnds a cut of value at most F. The running time is dominated by that of ElecFlow . This completes the proof of Theorem 12.2.

Notes

The max-ﬂow/min-cut theorem dates back to at least [26, 29], and a

The max-ﬂow/min-cut theorem dates back to at least [26, 29], and proof of it can be found in any text book on algorithms, see [5].

proof of it can be found in any text book on algorithms, see [5]. Theorems 12.1 and 12.2 are from [22]. In fact, there the power in the running time is 4 / 3 as opposed to the 3 / 2 presented in this section. Using Laplacian solvers to speed up interior point methods to solve the linear program for the max-ﬂow problem, Daitch and Spielman [24] obtained

the 3 / 2 result, which preceded the work of Christiano et al. [22]. The techniques in this section have been extended by Kelner et al. [46] to obtain algorithms for multicommodity ﬂows for a constant number of commodities.

Two challenging problems remain: To decrease the dependence on the error ε from poly 1 / ε to polylog 1 / ε ; and to improve the running time to ˜ O ( m ) with possibly worse dependence on ε.

The multiplicative weight update paradigm has had a remarkable success in a wide variety of areas. A recent survey of Arora et al. [11] should give the interested reader a good idea of its usefulness.

Cholesky Decomposition Based Linear Solvers

The remainder of this monograph is concerned with methods to solve a system of linear equations, A x = b where A is an n × n matrix and b is a vector in the column space of A . This section reviews an exact method for solving a linear system of equations based on Cholesky decomposi- tion. Subsequently, the Cholesky-based method is employed to present an O ( n ) time algorithm for solving a linear system of equations L x = b when L is a Laplacian of a tree.

13.1 Cholesky Decomposition

We are concerned with matrices which are symmetric and PSD. A matrix A is lower (respectively, upper) triangular if A ij = 0 whenever i < j (respectively, i > j ). Observe that if A is either lower or upper triangular, then A x = b can be solved by back-substitution with O ( m ) arithmetic operations, where m is the number of nonzero entries of A . Equivalently, A + b can be evaluated using O ( m ) arithmetic operations. The undergraduate textbook algorithm to solve a system of linear equa- tions is Gaussian elimination which performs row operations on the matrix A to convert it to a lower or upper triangular matrix. When A

is symmetric and PSD, a more systematic way of solving A x = b is via its Cholesky decomposition , which is a modiﬁcation of Gaussian elimina- tion. We present an algorithm to compute the Cholesky decomposition of A when A is positive deﬁnite. There are appropriate generalizations when A is not invertible. However, we will not cover these generaliza- tions since in order to solve a Laplacian system of a connected graph L x = b , the vector b is orthogonal to the all-ones vector. In this setting

Theorem 13.1. If A 0, then one can write A = ΛΛ where Λ is a lower triangular matrix. Such a decomposition is called the Cholesky decomposition of A.

Before we proceed with the proof of this theorem, we need the following lemma, often referred to as Schur’s lemma.

Proof. Since A ro
 0 , d 1 > 0 . Consider minimizing the quadratic form z 2 d 1 + 2 z u 1 y + y Ty
 B 1 y over the variable z for a ﬁxed y . The solution can be seen, by diﬀerentiating, to be z = − u 1 y / d 1 . The minimum value then is y ( B 1 − u 1 u 1 / d 1 ) y . Hence, if A 0 , then for all y , y ( B 1 − u 1 u 1 / d 1 ) y > 0 . This implies that B 1 − u 1 u 1 / d 1 0 . The other direction is straightforward.

Proof. [of Theorem 13.1] Since A ro
 0, it suﬃces to express A = Λ∆Λ for a diagonal matrix ∆. Positive deﬁniteness implies that ∆ ii > 0 and thus the decomposition in the theorem is obtained via A = (Λ∆ 1 / 2 )(Λ∆ 1 / 2 ) . For a symmetric, positive-deﬁnite matrix A , we write it as

Let the matrix in the middle of the r.h.s. be called A 1 . Note that the ﬁrst matrix is a lower triangular matrix Λ 1 and the third is Λ \T
 1 . Lemma 13.2 implies that B def = B 1 − u 1 u ul
 1 / d 1 a
 0 which gives A 1 0. Recursively, we can write B = Λ ∆ a
 Λ ‘aa
 as the product of ( n − 1) × ( n − 1) matrices, and

Thus, A = Λ 1 Λ We
 ∆Λ nT
 Λ 1 . Since product of lower triangular matrices is lower triangular, the theorem follows.

Given the Cholesky decomposition, one can solve A x = b easily. Indeed, one ﬁrst evaluates b = Λ + b and then evaluates x = (Λ AT}
 ) + b 7
 . Note that the number of such operations is of the order of nonzero entries in Λ. In the method to compute the Cholesky decomposition of A in the proof above, the choice of the ﬁrst row was arbitrary. In fact, any ordering of the rows (and the same ordering of the columns) of A can be used to recover a decomposition which results in a faster algorithm to solve A x = b . Formally, the Cholesky decomposition of A and Q AQ , (i.e., A with its rows and columns permuted by the same permutation matrix Q, ) can be very diﬀerent. 1 Therefore, the number of nonzero entries in the Cholesky decomposition of a matrix, called the ﬁll in , depends on the permutation of rows and columns. Finding the permutation which leads to the minimum ﬁll in is known to be NP-hard.

13.2 Fast Solvers for Tree Systems

Now we demonstrate how to use the combinatorial structure of the linear system to get a good Cholesky decomposition. This results in a fast solver for L T x = b when T is a tree. Any symmetric matrix A can be associated with an n vertex graph (with self-loops) where we have an edge ij of weight A ( i,j ) between vertices i and j . The number of edges in the graph is precisely the number of nonzero entries in A . Let us now view the Cholesky decomposition as described in the proof of

1 An n × n matrix Q is said to be a permutation matrix if all its entries are either 0 or 1 and there is exactly one 1 in each row and in each column.

Theorem

as a process which modiﬁes the graph: After the row i has been processed, the resulting graph (which corresponds to A

) has the following modiﬁcations:

(

) All edges ij with j = i are deleted; this corresponds to setting A

( i,j ) =

; and (

) for every pair jk ( j could be equal to k ) neighboring to i a (potentially new) edge is modiﬁed; this corresponds to setting A

( j,k ) = B

( j,k ) − A ( i,j ) A ( i,k ) A ( i,i ) .

Now suppose the graph corresponding to the linear system as described above is a tree and potentially self-loops. Then in every iteration of the Cholesky decomposition, we can choose a leaf node i . Since there is a single node adjacent to i , the graph associated with the matrix A 1 is a tree as well (potentially with an extra self-loop). In particular, this implies that we can write A as Λ∆Λ where Λ = Λ 1 Λ 2 ··· Λ n and each Λ i is a lower triangular matrix with at most one nonzero oﬀ-diagonal entry. This gives a Cholesky decomposition with at most O ( n ) nonzero entries. Moreover, in the computation of the Cholesky decomposition, in each iteration, at most O (1) operations are done. Therefore, the decomposition can be computed in O ( n ) time. Thus, we have proved the following theorem.

Theorem 13.3. Given a symmetric, PSD matrix A and a vector b such that the graph of A corresponds to a tree, one can ﬁnd in O ( n ) time a permutation matrix Q such that the Cholesky decomposition of Q AQ has at most O ( n ) nonzero entries.

This immediately implies the following corollary which will be used in Section 17.

Corollary 13.4. If L T is the Laplacian of a tree T and b is a vector such that if
 b , 1 = 0, then the solution of L T x = b can found in O ( n time.

Proof. Note that the graph associated with the Laplacian of a tree is the tree itself. Therefore, using Theorem 13.3, we can ﬁnd the Cholesky

decomposition of the permuted Laplacian to get ΛΛ AT,
 y = b in O ( n ) time. Here, y def = Q x and b def = Q yT}
 b . Note that Λ is not full rank since L T is not, however, b , 1 1)
 = 0 implies that b (and thus b 4")
 ) is in the column space of L T Q . Therefore, we are guaranteed a solution x and this can be calculated in the number of nonzero entries of L T , which is also O ( n ).

Notes

More on Cholesky decomposition of factorization can be found in the book [35]. George [33] pioneered the use of ﬁnding the right ordering in the Cholesky decomposition for special graphs, e.g., square grids. He showed that any PSD matrix whose graph is an n × n grid can be solved in O ( n 1 . 5 ) time. This is the nested dissection algorithm. Lipton et al. [52] generalized this result to planar graphs by showing the existence of O ( √ n )-sized separators in planar graphs. This gives an algorithm that runs in ˜ O ( n 1 . 5 ) time and, in general, gives an algo- rithm that runs roughly in time n 1+ ε for a family of graphs which have separators of size n ε and are closed under edge removal.

An old and simple iterative method to solve A x = b , due to Kaczmarz, starts with an arbitrary point, ﬁnds an equation that is not satisﬁed, forces the current solution to satisfy it, and repeats. In this section, the convergence of a randomized variant of the Kaczmarz is established. Subsequently, it is shown how this method has been recently employed to develop a new and simpler algorithm to solve Laplacian systems in approximately ˜ O ( m ) time.

14.1 A Randomized Kaczmarz Method

Let A x = b be a system of equations where A is an m × n matrix and x is one of its solutions. It is convenient to think of the solution x as lying on all the hyperplanes

where a i is the i -th row of A and b i is the corresponding entry of b . Thus, starting with an arbitrary initial point x 0 , consider the following

simple iterative algorithm to compute an approximation to x : Given x t such that A x t = b , choose any hyperplane a i , x = b i which does not contain x t and let x t +1 be the orthogonal projection of x t onto this hyperplane. It can be shown that this method will converge to x as long as every hyperplane is considered an inﬁnite number of times. For instance, one could cycle over the hyperplanes in a ﬁxed order. However, the convergence to x can be very slow and bounds on the rate of con- vergence for general A exist for any deterministic hyperplane selection rule. This motivates the use of randomization and we consider a simple randomized rule called the Randomized Kaczmarz method: Set x 0 = 0 . For t ≥ 0 , given x t such that A x t = b , choose a hyperplane a i , x = b i with probability proportional to a i 2 and let x t +1 be the orthogonal projection of x t onto this hyperplane. The rate of convergence of this algorithm turns out to be related to a notion of an average condition number deﬁned as follows.

Deﬁnition 14.1. Given an m × n matrix A, let A + be deﬁned as follows

Deﬁnition 14.1. Given an m × n matrix A, let A + be deﬁned as

Recall that A 4/3
 2 F is the squared-Frobenius norm of A which is deﬁned to be the sum of squares of all the entries of A. The average condition number is deﬁned to be

Theorem 14.1. The Randomized Kaczmarz method presented above, starting with x 0 = 0 , satisﬁes

for τ = O (˜ κ 2 ( A )log 1 / ε ) with probability at least 0 . 9 .

Iterative Linear Solvers I The Kaczmarz Method

Convergence in Terms of Average Condition Number

Convergence in Terms of Average Condition Number

The proof of Theorem 14.1 follows from the following lemma.

Lemma 14.2. Fix the choice of the hyperplanes up to t and let a i , x = b i be the random hyperplane selected with probabil- ity a i 2 / A 2 F . If x t +1 is the orthogonal projection of x t on to this hyperplane,

As a ﬁrst step in the proof of this lemma, let us calculate x t +1 in terms of the hyperplane and x t . The unit normal of the hyperplane a i , x = b i is ˆ a i def = a i a i . Since x t +1 is an orthogonal projection of x t onto this hyperplane, there is some γ ≥ 0 such that

Since x t +1 lies on the hyperplane, we know that

Since x t +1 and x lie in the hyperplane, a i , x t +1 − x *)
 = 0 , which implies that x t +1 − x is orthogonal to x t +1 − x t . Thus, by Pythagoras Theorem,

14.2 Convergence in Terms of Average Condition Number

= x t − x 2 1 − 1 A 2 F A ( x t − x ) ye
 2 x t − x 2 Defn . 14 . 1 ≤ x t − x 2 1 − 1 A 2 F A + 2 = x t − x 2 1 − 1 ˜ κ 2 ( A ) .

This completes the proof of the Lemma 14.2. By repeated application of this lemma we obtain that for t ≥ 0 ,

E x t +1 − x 2 ≤ (1 − 1 / ˜ κ 2 ( A ) ) t x 0 − x 2 = (1 − 1 / ˜ κ 2 ( A ) ) t x 2 .

E x t +

− x

≤ (

−

/ ˜ κ

( A ) ) x

− x

= (

−

/ ˜ κ

( A ) ) x

. Thus, from Markov’s Inequality, with probability at least

−

/

, x t +

− x

≤

(

−

/ ˜ κ

( A ) ) t x

. (

) Hence, by selecting τ = O (˜ κ

( A )log

/ ε ) , we obtain that

with probability at least 0 . 9 , completing the proof of Theorem 14.1.

The Kaczmarz method restricted to a subspace. Note that the Randomized Kaczmarz method and Theorem 14.1 above can be easily extended to the case when the optimal solution x is known to satisfy U x = v for some U and v , and A is such that UA ,7
 = 0 . This will be useful in the next section. In this setting, the Randomized Kaczmarz method maintains that U x t = v for all t. Thus, we ﬁrst need an initial choice x 0 s.t. U x 0 = v . As a consequence, x 0 may no longer be 0 and, hence, in Equation (14.3) we can no longer replace x 0 − x by x . Further, U x 0 − U x t = 0 for all t. Thus, a careful look at the proof of Theorem 14.1 suggests that the following deﬁnition of A + suﬃces.

This can be smaller, giving rise to the possibility that ˜ κ ( A ) in this setting is smaller, which in turn could mean a reduced number of iterations. Without any additional eﬀort, an analysis identical to that of Theorem 14.1 shows that after τ ∼ ˜ κ 2 ( A )log 1 / ε iterations, w.h.p. x τ − x 2 ≤ ε 2 x 0 − x 2 .

Iterative Linear Solvers I The Kaczmarz Method

Toward an ˜ O ( m ) -Time Laplacian Solver

Toward an ˜ O ( m ) -Time Laplacian Solver

In this section we illustrate how the Randomized Karcmarz method can be applied to a Laplacian system. We focus on presenting the algorithm and derive a bound on the number of iterations. This approach has recently led to a new algorithm and a simpler proof of the main theorem (Theorem 3.1) in this monograph.

For this section let us focus on the case of computing the ﬂow through each edge when one unit of current is pumped from s to t and the edges have unit resistances. Such a computational primitive was required in Section 12. It is an easy exercise to extend this argu- ment to solve L x = b when b is not necessarily e s − e t . To set things up, let G = ( V,E ) be an undirected, unweighted graph on n vertices and m edges, and let B ∈ {− 1 , 0 , 1 } n × m be an edge-vertex incidence matrix corresponding to a ﬁxed orientation of the edges. Recall from Section 4 the unit s,t ﬂow vector i = B aT Y
 L + ( e s − e t ) . Given i , it is easy to compute L + ( e s − e t ) in O ( m ) time. Thus, let us focus on computing an approximation to i .

For an undirected cycle C in G, let 1 C ∈ {− 1 , 0 , 1 } m be a vector supported exactly on the edges of C that satisﬁes B 1 C = 0 . Since the sum of voltages across any cycle sum up to zero if all resistances are one, 1 C , i = 0 for all cycles C in G. Fix a spanning tree T in G and, for every e (cid:11)∈ T, let C e be the unique cycle formed by adding e to T. The indicator vectors for these m − n + 1 cycles form a basis of the space of cycle vectors and can be used to generate the indicator vector of any cycle in G. Hence, to ﬁnd i it is suﬃcient to solve the following system of equations

Let A be an m − n + 1 × m matrix whose rows are indexed by edges e (cid:11)∈ T and are equal to 1 C e . Then the problem reduces to solving A f = 0 subject to B f = e s − e t . We will apply the Randomized Karcmarz method to the system A f = 0 while working in the space B f = e s − e t : Initially choose f 0 such that B f 0 = e s − e t . One such choice is to put a unit ﬂow from s to t on the unique path from s to t in T. At time t , given f t , compute f t +1 by orthogonally projecting f t onto the hyperplane

1 C , f f)
 = 0 , where C is chosen from one of the m − n + 1 cycles with probability proportional to a
 1 C 2 = | C | . The number of iterations nec- essary to get within ε of the optimal solution, from Theorem 14.1, is O (˜ κ 2 ( A )log 1 / ε ).

where str T ( e ) is the stretch of e in T deﬁned as the length of the unique path in T between the endpoints of e. 1 On the other hand,

We will now show that this is at most 1 .

Let f T denote the entries of f corresponding to the tree edges and N denote those corresponding to the non-tree edges. Similarly, we can split the columns of A into those corresponding to tree edges and non- tree edges. It follows that one can write A = [ A T I N ] , where I N is a diagonal matrix on the non-tree edges, if the direction of each cycle is chosen to coincide with the edge that was added to form it from T. Suppose f is supported only on one of the cycles determined by T, then A 474
 T f N = f T . Thus, by linearity, the following holds for any vector f such that B f = 0 ,

Note that A f f\|2
 2 = A T f T 2 + 2 f T A T f N + f N 2 . Thus, by (14.4),

This proves the claim that ˜ κ 2 ( A ) ≤ m − n + 1 + e (cid:7)∈ T str T ( e ) . We will see in Theorem 17.3 (without proof) that one can construct, in ˜ O ( m ) time, trees with e (cid:7)∈ T str T ( e ) = ˜ O ( m ) . Since, f 0 2 ≤ n as the initial ﬂow is just on a path from s to t, applying Theorem 14.1 in the sub- space restricted setting of the previous section, the number of iterations

1 This notion of stretch will play a central role in the proof of Theorem 3.1 presented in this monograph and more on this appears in Section 17.

Iterative Linear Solvers I The Kaczmarz Method

required by the Randomized Kaczmarz method to get within ε can be seen to be bounded by ˜ O ( m log 1 / ε )

In order to complete the proof of Theorem 3.1 the key issues that remain are, given T, how to choose an edge not in T with probability proportional to the length of the cycle it forms with T and how to update f t +1 from f t . The former is a simple exercise. For the latter, note that the updates involve simply adding or removing ﬂow along cycles, which can be implemented in logarithmic time using a data structure called link-cut trees . The details of this data structure and how it is employed are beyond the scope of this monograph.

how it is employed are beyond the scope of this monograph.

Notes

The Kaczmarz method appears in [43]. The Randomized Kaczmarz procedure and its analysis appear in [83]. This analysis is tied to the alternating projections framework [19], which also has many other algo- rithmic applications. Section 14.3 is based on a recent result due to [47] where details about the link-cut tree data structures can be found. It is also shown in [47] that the inverter derived from the Randomized Kaczmarz method is linear in the sense of Section 3.4.

This section phrases the problem of solving a system of linear equations as an optimization problem and uses the technique of gradient descent to develop an iterative linear solver. Roughly, iterative methods to solve a system A x = b maintain a guess x t for the solution at an iteration t, and update to a new guess x t +1 through feedback about how good their guess was. The update typically uses a simple vector–matrix product and is often very fast and popular in practice due to its small space complexity.

15.1 Optimization View of Equation Solving

In this section we assume that A is symmetric and positive deﬁnite and, as usual, we let the eigenvalues of A be 0 < λ 1 ≤ ··· ≤ λ n . To start, observe that solving A x = b is equivalent to ﬁnding the minimum of f ( x ) where f is deﬁned to be

Observe that when A is positive deﬁnite, ∇ 2 f = A rr
 0 , so f is strictly convex and, thus, has a unique minimum x . This x must satisfy ∇ f ( x *)
 ) = A x − b = 0.

100 Iterative Linear Solvers II The Gradient Method

Since f is a convex function, we can use the gradient descent approach popular in convex optimization to solve for x . A typical gradient descent algorithm starts with an initial vector x 0 , and at iter- ation t moves to a new point x t +1 in the direction opposite to the gradient of f at x t . We would like to move in this direction because f decreases along it. Note that ∇ f ( x t ) = A x t − b which means that the time it takes to compute the gradient requires a single multiplication of a vector with a matrix A. We use t A to denote the time it takes to multiply an n × n matrix A with a vector. 1 The hope is that in a small number of iterations, each of which is geared to reducing the function value, the process will converge to x . We prove the following theorem.

Theorem 15.1. There is an algorithm GDSolve that, given an n × n symmetric matrix A 0 , a vector b , and ε > 0 , ﬁnds a vector x such that

in time O ( t A · κ ( A )log 1 / ε ) . Here the condition number of A is deﬁned to be κ ( A ) def = λ n ( A ) / λ 1 ( A ) and, for a vector v , v A def = √ v A v .

15.2 The Gradient Descent-Based Solver

Before we describe the algorithm GDSolve formally and prove Theo- rem 15.1, we provide a bit more intuition. First, let us ﬁx some notation. For t ≥ 0, let d t def = x − x t be a vector which tells us where the cur- rent solution x t is w.r.t. x , and let r t def = A d t = b − A x t . Observe that r t = −∇ f ( x t ). Think of r t as a crude approximation to d t . At itera- tion t , there is a parameter η t which determines how much to move in the direction of r t . We will discuss the choice of η t shortly, but for now, the new point is

1 t A is at least n and, even if A has m nonzero entries, can be much less than m, e.g., if A = vv and is speciﬁed in this form.

How does one choose η t ? One way is to choose it greedily: Given x t (and therefore r t ) choose η t which minimizes f ( x t +1 ). Towards this end, consider the function

g ( η ) def = f ( x t + η r t ) = 2( x t + η r t ) yy
 A ( x t + η r t ) − b AT Y
 ( x t + η r t ) .

The minimum is attained when the gradient of g w.r.t. η is 0 . This can be calculated easily:

GDSolve is described formally in Algorithm 15.1.

Thus, to prove Theorem 15.1 the only thing one needs to show is that for T = O ( κ ( A )log 1 / ε ) , x T − A + b Dl
 A ≤ ε A + b 7
 A . Towards this, we prove Lemma 15.2 which shows how the A -norm of the error vector d t = x t − x drops with t .

Proof. We start with the following observation: From the update rule we get d t +1 = d t − η t r t and r t +1 = r t − η t A r t . This gives, r t r t +1 = r t r t − r t r t r t A r t r t A r t = 0. Therefore, two consecutive update directions

r t and r t +1 are orthonormal. This is expected since from x t we move in the direction r t as far as we can to minimize f . Now,

In the last inequality, we use r t = A d t . Now, the ﬁrst fraction is at least 1 / λ n , while the second is at least λ 1 since

which completes the proof of the lemma.

As a corollary we get that in T = 2 κ ( A )log 1 / ε steps, x T − x A ≤ ε x 0 − x A ; in fact, with a better analysis the factor 2 can be removed. This completes the proof of Theorem 15.1.

tion to check when to terminate. It is easy to check that 2
 A x t − b >|
 ≤ δ. For this termination condition to imply the termination condition in the theorem, one has to choose δ small enough; this seems to require an upper bound on the condition number of A. We leave it as an exercise to check that for graph Laplacians, an easy estimate can be obtained in terms of the ratio of the largest to the smallest edge weight. In the next section we will see how to reduce the number of iterations to about κ ( A ) when A is PSD. This will require a deeper look into GDSolve .

Notes

The method GDSolve suggested here is well known and bears simi- larity with Richardson’s iterative method [64], see also [36, 37]. More on gradient descent can be found in the book [19].

Iterative Linear Solvers III The Conjugate Gradient Method

This section presents a sophisticated iterative technique called the conjugate gradient method which is able to approximately solve a linear system of equations in time that depends on the square root of the con- dition number. It introduces the notion of Krylov subspace and shows how Chebyshev polynomials play a crucial role in obtaining this square root saving. Finally, the Chebyshev iteration method is presented which is used in Section 18 to construct linear Laplacian solvers.

16.1 Krylov Subspace and A -Orthonormality

Given a symmetric and positive-deﬁnite matrix A, we start where we left oﬀ in the previous section. Recall that for a vector b , we set up the optimization problem

We presented an algorithm GDSolve which, in its t -th iteration, gen- erates x t where

x 2 = x 1 + η 1 r 1 = x 1 + η 1 ( r 0 − η 0 A r 0 ) = x 0 + η 1 r 0 − η 1 η 0 A r 0 ,

104 Iterative Linear Solvers III The Conjugate Gradient Method

and so on. Here r 0 = b and r i = b − A x i . Thus, it follows by induction that x t lies in x 0 + K t where K t is the subspace spanned by { r 0 ,A r 0 ,...,A t − 1 r 0 } = { b ,A b ,...,A t − 1 b } . K t is called the Krylov subspace of order t generated by A and b .

In GDSolve , although at each iteration t we move greedily along the direction r t − 1 , the resulting point x t which lies in x 0 + K t is not guaranteed to be a minimizer of f over this subspace. That is, there could be a y ∈ x 0 + K t such that f ( y ) < f ( x t ). The main idea of the conjugate gradient method can be summarized in one sentence:

Note that the problem of ﬁnding x is precisely that of ﬁnding the f -minimizer over x 0 + K n . In this section we will see how to make this idea work and will prove the following theorem.

Theorem 16.1. There is an algorithm CGSolve that, given an n × n symmetric matrix A 0 , a vector b , and ε > 0 , ﬁnds a vector x such that

in time O ( t A · κ ( A )log 1 / ε ) .

The ﬁrst question we would like to ask is: How can we ﬁnd the f - minimizer over x 0 + K t quickly ? Suppose we have access to a basis { p 0 , p 1 ,..., p t − 1 } of K t such that for any scalars β 0 ,...,β t − 1 , we have the following decomposition :

i.e., the optimization problem becomes separable in the variables β i . Then, it is suﬃcient to ﬁnd, for all i, α i which is the α that minimizes f ( x 0 + α p i ) − f ( x 0 ): Observe that x t def = x 0 + t − 1 i =1 α i p i minimizes f ( y ) for all y ∈ x 0 + K t . Therefore, if we have access to a basis as described above, we are able to move to a minimizer over x 0 + K t iteratively.

Consider Equation (16.1). If f were a linear function, then equality clearly holds. Thus, we can just look at the quadratic portion of f to decide when this equality holds. For brevity, let v i def = β i p i . Evaluating the l.h.s. of Equation (16.1) gives

1 2 x + i v i A x + i v i − 1 2 x A x = x A i v i + i v i A i v i .

Evaluating the r.h.s. gives 12 i ( x A v i + v i A v i ). The above equations are equal if the cross terms v i A v j are equal to 0. This is precisely the deﬁnition of A -orthonormal vectors.

Deﬁnition 16.1. Given a symmetric matrix A , a set of vectors p 0 , p 1 ,..., p t are A -orthonormal iﬀ for all i = j , p i A p j = 0.

Now we are armed to give an overview of the conjugate gradient algorithm. Let x 0 be the initial vector, and let r 0 def = A ( x − x 0 ). Let p 0 , p 1 ,..., p t be a set of A -orthonormal vectors spanning K t = span { r 0 ,A r 0 ,...,A t − 1 r 0 } . In the next section we will see how to com- pute this A -orthonormal basis for K t . In fact, we will compute the vec- tor p t itself in the ( t + 1)-st iteration taking O (1) extra matrix–vector computations; for the time being, suppose they are given.

Let α t be scalars that minimize f ( x 0 + α p t ) − f ( x 0 ); by a simple calculation, we get α t = p t r 0 p t A p t . The conjugate gradient algorithm updates the vectors as follows

We now show how to compute an A -orthonormal basis of { r 0 ,A r 0 ,... , A t − 1 r 0 } . One could use Gram–Schmidt orthonormalization which

106 Iterative Linear Solvers III The Conjugate Gradient Method

where v is the new vector which one is trying to A -orthonormalize. Given that p j A p i = 0 for all i = j ≤ t, this ensures that p t +1 A p i = 0 for all i ≤ t. Note that the above can take up to O ( t ) matrix–vector calculations to compute p t +1 . We now show that if one is trying to A - orthonormalize the Krylov subspace generated by A , one can get away by performing only O (1) matrix–vector computations. This relies on the fact that A is symmetric.

Let p 0 = r 0 . Suppose we have constructed vectors { p 0 , p 1 ,..., p t } which form an A -orthonormal basis for K t +1 . Inductively assume that the vectors satisfy

i +1 = span { r 0 ,A r 0 ,...,A r 0 } = span { p 0 , p 1 ,..., p i }

and that A p i ∈ K i +2 for all i ≤ t − 1 . Note that this is true when i = as p 0 = r 0 . Let us consider the vector A p t . If A p t ∈ K t +1 , K j = K t +1 for all j > t + 1 and we can stop. On the other hand, if A p t (cid:11)∈ K t +1 , we construct p t +1 by A -orthonormalizing it w.r.t. p i for all i ≤ t. Thus,

This way of picking p t +1 implies, from our assumption in Equa- tion (16.3), that

K t +2 = span { r 0 ,A r 0 ,...,A t +1 r 0 } = span { p 0 , p 1 ,..., p t +1 }

It also ensures that A p t can be written as a linear combination of p i s for i ≤ t + 1 . Hence, A p t ∈ K t +2 , proving our induction hypothesis. Thus, for every i ≤ t, there are constants c j s such that

Hence,

16.3 Analysis via Polynomial Minimization

for all i < t − 1 . Hence, Equation (16.4) simpliﬁes to

Thus, to compute p t +1 we need only O (1) matrix–vector multiplications with A, as promised. This completes the description of the conjugate gradient algorithm which appears formally below.

Algorithm 16.1 CGSolve Input: Symmetric, positive-deﬁnite matrix A ∈ R n × n , b ∈ R n and Output: x T ∈ R n 1: x 0 ← 0 2: r 0 ← b 3: p 0 = r 0 4: for t = 0 → T − 1 do 5: Set α t = p t r 0 p t A p t 6: Set r t = b − A x t 7: Set x t +1 = x t + α t p t 8: Set p t +1 = A p t − p t A 2 p t p t A p t p t − p t A 2 p t − 1 p t − 1 A p t − 1 p t − 1 . 9: end for 10: return x T

Since each step of the conjugate gradient algorithm requires O (1) matrix–vector multiplications, to analyze its running time, it suﬃces to bound the number of iterations. In the next section, we analyze the convergence time of the conjugate gradient algorithm. In partic- ular, we call a point x t an ε -approximate solution if f ( x t ) − f ( x *)
 ) ≤ ε ( f ( x 0 ) − f ( x )). We wish to ﬁnd how many iterations the algorithm needs to get to an ε -approximate solution. We end this section with the following observation: In n steps, the conjugate gradient method returns x exactly. This is because x ∈ x 0 + K n .

16.3 Analysis via Polynomial Minimization

We now utilize the fact that x t minimizes f over the subspace x 0 + K t to prove an upper bound on f ( x t ) − f ( x ). The following is easily seen.

108 Iterative Linear Solvers III The Conjugate Gradient Method

Since x t lies in x 0 + K t , we can write x t = x 0 + 1 i =0 γ i A i r 0 for some scalars γ i . Let p ( x ) be the polynomial deﬁned to be t − 1 i =0 γ i x i . Note that there is a one-to-one correspondence between points in x 0 + K t and degree t − 1 polynomials in one variable. Then, we get x t = x 0 + p ( A ) r 0 = x 0 + p ( A ) A ( x − x 0 ). Therefore, we get

where q ( x ) = 1 − xp ( x ). Now, note that there is a one-to-one corre- spondence between degree t − 1 polynomials and degree t polynomials that evaluate to 1 at 0. Let this latter set of polynomials be Q t . Since x t minimizes x t − x 2 A over K t , we get that f ( x t ) − f ( x ) equals

Before we proceed with the proof of Theorem 16.1, we need the follow-

Lemma

Let A be a symmetric matrix with eigenvalues λ

,...,λ n . Then, for any polynomial p ( · ) and vector v , ( p ( A ) v ) \T.
 A ( p ( A ) v ) ≤ v A v · n max i =

| p ( λ i ) |

.

Proof. Write A = U Γ U 7
 where Γ is the diagonal matrix consisting of eigenvalues and the columns of U are the corresponding orthonormal eigenvectors. Note that p ( A ) = Up (Γ) U . Now for any vector v , we get

v p ( A ) Ap ( A ) v = v ar
 Up (Γ) U U Γ U Up (Γ) U v = v Ty
 U Γ p 2 (Γ) U

Thus, if v = j ζ j u j where u j is the eigenvector of A corresponding to λ j , we get that the l.h.s. of the above equality is A co
 j ζ 2 j λ j p 2 ( λ j ). Similarly, v A v = j ζ 2 j λ j . The lemma follows.

Plugging the above claim into Equation (16.5) we get the following upper bound on f ( x t ) which is crux in the analysis of the conjugate gradient algorithm:

f ( x t ) − f ( x ) ≤ min q ∈Q t n max i =1 | q ( λ i ) | 2 · ( f ( x 0 ) − f ( x )) .

16.3 Analysis via Polynomial Minimization

Note that, since the eigenvalues of A satisfy λ 1 ≤ ··· ≤ λ n , the r.h.s. of Equation (16.6) can be further approximated by

Thus, since f ( x 0 ) = f ( 0 ) = 0 and f ( x ) = − 1 / 2 · x 2 A , we have proved the following lemma about CGSolve .

Lemma 16.3. Let A 0 , λ 1 ,λ n be the smallest and the largest eigen- values of A , respectively, and let Q t be the set of polynomials of degree at most t which take the value 1 at 0 . Then,

Therefore, any polynomial in Q t gives an upper bound on f ( x t ). In particular, the polynomial q ( x ) def = (1 − 2 x λ 1 + λ n ) t gives

where κ = λ n / λ 1 . This is slightly better than what we obtained in Lemma 15.2. As a corollary of the discussion above, we again obtain that after n iterations the solution computed is exact.

Corollary 16.4. After n steps of CGSolve , x n = x . Hence, in O ( t A n ) time, one can compute x = A + b for a positive deﬁnite A.

Proof. Let λ 1 ≤ ··· ≤ λ n be the eigenvalues of A. Consider the polyno- mial q ( x ) def = - ni =1 (1 − x / λ i ) . Note that q (0) = 1 , and q ( λ i ) = 0 for all eigenvalues of A. From Lemma 16.3,

In the previous section we reduced the problem of bounding the error after t iterations to a problem about polynomials. In particular, the goal reduced to ﬁnding a polynomial q ( · ) of degree at most t which takes the value 1 at 0 and minimizes max ni =1 | q ( λ i ) | where λ i s are eigenvalues of A . In this section we present a family of polynomials, called Chebyshev polynomials, and use them to prove Theorem 16.1.

For a nonnegative integer t, we will let T t ( x ) denote the degree t Chebyshev polynomial of the ﬁrst kind, which are deﬁned recursively as follows: T 0 ( x ) def = 1 ,T 1 ( x ) def = x, and for t ≥ 2 ,

The following is a simple consequence of the recursive deﬁnition above.

Proposition 16.5. If x ∈ [ − 1 , 1], then we can deﬁne θ def = arccos( x ), and the degree t Chebyshev polynomial is given by T t (cos θ ) = cos( tθ ). Hence, T t ( x ) ∈ [ − 1 , 1] for all x ∈ [ − 1 , 1].

Proof. First, note that T 0 (cos θ ) = cos0 = 1 and T 1 (cos θ ) = cos θ = x . Additionally, cos(( t + 1) θ ) = cos( tθ )cos( θ ) − sin( tθ )sin( θ ) and simi- larly cos(( t − 1) θ ) = cos( tθ )cos( θ ) − sin( tθ )sin( θ ) . Hence, it is clear that the recursive deﬁnition for Chebyshev polynomials, namely cos(( t + 1) θ ) = 2cos θ cos( tθ ) + cos(( t − 1) θ ), applies.

Thus, letting x = cos θ and using de Moivre’s formula, T t ( x ) = cos( tθ ) 12 (exp( itθ ) + exp( − itθ )) can be written as

which is a polynomial of degree t. For 0 < a < b, we deﬁne the polyno- mial Q a,b,t as follows:

Note that Q a,b,t is of degree t and evaluates to 1 at x = 0 and hence lies in Q t . Furthermore, for x ∈ [ a,b ], the numerator takes a value of at most 1 (by the deﬁnition of T t ). Therefore, if we plug in a = λ 1 and b = λ n , the smallest and the largest eigenvalues of A , respectively, we obtain

Thus, by Lemma 16.3, for any t ≥ Ω ( κ ( A )log 1 / ε ), after t steps of CGSolve we get x t satisfying f ( x t ) − f ( x *)
 ) ≤ ε 2 · ( f ( x 0 ) − f ( x )). Thus, for this value of t,

x t − A + b Dll
 A ≤ ε A + b |
 A .

This completes the proof of Theorem 16.1.

16.5 The Chebyshev Iteration

In this section we consider the possibility of attaining a bound of κ ( A )log 1 / ε iterations when x t = p t ( A ) b where p t is a sequence of polynomials. When compared with CGSolve , this will have the advan- tage that x t is a linear operator applied to b . As in the proof of Lemma 16.3, it is suﬃcient to deﬁne p t s such that

max x ∈ [ λ 1 ,λ n ] | xp t ( x ) − 1 | ≤ O (1 − λ 1 / λ n ) t ,

x t − A + b p|l?
 2 A ≤ O (1 − λ 1 / λ n ) t · A + b p|l?
 2 A

Thus, in O ( κ ( A )log 1 / ε ) iterations x t is ε close to A + b . Note that Equation (16.9) implies that, if we let Z t def = p t ( A ) , then for all b , Z b − A + b DI
 ≤ O (1 − λ / λ ) t .

112 Iterative Linear Solvers III The Conjugate Gradient Method

Hence, if 2
 Z t b − A + b pI
 A ≤ δ, then Z t − A i)
 ≤ δ · A + 1 / 2 . In fact, we can set p t to be Q λ 1 ,λ n ,t , deﬁned using the Chebyshev polynomials in the previous section, and use Equation (16.8) to obtain the bound in Equation (16.9). The only issue that remains is how to compute Q λ 1 ,λ n ,t ( A ) b in ˜ O ( t A t )-time. This can be done using the recursive deﬁ- nition of the Chebyshev polynomials in the previous section and is left as an exercise. The update rule starts by setting

for ﬁxed scalars α 0 ,α 1 ,α 2 which depend on λ 1 and λ n . This is called the Chebyshev iteration and, unlike CGSolve , requires the knowledge of the smallest and the largest eigenvalues of A. 1 We summarize the discussion in this section in the following theorem.

Theorem 16.6. There is an algorithm which given an n × n sym- metric positive-deﬁnite matrix A, a vector b , numbers λ l ≤ λ 1 ( A ) and λ u ≥ λ n ( A ) and an error parameter ε > 0 , returns an x such that

(

) x − A + b Dl
 A ≤ ε · A + b Dl
 A , (

) x = Z b where Z depends only on A and ε, and (

) Z − A + ≤ ε,

where we have absorbed the A + 1 / 2 term in the error. The algorithm runs in ( t A · λ u / λ l log 1 / ( ελ l )) time.

16.6 Matrices with Clustered Eigenvalues

As another corollary of the characterization of Lemma 16.3 we show that the rate of convergence of CGSlove is better if the eigenvalues of A are clustered . This partly explains why CGSolve is attractive in practice; it is used in Section 17 to construct fast Laplacian solvers.

For our application in Section 18, we will be able to provide estimates of these eigenvalues.

Corollary 16.7. For a matrix A , suppose all but c eigenvalues are contained in a range [ a,b ]. Then, after t ≥ c + O ( b / a log 1 / ε ) iterations,

Proof. Let q ( x ) def = Q a,b,i ( x ) · Π ci =1 (1 − x / λ i ) where λ 1 ,...,λ c are the c eigenvalues outside of the interval [ a,b ] , and Q a,b,i deﬁned earlier. From Lemma 16.3, since the value of q ( λ i ) = 0 for all i = 1 ,...,c , we only need to consider the maximum value of | q ( x ) | 2 in the range [ a,b ]. By the properties of Q a,b,i described above, if we pick i = Θ( b / a log 1 / ε ), for all x ∈ [ a,b ], | q ( x ) | 2 ≤ ε . Therefore, CGSolve returns an ε -approximation in c + O ( la
 b / a log 1 / ε ) steps; note that this could be much better than κ ( A ) since no restriction is put on the c eigenvalues.

Notes

The conjugate gradient method was ﬁrst proposed in [39]. There is a vast amount of literature on this algorithm and the reader is directed to the work in [36, 37, 68, 72]. More on Chebyshev polynomials and their centrality in approximation theory can be found in the books [21, 65]. An interested reader can check that a Chebyshev-like iterative method with similar convergence guarantees can also be derived by applying Nesterov’s accelerated gradient descent method to the convex function in the beginning of this section.

In this section, the notion of preconditioning is introduced. Instead of solving A x = b , here one tries to solve P A x = P b for a matrix P such that κ ( P A ) κ ( A ) where it is not much slower to compute P v than A v , thus speeding up iterative methods such as the conjugate gradient method. As an application, preconditioners are constructed for Laplacian systems from low-stretch spanning trees that result in a ˜ O ( m 4 / 3 ) time Laplacian solver. Preconditioning plays an important role in the proof of Theorem 3.1 presented in Section 18. Low-stretch spanning trees have also been used in Section 4.

17.1

17.1 Preconditioning

We saw in the previous section that using the conjugate gradient method for a symmetric matrix A > (
 0 , one can solve a system of equa- tions A x = b in time O ( t A · κ ( A )log 1 / ε ) up to an error of ε. Let us focus on the two quantities in this running time bound: t A which is the time it takes to multiply a vector with A, and κ ( A ) , the condition num- ber of A. Note that the algorithm requires only restricted access to A : Given a vector v , output A v . Thus, one strategy to reduce this running

time is to ﬁnd a matrix P s.t. κ ( P A ) κ ( A ) and t P ∼ t A . Indeed, if we had access to such a matrix, we could solve the equivalent system of equations P A x = P b . 1 Such a matrix P is called a preconditioner for A. One choice for P is A + . This reduces the condition number to 1 but reduces the problem of computing A + b to itself, rendering it useless. Surprisingly, as we will see in this section, for a Laplacian sys- tem one can often ﬁnd preconditioners by using the graph structure, thereby reducing the running time signiﬁcantly. The following theorem is the main result of this section.

Theorem 17.1. For any undirected, unweighted graph G with m edges, a vector b with b , 1 1)
 = 0, and ε > 0 , one can ﬁnd an x such that x − L + G b DI
 L G ≤ ε L + G b bil
 L G in ˜ O ( m 4 / 3 log 1 / ε ) time.

There are two crucial ingredients to the proof. The ﬁrst is the following simple property of CGSolve .

Lemma 17.2. Suppose A is a symmetric positive-deﬁnite matrix with minimum eigenvalue λ 1 ≥ 1 and trace Tr( A ) ≤ τ . Then CGSolve con- verges to an ε -approximation in O ( τ 1 / 3 log 1 / ε ) iterations.

Proof. Let Λ be the set of eigenvalues larger than τ / γ , where γ is a parameter to be set later. Note that | Λ | ≤ γ . Therefore, apart from these γ eigenvalues, all the rest lie in the range [1 , τ / γ ]. From Corollary 16.7, we get that CGSolve ﬁnds an ε -approximation in γ + O ( τ / γ log 1 / ε ) iterations. Choosing γ def = τ 1 / 3 completes the proof of the lemma.

The second ingredient, and the focus of this section, is a construction of a combinatorial preconditioner for L G . The choice of preconditioner is L + T where T is a spanning tree of G . Note that from Corollary 13.4, this preconditioner satisﬁes the property that L + T v can be computed in O ( n ) time. Thus, the thing we need to worry about is how to construct a spanning tree T of G such that κ ( L + T L G ) is as small as possible.

1 One might worry that the matrix PA may not be symmetric; one normally gets around this by preconditioning by P 1 / 2 AP 1 / 2 . This requires P 0 .

Combinatorial Preconditioning via Trees

Let G be an unweighted graph with an arbitrary orientation ﬁxed for the edges giving rise to the vectors b e which are the rows of the corre- sponding incidence matrix. We start by trying to understand why, for a spanning tree T of a graph G, L + T might be a natural candidate for preconditioning L G . Note that

Thus, I ≺ L + T L G , which implies that λ 1 ( L + T L G ) ≥ 1 . Thus, to bound the condition number of κ ( L + T L G ) , it suﬃces to bound λ n ( L + T L G ) . Unfortunately, there is no easy way to bound this. Since L + T L G is PSD, an upper bound on its largest eigenvalue is its trace, Tr( L + T L G ) . If this upper bound is τ, Lemma 17.2 would imply that the con- jugate gradient method applied to L + T L G takes time approximately τ 1 / 3 t L + T L G ∼ O ( τ 1 / 3 ( m + n )) . Thus, even though it sounds wasteful to bound the trace by τ rather than by the largest eigenvalue by τ, 1 /

bound the trace by τ rather than by the largest eigenvalue by τ, Lemma 17.2 allows us to improve the dependency on τ from τ 1 / 2 to 1 / 3 . We proceed to bound the trace of L + T L G :

where we use Tr( A + B ) = Tr( A ) + Tr( B ) and Tr( ABC ) = Tr( CAB ) . Note that b AT]
 e L + T b e is a scalar and is precisely the eﬀective resistance across the endpoints of e in the tree T where each edge of G has a unit resistance. The eﬀective resistance across two nodes i,j in a tree is the sum of eﬀective resistances along the unique path P ( i,j ) on the tree. Thus, we get

A trivial upper bound on Tr( L + T L G ) , thus, is nm. This can also be shown to hold when G is weighted. This leads us to the following deﬁnition.

Deﬁnition 17.1. For an unweighted graph G, the stretch of a spanning T is deﬁned to be str T ( G ) def = Tr( L + T L G ) .

Thus, to obtain the best possible bound on the number of iterations of CGSolve , we would like a spanning tree T of G which minimizes the average length of the path an edge of G has to travel in T. The time it takes to construct T is also important.

17.3 An ˜ O ( m 4 / 3 ) -Time Laplacian Solver

In this section we complete the proof of Theorem 17.1. We start by stating the following nontrivial structural result about the existence and construction of low-stretch spanning trees whose proof is graph- theoretic and outside the scope of this monograph. The theorem applies to weighted graphs as well.

Theorem 17.3. For any undirected graph G, a spanning tree T can be constructed in ˜ O ( m log n + n log n loglog n ) time such that str T ( G ) = ˜ O ( m log n ) . Here ˜ O ( · ) hides loglog n factors.

This immediately allows us to conclude the proof of Theorem 17.1 using Lemma 17.2 and the discussion in the previous section. The only thing that remains is to address the issue that the matrix L + T L G is symmetric. The following trick is used to circumvent this diﬃculty. Recall from Theorem 13.3 that the Cholesky decomposition of a tree can be done in O ( n ) time. In particular, let L T = EE , where E is a lower triangular matrix with at most O ( n ) nonzero entries. The idea is to look at the system of equations E + L G E + y = E + b instead of L G x = b . If we can solve for y then we can ﬁnd x = E + y , which is computationally fast since E is lower triangular with at most O ( n ) nonzero entries. Also, for the same reason, E + b can be computed quickly.

Now we are in good shape. Let A def = E + L G E + and b 7
 def = E + b ; note that A is symmetric. Also, the eigenvalues of A are the same as those of E + AE = L + T L G . Thus, the minimum eigenvalue of A is 1

and the trace is ˜ O ( m ) by Theorem 17.3. Thus, using the conjugate gra- dient method, we ﬁnd an ε -approximate solution to A y = b in ˜ O ( m 1 / 3 ) iterations.

In each iteration, we do O (1) matrix–vector multiplications. Note that for any vector v , A v can be computed in O ( n + m ) operations since E + v takes O ( n ) operations (since E is a lower triangular matrix with at most O ( n ) nonzero entries) and L G v takes O ( m ) operations (since L G has at most O ( m ) nonzero entries).

Notes

While preconditioning is a general technique for Laplacian systems, its use originates in the work of Vaidya [86]. Using preconditioners, Vaidya obtained an ˜ O ((∆ n ) 1 . 75 log 1 / ε ) time Laplacian solver for graphs with maximum degree ∆ . Vaidya’s paper was never published and his ideas and their derivatives appear in the thesis [42]. Boman and Hendrickson [18] use low-stretch spanning trees constructed by Alon et al. [6] to obtain ε -approximate solutions to Laplacian systems roughly in m 3 / 2 log 1 / ε time. The main result of this section, Theorem 17.1, is from [81]. The ﬁrst polylog n stretch trees in near-linear-time were constructed by Elkin et al. [27]. Theorem 17.3 is originally from [1], and improvements can be found in [2]. An important open problem is to ﬁnd a simple proof of Theorem 17.3 (potentially with worse log factors).

Building on the techniques developed hitherto, this section presents an algorithm which solves L x = b in ˜ O ( m ) time.

18.1 Main Result and Overview

In Section 17 we saw how preconditioning a symmetric PSD matrix A by another symmetric PSD matrix P reduces the running time of computing A + b for a vector b to about O (( t A + t P ) κ ( P A )) , where t A and t P are the times required to compute matrix–vector product with A and P , respectively. We saw that if A = L G for a graph G, then a natural choice for P is L + T where T is a spanning tree of G with small total stretch. 1 For a graph G with edge weights given by w G and a tree T, the stretch of an edge e ∈ E ( G ) in T is deﬁned to be

where P e is the set of edges on the unique path that connects the endpoints of e in T. Thus, if e ∈ T, then str T ( e ) = 1 . We deﬁne

1 Unless speciﬁed, when we talk about a spanning tree T of a weighted graph, the edges of T inherit the weights from G.

T ( G ) def = e ∈ E str T ( e ) . With this deﬁnition, it is easy to see that

This is the same as the deﬁnition in Section 17 where Theorem 17.3 asserted a remarkable result: A spanning tree T such that str T ( G ) = ˜ O ( m ) can be constructed in time ˜ O ( m ). Note that choosing a spanning tree is convenient since we can compute L + T v exactly in O ( n ) time using Cholesky decomposition, see Theorem 13.3. If T is such a spanning tree and L T is its Laplacian, then, using L + T as a preconditioner, we showed in Theorem 17.1 how the conjugate gradient method can be used to compute a vector x such that x − L + G b L G ≤ ε aT
 L + G b Dl
 L G in ˜ O ( m 4 / 3 log 1 / ε ) time. In this section we improve this result and prove the following theorem; the key result of this monograph.

Theorem 18.1. For any undirected, unweighted graph G with m edges, a vector b with b , 1 1)
 = 0, and ε > 0 , one can ﬁnd an x such that x − L + G b DI
 L G ≤ ε L + G b bil
 L G in ˜ O ( m log 1 / ε ) time.

This theorem is a restatement of Theorem 3.1 where the algorithm which achieves this bound is referred to as LSolve . In this section we present LSolve and show that it runs in ˜ O ( m log 1 / ε ) time. Toward the end we discuss the linearity of LSolve : Namely, the output x of LSolve on input L G , b and ε is a vector Z x , where Z is a n × n matrix that depends only on G and ε, and satisﬁes Z − L + ≤ ε.

Proof Overview

Unlike the proof of Theorem 17.1, it is not clear how to prove Theorem 18.1 by restricting to tree preconditioners. The reason is that, while for a tree T of G we can compute L + T v in O ( n ) time, we do not know how to improve upon the upper bound on κ ( L + T L G ) beyond what was presented in Section 17. A natural question is whether we can reduce the condition number by allowing more edges than those contained in a tree. After all, the Cholesky decomposition-based algo- rithm to solve tree systems in Section 13 works if, at every step during the elimination process, there is always a degree 1 or a degree 2 vertex.

The ﬁrst observation we make, proved in Section 18.2, is that if we have a graph on n vertices and n − 1 + k edges (i.e., k more edges than in a tree), after we are done eliminating all degree 1 and 2 ver- tices, we are left with a graph that has at most 2( k − 1) vertices and 3( k − 1) edges. Thus, if k is not too large, there is a serious reduc- tion in the size of the linear system that is left to solve. Now, can we ﬁnd a subgraph H of G (whose edges are allowed to be weighted) that has at most n − 1 + k edges and the condition number of L + H L G is much smaller than n ? The answer turns out to be yes. We achieve this by a combination of the spectral sparsiﬁcation technique from Sec- tion 10 with low-stretch spanning trees. Speciﬁcally, for a graph G with n vertices and m edges, in ˜ O ( m ) time we can construct a graph H with n − 1 + k edges such that κ ( L + H L G ) ≤ O ( ( m log 2 n ) / k ) . Thus, if we choose k = m / (100log 2 n ) , apply crude sparsiﬁcation followed by eliminating all degree 1 and 2 vertices, we are left with a Laplacian system of size O ( m / (100log 2 n ) ) , call it ˜ G. The details of how to ﬁnd such an H appear in Section 18.3.

We now need to make about κ ( L + H L G ) ≤ 10log 2 n calls to the con- jugate gradient method to solve for L + G b . This immediately leads us to the problem of computing approximately 10log 2 n products of the form L + H v for some vector v . This, in turn, requires us to compute the same number of L +˜ G u products. ˜ G is neither a tree nor does it contain any ˜

the problem of computing approximately 10log n products of the form L + H v for some vector v . This, in turn, requires us to compute the same number of L +˜ G u products. ˜ G is neither a tree nor does it contain any degree 1 or 2 vertices. Thus, to proceed, we recurse on ˜ G. Fortunately, the choice of parameters ensures that κ ( L + H L G ) / (100log 2 n ) ≤ 1 / 10 . This shrinkage ensures that the work done at any level remains bounded by ˜ O ( m ) . We stop the recursion when the size of the instance becomes polylog n. This means that the depth of the recursion is bounded by O ( log n / loglog n ) . The details of how the recursion is employed and how the parameters are chosen to ensure that the total work remains ˜ O ( m ) are presented in Section 18.4.

There is an important issue regarding the fact that the conjugate gradient is error-prone. While in practice this may be ﬁne, in the- ory, this can cause serious problems. To control the error, one has to replace the conjugate gradient with its linear version, Chebyshev itera- tion, presented in Section 16.5. This results in an algorithm that makes

LSolve a linear operator as claimed above. The details are presented in Section 18.5 and can be omitted.

If we were to present all the details that go into the proof of Theorem 18.1, including a precise description of LSolve , it would be overwhelming to the reader and make the presentation quite long. Instead, we bring out the salient features and important ideas in the algorithm and proof, and show how it marries ideas between graph the- ory and linear algebra that we have already seen in this monograph. The goal is not to convince the reader about the proof of Theorem 18.1 to the last constants, but to give enough detail to allow a keen reader to reconstruct the full argument and adapt it to their application. Finally, in what follows, we will ignore poly(loglog n ) factors.

18.2 Eliminating Degree 1 , 2 Vertices

Suppose H is a graph with n vertices and m = n − 1 + k edges. As in Section 13.2, we greedily eliminate all degree 1 and 2 vertices to obtain a graph G . If m ≤ n − 1 , then we can compute L + H v in O ( n ) time via Theorem 13.3. Hence, we may assume that k ≥ 1 . This reduces the computation of L + H v to that of computing L + G v in O ( m + n )
 ) time.

How many edges and vertices does G have? Let k 1 and k 2 be the number of vertices of degree 1 and 2 eliminated from H , respectively. Then | V ( G ) | = n − k 1 − k 2 and | E ( G ) | ≤ m − k 1 − k 2 . The latter occurs because we removed two edges adjacent to a degree 2 vertex and added one edge between its neighbors. In G , however, every vertex has degree at least 3 . Hence,

( n − k

− k

) ≤

( m − k

− k

) =

( n −

+ k − k

− k

Hence, | V ( G ) | = n − k

− k

≤

( k −

), and consequently, | E ( G ) | ≤ m − k

− k

≤

( k −

) + k −

=

( k −

) .

Thus, we have proved the following lemma.

Lemma 18.2. Given a graph H with n vertices and m = n − 1 + k edges for k ≥ 1, the computation of L + H v can be reduced to computing L + G v in time O ( n + m )
 ) where G has at most 2( k − 1) vertices and at most 3( k − 1) edges.

18.3 Crude Sparsiﬁcation Using Low-Stretch Spanning Trees

We now present the following crude spectral sparsiﬁcation theorem, which is a rephrasing of Theorem 10.4.

Theorem 18.3. There is an algorithm that, given a graph G with edge weights w G , γ > 0 , and numbers q e such that q e ≥ w G ( e ) R e for all e, outputs a weighted graph H s.t.

and O ( W log W log 1 / γ ) edges. The algorithm succeeds with probability at least 1 − γ and runs in ˜ O ( W log W log 1 / γ ) time. Here W def = e q e and R e is the eﬀective resistance of e.

We show how one can compute q e s that meet the condition of this the- orem and have small e q e in time O ( m log n + n log 2 n ). Thus, we do not rely on the Laplacian solver as in the proof of Theorem 10.2. First notice that if T is a spanning tree of G with weight function w G and e ∈ G, then R e ≤ f ∈ P e w − 1 G ( f ) , where P e is the path in T with end- points the same as e. This is because adding more edges only reduces the eﬀective resistance between the endpoints of e. Hence, w e R e ≤ str T ( e ) . Thus, str T ( e ) satisﬁes one property required by q e in the theorem above. Moreover, we leave it as an exercise to show that, given T, one can com- pute str T ( e ) for all e ∈ G in O ( m log n ) time using elementary methods. The thing we need to worry about is how to quickly compute a T with small thing
 e str T ( e ) . This is exactly where the low-stretch spanning trees from Theorem 17.3, and mentioned in the introduction to this section, come into play. Recall that, for a graph G with weight function w G , the tree T from Theorem 17.3 satisﬁes e ∈ G str T ( e ) = O ( m log n ) and can be computed in time O ( n log 2 n + m log n ) . Hence, the number of edges in the crude sparsiﬁer from Theorem 18.3 is O ( m log 2 n ) if we set γ = 1 / log n . This is not good as we have increased the number of edges in the sampled graph.

There is a trick to get around this. Let 1 κ < m be an addi- tional parameter and let T be the low-stretch spanning tree from Theorem 17.3 for the graph G . Consider the graph ˆ G = ( V,E ) with

weight function w ˆ G ( e ) def = κw G ( e ) if e ∈ T and w ˆ G ( e ) def = w G ( e ) if e (cid:11)∈ T. Thus, in ˆ G we have scaled the edges of T by a factor of κ. This trivially implies that

We now let q e def = str T ( e ) = 1 if e ∈ T and q e def = 1 / κ · str T ( e ) if e (cid:11)∈ T. Thus, again it can be checked that the q e s satisfy the conditions for Theorem 18.3 for ˆ G with weights w ˆ G . First note that

as str T ( G ) = O ( m log n ) . Now, let us estimate the number of edges we obtain when we sample from the distribution { q e } e ∈ E approximately W log W times as in Theorem 10.4. Note that even if an edge is chosen multiple times in the sampling process, there is exactly one edge in H corresponding to it with a weight summed up over multiple selections. Thus, in H , we account for edges from T separately. These are exactly n − 1 . On the other hand, if an edge e / ∈ T , a simple Chernoﬀ bound argument implies that with probability at least 1 − 1 / n 0 . 1 1 − 1 / log n , the number of such edges chosen is at most O ( m log 2 n / κ ) . Since it takes O ( n log 2 n + m log n ) time to ﬁnd T and approximately W log W addi- tional time to sample H, in

time, with probability at least 1 − 1 / log 2 n , we obtain a graph H such that

time, with probability at least 1 − 1 / log 2 n , we obtain a graph H such

Moreover, the number of edges in H is n − 1 + O ( m log 2 n / κ ). Impor- tantly, κ ( L + H L G ) = O ( κ ) . Thus, to compute L + G v , we need about O ( √ κ ) computations of the form L + H u if we deploy the conjugate gradient method. To summarize, we have proved the following lemma.

Lemma 18.4. There is an algorithm that, given a graph G on n ver- tices with m edges, a vector v , an ε > 0 and a parameter κ, constructs

18.4 Recursive Preconditioning — Proof of the Main Theorem

a graph H on n vertices such that, with probability at least 1 − 1 / log

(

) κ ( L + H L G ) = O ( κ ) , (

) the number of edges in H is n −

+ O ( m log

n / κ ), and (

) the time it takes to construct H is O ( n log

n + m log n + m log

n / κ ) .

Now, if we apply the greedy degree 1 , 2 elimination procedure on H, we obtain G with O ( m log 2 n / κ ) edges and vertices. Thus, we have reduced our problem to computing L + G u for vectors u where the size of G has gone down by a factor of κ from that of G.

Recursive Preconditioning — Proof of the Main Theorem

Now we show how the ideas we developed in the last two sections can be used recursively. Our starting graph G = G 1 has m 1 = m edges and n 1 = n vertices. We use a parameter κ < m which will be determined later. We then crudely sparsify G 1 using Lemma 18.4 to obtain H 1 such that, with probability 1 − 1 / log n , (this probability is ﬁxed for all iterations), the number of edges in H 1 is at most n 1 − 1 + O ( m 1 log 2 n 1 / κ ) and κ ( L + H 1 L G 1 ) = O ( κ ) . If H 1 does not satisfy these properties, then we abort and restart from G 1 . Otherwise, we apply greedy elimination, see Lemma 18.2, to H 1 and obtain G 2 ; this is a deterministic procedure. If n 2 ,m 2 are the number of vertices and edges of G 2 , then we know that m 2 = O ( m 1 log 2 n 1 / κ ) and n 2 = O ( m 1 log 2 n 1 / κ ) . If n 2 ≤ n f (for some n f to be determined later), then we stop, otherwise we iterate and crude sparsify G 2 . We follow this with greedy elimination to get H 2 and proceed to ﬁnd G 3 and so on. Once we terminate, we have a sequence

such that

(

) n d = O ( n f ), (

) For all

≤ i ≤ d −

, κ ( L + H i L G i ) = O ( κ ) , and (

) For all

≤ i ≤ d −

, n i ,m i ≤ m i −

log

n i −

κ .

Note that n i ,m i are the number of vertices and edges of G i , and not H i . The probability that this process is never restarted is at least 1 − d / log n . For this discussion we assume that we use the conjugate gradient method which, when given H and G , can compute L + G u for a

given vector u with about κ ( L + H L G ) computations of the form L + H v for some vector v . The (incorrect) assumption here is that there is no error in computation and we address this in the next section. The depth of the recursion is

Now to compute L + G 1 u for a given vector u , the computation tree is of degree O ( √ κ ) and depth d. At the bottom, we have to compute roughly ( √ κ ) d problems of the type L + G d v . We can do this exactly in time O ( n 3 f ) using Gaussian elimination. Hence, the total work done at the last level is

At level i, the amount of work is

We want the sum over all except the top level to be O ( m ) . This is achieved if

Finally, note that the work done in building the sequence of precondi-

Finally, note that the work done in building the sequence of precondi- tioners is

O ( m i log n i + n i log 2 n i + m i log 2 n i / κ ) = O ( n log 2 n + m log n )

if κ ≥ log 3 n. The depth of the recursion is d = nf log κ / log2 n . Hence, choice of parameters is dictated by the following constraints:

(

) O ( n

f ) · ( κ ) d = O ( m log

n (

) O ( √ κ · log

n κ ) ≤

, and (

) κ ≥ log

n.

at the top level and O ( m log 2 n ) in the remaining levels. We set κ def = 100log 4 n and n f def = log n. This choice of κ and n f also ensures that d is at most log n / (2loglog n ) and, hence, condition (2) above is also satisﬁed. Finally, the probability that we never restart is at least 1 − d / log n 1 − 1 / 100 for large enough n. The total running time is bounded by O (( m + n )log 2 n ) . In the calculations above, note that as long as there is an algorithm that reduces computing A + b to computing at most κ ( A ) 1 − ε products A u , one can obtain an algorithm that runs in ˜ O ( m log O ( 1 / ε ) n ) time. This concludes the proof of Theorem 18.1 assuming that there is no error in the application of the conjugate gradient method. In the following section we the intuition behind managing this error.

18.5 Error Analysis and Linearity of the Inverse

As discussed above, the conjugate gradient is not error-free. Since our algorithm uses conjugate gradient recursively, the error could cascade in a complicated manner and it is not clear how to analyze this eﬀect. Here, we use the Chebyshev iteration, see Section 16.5, which achieves the same running time as the conjugate gradient method but has the additional property that on input a matrix A, a vector b , and an ε > 0 , outputs x = Z b where Z is a matrix such that (1 − ε ) Z A + (1 + ε ) Z. This extra property comes at a price; it requires the knowl- edge of the smallest and the largest eigenvalues of A. A bit more for- mally, if we are given a lower bound λ l on the smallest eigenvalue and an upper bound λ u on the largest eigenvalue of A, then, after O ( λ u / λ l log 1 / ε ) iterations, the Chebyshev iteration-based method out- puts an x = Z b such that (1 − ε ) Z A + (1 + ε ) Z. Let us see how to use this for our application. To illustrate the main

Let us see how to use this for our application. To illustrate the main idea, consider the case when the chain consists of

where we compute L + G 3 u exactly. In addition, we know that all the eigenvalues of L + H i L G i lie in the interval [1 ,κ ] , where we ﬁxed κ ∼ log 4 n. Given L + G 3 , it is easy to obtain L + H 2 since G 3 is obtained by elimi- nating variables from H 2 ; hence there is no error. Thus, if we want

to compute L + G 2 u , we use the Chebyshev iteration-based solver from Theorem 16.6 with error ε and the guarantee that all eigenvalues of L + H 2 L G 2 lie in the interval [1 ,κ ]. Thus, the linear operator that tries to approximate L + G 2 , Z 2 , is such that (1 − ε ) Z 2 L + G 2 (1 + ε ) Z 2 . This is where the error creeps in and linearity is used. From Z 2 we can easily construct ˜ Z 1 which is supposed to approximate L + H 1 ; namely, (1 − ε ) ˜ Z 1 L + H 1 (1 + ε ) ˜ Z 1 . This implies that the eigenvalues for our approximator ˜ Z 1 L G 1 of L + H 1 L G 1 which were supposed to lie in the interval [1 ,κ ] may spill out. However, if we enlarge the interval to [ 1 / 1+ δ , (1 + δ ) κ ] for a small constant δ suﬃciently bigger than ε, (at the expense of increasing the number of iterations by a factor of 1 + δ ) we ensure that (1 − ε ) Z 1 L + G 1 (1 + ε ) Z 1 . This argument can be made formal via induction by noting that κ,ε , and δ remain ﬁxed through out.

Notes

Theorem 18.1 was ﬁrst proved by Spielman and Teng [77, 78, 79, 80] and the proof presented in this section, using crude sparsiﬁers, draws sig- niﬁcantly from [49, 50]. The idea of eliminating degree 1 and 2 vertices was implicit in [86]. The idea of recursive preconditioning appears in the thesis [42]. Theorem 18.3 is in [49].

This section looks beyond solving linear equations and considers the more general problem of computing f ( A ) v for a given PSD matrix A, vector v and speciﬁed function f ( · ) . A variant of the conjugate gradient method, called the Lanczos method, is introduced which uses Krylov subspace techniques to approximately compute f ( A ) v . The results of this section can also be used to give an alternative proof of the main result of Section 9 on computing the matrix exponential.

19.1 From Scalars to Matrices

f : R (cid:9)→ R . Then one can deﬁne f ( A ) as follows: Let u 1 ,..., u n be eigen- vectors of A with eigenvalues λ 1 ,...,λ n , then f ( A ) def = i f ( λ i ) u i u i . Given a vector v , we wish to compute f ( A ) v . One way to do this exactly is to implement the deﬁnition of f ( A ) as above. This requires the diagonalization of A, which is costly. Hence, in the interest of speed, we are happy with an approximation to f ( A ) v . The need for such primitive often arises in theory and practice. While the conjugate gra- dient method allows us to do this for f ( x ) = 1 / x , it seems speciﬁc to this function. For instance, it is not clear how to adapt the conjugate

gradient method to compute exp( A ) v , a primitive central in several areas of optimization and mathematics. See Section 9 for the deﬁni- tion of matrix exponential. In this section we present a meta-method, called the Lanczos method, to compute approximations to f ( A ) v . The method has a parameter k that provides better and better approxima- tions to f ( A ) v as it increases: The error in the approximation after k rounds is bounded by the maximum distance between the best degree k polynomial and f in the interval corresponding to the smallest and the largest eigenvalues of A. The linear algebra problem is, thus, reduced to a problem in approximation theory. The following is the main result of this section.

Theorem 19.1. There is an algorithm that, given a symmetric PSD matrix A , a vector v with v = 1 , a function f , and a positive integer parameter k , computes a vector u such that,

W
 f ( A ) v − u ≤ 2 · min p k ∈ Σ k max λ ∈ Λ( A ) | f ( λ ) − p k ( λ ) |

Here Σ k denotes the set of all degree k polynomials and Λ( A ) denotes the interval containing all the eigenvalues of of A . The time taken by the algorithm is O (( n + t A ) k + k 2 ) .

We remark that this theorem can be readily applied to the computation of exp( A ) v ; see the notes.

19.2 Working with Krylov Subspace

For a given positive integer k, the Lanczos method looks for an approxi- mation to f ( A ) v of the form p ( A ) v where p is a polynomial of degree k. Note that for any polynomial p of degree at most k, the vector p ( A ) v is a linear combination of the vectors { v ,A v ,...,A k v } . The span of these vectors is referred to as the Krylov subspace of order k of A w.r.t. v and is deﬁned below.

Deﬁnition 19.1. Given a matrix A and a vector v , the Krylov sub- space of order k , denoted by K k , is deﬁned as the subspace that is spanned by the vectors { v ,A v ,...,A k v } .

Since A and v are ﬁxed, we denote this subspace by K k . (This deﬁnition diﬀers slightly from the one in Section 16 where K k +1 was used to denote this subspace.) Note that any vector in K k has to be of the form p ( A ) v , where p is a degree k polynomial. The Lanczos method to compute f ( A ) v starts by generating an orthonormal basis for K k . Let v 0 ,..., v k be any orthonormal basis for K k , and let V k be the n × ( k + 1) matrix with { v i } ki =0 as its columns. Thus, V k V k = I k +1 and V k V k denotes the projection onto the subspace. Also, let T k be the operator A in the basis { v i } ki =0 restricted to this subspace, i.e., T k def = V k AV k . Since all the vectors v ,A v ,...,A k v are in the subspace, any of these vectors (or their linear combination) can be obtained by applying T k to v (after a change of basis), instead of A . The following lemma states this formally.

Lemma 19.2. Let V k be the orthonormal basis and T k be the oper- ator A restricted to K k where v = 1, i.e., T k = V k AV k . Let p be a polynomial of degree at most k . Then,

p ( A ) v = V k p ( T k ) V k v .

Proof. Recall that V k V k is the orthogonal projection onto the subspace K k . By linearity, it suﬃces to prove this for p = x t for all t ≤ k . This is true for t = 0 since V k V k v = v . For all j ≤ k , A j v lies in K k , thus, V k V k A j v = A j v . Hence,

The next lemma shows that V k f ( T k ) V k v approximates f ( A ) v as well as the best degree k polynomial that uniformly approximates f . The proof is based on the observation that if we express f as a sum of any degree k polynomial and an error function, the above lemma shows that the polynomial part is computed exactly in this approximation.

Lemma 19.3. Let V k be the orthonormal basis, and T k be the opera- tor A restricted to K k where v = 1, i.e., T k = V k AV k . Let f : R → R be any function such that f ( A ) and f ( T k ) are well deﬁned. Then, be

Wy
 f ( A ) v − V k f ( T k ) V k v th
 is at most

min p k ∈ Σ k max λ ∈ Λ( A ) | f ( λ ) − p k ( λ ) | + max λ ∈ Λ( T k ) | f ( λ ) − p k ( λ ) |

Minimizing over p k gives us our lemma.

Observe that in order to compute this approximation, we do not need to know the polynomial explicitly. It suﬃces to prove that there exists a degree k polynomial that uniformly approximates f on the inter- val containing the spectrum of A and T k (for exact computation, Λ( T k ) = [ λ min ( T k ) ,λ max ( T k )] ⊆ [ λ min ( A ) ,λ max ( A )] = Λ( A ).) Moreover, if k n , the computation is reduced to a much smaller matrix. We now show that an orthonormal basis for the Krylov subspace, V k , can be computed quickly and then describe the Lanczos procedure which underlies Theorem 19.1.

19.3 Computing a Basis for the Krylov Subspace

In this section, we show that if we construct the basis { v i } ki =0 in a particular way, the matrix T k has extra structure. In particular, if A is symmetric, we show that T k must be tridiagonal . This helps us speed up the construction of the basis.

19.3 Computing a Basis for the Krylov Subspace 133

Suppose we compute the orthonormal basis { v i } ki =0 iteratively, starting from v 0 = v : For i = 0 ,...,k , we compute A v i and remove the components along the vectors { v 0 ,..., v i } to obtain a new vec- tor that is orthogonal to the previous vectors. This vector, scaled to norm 1, is deﬁned to be v i +1 . These vectors, by construction, are such that for all i ≤ k, Span { v 0 ,..., v i } = Span { v ,A v ,...,A k v } . Note that ( T k ) ij = v i A v j .

Moreover, if A is symmetric, v Ty
 j ( A v i ) = v Te
 i ( A v j ) , and hence T k is symmetric and tridiagonal. This means that at most three coeﬃcients are nonzero in each row. Thus, while constructing the basis, at step i + 1, we need to orthonormalize A v i only w.r.t. v i − 1 and v i . This fact is used for eﬃcient computation of T k . The algorithm Lanczos appears in Figure 19.1.

Completing the Proof of Theorem 19.1

The algorithm Lanczos implements the Lanczos method discussed in this section. The guarantee on u follows from Lemma 19.3 and the fact that Λ( T k ) ⊆ Λ( A ). We use the fact that ( T k ) ij = v i A v j and that T k must be tridiagonal to reduce our work to just computing O ( k ) entries in T k . The total running time is dominated by k multiplications of A with a vector, O ( k ) dot-products and the eigendecomposition of the tridiagonal matrix T k to compute f ( T k ) (which can be done in O ( k 2 ) time), giving a total running time of O (( n + t A ) k + k 2 ) .

Notes

The presentation in this section is a variation of the Lanczos method, a term often used speciﬁcally to denote the application of this meta- algorithm to computing eigenvalues and eigenvectors of matrices. It has been used to compute the matrix exponential, see [60, 67, 87]. The Lanczos method was combined with a semideﬁnite programming technique from [62], a rational approximation result from [70] and the Spielman–Teng Laplacian solver by Orecchia et al. [60] to obtain an ˜ O ( m ) time algorithm for the Balanced Edge-Separator problem from Section 7. Speciﬁcally, since Theorem 19.1 does not require the explicit knowledge the best polynomial, it was used in [60] to com- pute an approximation to exp( − L ) v and, hence, give an alternative proof of Theorem 9.1 from Section 9. The eigendecomposition result

for tridiagonal matrices, referred to in the proof of Theorem 19.1, is from [63]. The reader is encouraged to compare the Lanczos method with the conjugate gradient method presented in Section 16. Con- cretely, it is a fruitful exercise to try to explore if the conjugate gradient method can be derived from the Lanczos method by plugging in f ( x ) = x − 1 .

[

] I. Abraham, Y. Bartal, and O. Neiman, “Nearly tight low stretch spanning trees,” in Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS) , pp.

–

,

[

] I. Abraham and O. Neiman, “Using petal-decompositions to build a stretch spanning tree,” in ACM Symposium on Theory of Computing (STOC) pp.

–

,

[

] D. Achlioptas, “Database-friendly random projections: Johnson-Lindenstrauss with binary coins,” Journal of Computer and Systems Sciences , vol.

, no. pp.

–

,

[

] R. Ahlswede and A. Winter, “Addendum to “Strong converse for identiﬁcation via quantum channels”,” IEEE Transactions on Information Theory , vol. no.

, p.

,

[

] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory, Algo- rithms, and Applications . Prentice Hall, February

[

] N. Alon, R. M. Karp, D. Peleg, and D. B. West, “A graph-theoretic game its application to the k -server problem,” SIAM Journal on Computing , vol. no.

, pp.

–

,

[

] N. Alon and V. D. Milman, “ λ

, isoperimetric inequalities for graphs, superconcentrators,” Journal of Combinational Theory, Series B , vol.

, no. pp.

–

,

[

] R. Andersen, F. R. K. Chung, and K. J. Lang, “Local graph partitioning using pagerank vectors,” in Proceedings of the IEEE Symposium on Foundations Computer Science (FOCS) , pp.

–

,

[

] R. Andersen and Y. Peres, “Finding sparse cuts locally using evolving sets,” ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] S. Arora, E. Hazan, and S. Kale, “ ( log ) approximation to sparsest cut in ˜ O ( n

) time,” in Proceedings of the IEEE Symposium on Foundations Computer Science (FOCS) , pp.

–

,

[

] S. Arora, E. Hazan, and S. Kale, “The multiplicative weights update method: A meta-algorithm and applications,” Theory of Computing , vol.

, no. pp.

–

,

[

] S. Arora and S. Kale, “A combinatorial, primal-dual approach to semideﬁnite programs,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] S. Arora, S. Rao, and U. V. Vazirani, “Expander ﬂows, geometric embeddings and graph partitioning,” Journal of the ACM , vol.

, no.

,

[

] J. D. Batson, D. A. Spielman, and N. Srivastava, “Twice-Ramanujan spar- siﬁers,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] M. Belkin, I. Matveeva, and P. Niyogi, “Regularization and semi-supervised learning on large graphs,” in Proceedings of the Workshop on Computational Learning Theory (COLT) , pp.

–

,

[

] A. A. Bencz´ur and D. R. Karger, “Approximating s–t minimum cuts in ˜ O ( n

time,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] R. Bhatia, Matrix Analysis (Graduate Texts in Mathematics) . Springer,

[

] E. G. Boman and B. Hendrickson, “Support theory for preconditioning,” SIAM Journal on Matrix Analysis Applications , vol.

, no.

, pp.

–

,

[

] S. Boyd and L. Vandenberghe, Convex Optimization . Cambridge University Press, March

[

] J. Cheeger, “A lower bound for the smallest eigenvalue of the Laplacian,” Prob- lems in Analysis , pp.

–

,

[

] E. W. Cheney, Introduction to Approximation Theory/E.W. Cheney . New York: M

raw-Hill,

[

] P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S. Teng, “Elec- trical ﬂows, Laplacian systems, and faster approximation of maximum ﬂow undirected graphs,” in ACM Symposium on Theory of Computing (STOC) pp.

–

,

[

] F. R. K. Chung, Spectral Graph Theory (CBMS Regional Conference Series Mathematics, No.

) . American Mathematical Society,

[

] S. I. Daitch and D. A. Spielman, “Faster approximate lossy generalized ﬂow via interior point algorithms,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] P. G. Doyle and J. L. Snell, Random Walks and Electric Networks . Washington, DC: Mathematical Association of America,

[

] P. Elias, A. Feinstein, and C. Shannon, “A note on the maximum ﬂow through network,” IEEE Transactions on Information Theory , vol.

, no.

, pp.

–

, December

[

] M. Elkin, Y. Emek, D. A. Spielman, and S.-H. Teng, “Lower-stretch spanning trees,” SIAM Journal on Computing , vol.

, no.

, pp.

–

,

[

] M. Fiedler, “Algebraic connectivity of graphs,” Czechoslovak Mathematical Journal , vol.

, pp.

–

,

References [

] L. R. Ford and D. R. Fulkerson, “Maximal ﬂow through a network,” Canadian Journal of Mathematics , vol.

, pp.

–

,

[

] A. Frangioni and C. Gentile, “Prim-based support-graph preconditioners for min-cost ﬂow problems,” Computational Optimization and Applications vol.

, no.

–

, pp.

–

, April

[

] W. S. Fung, R. Hariharan, N. J. A. Harvey, and D. Panigrahi, “A general frame- work for graph sparsiﬁcation,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness . W.H. Freeman,

[

] A. George, “Nested dissection of a regular ﬁnite element mesh,” SIAM Journal on Numerical Analysis , vol.

, no.

, pp.

–

,

[

] C. D. Godsil and G. Royle, Algebraic Graph Theory . Springer,

[

] G. H. Golub and C. F. Van Loan, Matrix Computations . Johns Hopkins Univ. Press,

[

] G. H. Golub and M. L. Overton, “The convergence of inexact Chebyshev and Richardson iterative methods for solving linear systems,” Technical Report, Stanford University, Stanford, CA, USA,

[

] G. H. Golub and R. S. Varga, “Chebyshev semi-iterative methods, successive overrelaxation iterative methods, and second order Richardson iterative meth- ods,” Numerische Mathematik , vol.

, pp.

–

,

[

] K. Gremban, “Combinatorial preconditioners for sparse, symmetric, diagonally dominant linear systems,” P

Thesis, Carnegie Mellon University, Pittsburgh, CMU CS Tech Report CMU-CS-

-

, October

[

] M. R. Hestenes and E. Stiefel, “Methods of conjugate gradients for solving linear systems,” Journal of Research of the National Bureau of Standards , vol.

, pp.

–

, December

[

] G. Iyengar, D. J. Phillips, and C. Stein, “Approximating semideﬁnite packing programs,” SIAM Journal on Optimization , vol.

, no.

, pp.

–

,

[

] G. Iyengar, D. J. Phillips, and C. Stein, “Approximation algorithms for semidef- inite packing problems with applications to maxcut and graph coloring,” in IPCO’

: Proceedings of Conference on Integer Programming and Combinato- rial Optimization , pp.

–

,

[

] A. Joshi, “Topics in optimization and sparse linear systems,” P

Thesis, Uni- versity of Illinois at Urbana-Champaign, Champaign, IL, USA, UMI Order No. GAX

-

,

[

] S. Kaczmarz, “ ¨ Angen¨aherte Auﬂ¨osung von Systemen linearer Gleichungen,” Bulletin International del’ Acad´emie Polonaise Sciences et des Lettres pp.

–

,

[

] S. Kale, “Eﬃcient algorithms using the multiplicative weights update method,” P

Thesis, Princeton University, Department of Computer Science,

[

] J. A. Kelner and A. Madry, “Faster generation of random spanning trees,” in Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS) , pp.

–

,

[

] J. A. Kelner, G. L. Miller, and R. Peng, “Faster approximate multicommod- ity ﬂow using quadratically coupled ﬂows,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] J. A. Kelner, L. Orecchia, A. Sidford, and Z. A. Zhu, “A simple, combinatorial algorithm for solving SDD systems in nearly-linear time,” in ACM Symposium on Theory of Computing (STOC) ,

[

] R. Khandekar, S. Rao, and U. V. Vazirani, “Graph partitioning using single commodity ﬂows,” Journal of the ACM , vol.

, no.

,

[

] I. Koutis, G. L. Miller, and R. Peng, “Approaching optimality for solving SDD linear systems,” in Proceedings of the IEEE Symposium on Foundations of Com- puter Science (FOCS) , pp.

–

,

[

] I. Koutis, G. L. Miller, and R. Peng, “A nearly- m log n time solver for SDD linear systems,” in Proceedings of the IEEE Symposium on Foundations Computer Science (FOCS) , pp.

–

,

[

] D. A. Levin, Y. Peres, and E. L. Wilmer, Markov Chains and Mixing Times American Mathematical Society,

[

] R. J. Lipton, D. J. Rose, and R. E. Tarjan, “Generalized nested dissection,” SIAM Journal on Numerical Analysis , vol.

, no.

, pp.

–

,

[

] L. Lov´asz and M. Simonovits, “Random walks in a convex body and improved volume algorithm,” Random Structures & Algorithms , vol.

, no. pp.

–

,

[

] R. Lyons and Y. Peres, Probability on Trees and Networks . To Appear Cambridge University Press,

[

] A. Madry, “Fast approximation algorithms for cut-based problems in undi- rected graphs,” in Proceedings of the IEEE Symposium on Foundations of Com- puter Science (FOCS) , pp.

–

,

[

] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A spectral algorithm for improving graph partitions,” Journal of Machine Learning Research , vol.

, pp.

–

,

[

] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp.

–

,

[

] M. Mihail, “Conductance and convergence of markov chains — a combinatorial treatment of expanders,” in Proceedings of the IEEE Symposium on Founda- tions of Computer Science (FOCS) , pp.

–

,

[

] L. Orecchia, “Fast approximation algorithms for graph partitioning using spec- tral and semideﬁnite-programming techniques,” P

Thesis, EECS Depart- ment, University of California, Berkeley, May

[

] L. Orecchia, S. Sachdeva, and N. K. Vishnoi, “Approximating the exponential, the Lanczos method and an ˜ O ( m )-time spectral algorithm for balanced sepa- rator,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] L. Orecchia, L. J. Schulman, U. V. Vazirani, and N. K. Vishnoi, “On parti- tioning graphs via single commodity ﬂows,” in ACM Symposium on Theory Computing (STOC) , pp.

–

,

[

] L. Orecchia and N. K. Vishnoi, “Towards an SDP-based approach to spectral methods: A nearly-linear-time algorithm for graph partitioning and decom- position,” in Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) , pp.

–

,

[47] J. A. Kelner, L. Orecchia, A. Sidford, and Z. A. Zhu, “A simple, combinatorial algorithm for solving SDD systems in nearly-linear time,” in ACM Symposium

[

] V. Y. Pan and Z. Q. Chen, “The complexity of the matrix eigenproblem,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

,

[

] L. F. Richardson, “The approximate arithmetical solution by ﬁnite diﬀerences of physical problems involving diﬀerential equations with an application to the stresses in a masonry dam,” Transactions of the Royal Society of London vol. Series A, no.

, pp.

–

,

[

] T. J. Rivlin, An Introduction to the Approximation of Functions . Blaisdell book in numerical analysis and computer science. Blaisdell Pub. Co.,

[

] M. Rudelson, “Random vectors in the isotropic position,” Journal of Functional Analysis , vol.

, no.

, pp.

–

,

[

] Y. Saad, “Analysis of some Krylov subspace approximations to the matrix expo- nential operator,” SIAM Journal on Numerical Analysis , vol.

, pp.

–

, February

[

] Y. Saad, Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied Mathematics. Philadelphia, PA, USA,

nd Edition,

[

] S. Sachdeva and N. K. Vishnoi, “Inversion is as easy as exponentiation,” manuscript ,

[

] E. B. Saﬀ, A. Schonhage, and R. S. Varga, “Geometric convergence to e − z by rational functions with real poles,” Numerische Mathematik , vol.

, pp.

–

,

[

] J. Sherman, “Breaking the multicommodity ﬂow barrier for O ( √ log n )- approximations to sparsest cut,” in Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS) ,

[

] J. R. Shewchuk, “An introduction to the conjugate gradient method without the agonizing pain,” Technical Report, Carnegie Mellon University, Pittsburgh, PA, USA,

[

] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Trans- actions on Pattern Analysis and Machine Intelligence , vol.

, pp.

–

,

[

] D. B. Shmoys, Cut Problems and Their Application to Divide-and-Conquer pp.

–

,

[

] D. A. Spielman, “Algorithms, graph theory, and linear equations in Lapla- cian matrices,” in Proceedings of International Congress of Mathematicians (ICM’

) ,

[

] D. A. Spielman and N. Srivastava, “Graph sparsiﬁcation by eﬀective resis- tances,” SIAM Journal on Computing , vol.

, no.

, pp.

–

,

[

] D. A. Spielman and S.-H. Teng, “Nearly-linear time algorithms for graph parti- tioning, graph sparsiﬁcation, and solving linear systems,” in ACM Symposium on Theory of Computing (STOC) , pp.

–

, New York, NY, USA,

[

] D. A. Spielman and S.-H. Teng, “Nearly-linear time algorithms for precondi- tioning and solving symmetric, diagonally dominant linear systems,” C

R, abs/cs/

,

[

] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for massive graphs and its application to nearly-linear time graph partitioning,” C

R, abs/

,

[

] D. A. Spielman and S.-H. Teng, “Spectral sparsiﬁcation of graphs,” SIAM Journal on Computing , vol.

, no.

, pp.

–

,

[

] D. A. Spielman and J. Woo, “A note on preconditioning by low-stretch spanning trees,” C

R, abs/

,

[

] G. Strang, Linear Algebra and Its Applications. Harcourt Brace Jonanovich. San Diego,

rd Edition,

[

] T. Strohmer and R. Vershynin, “A randomized Kaczmarz algorithm with expo- nential convergence,” Journal of Fourier Analysis and Applications , vol.

, pp.

–

,

[

] S. Teng, “The Laplacian paradigm: Emerging algorithms for massive graphs,” in Proceedings of the Annual Conference on Theory and Applications of Models of Computation (TAMC) , pp.

–

,

[

] L. N. Trefethen and D. Bau, Numerical Linear Algebra . SIAM,

[

] P. M. Vaidya, “Solving linear equations with symmetric diagonally dominant matrices by constructing good preconditioners,” Technical Report, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL,

[

] J. vanden Eshof and M. Hochbruck, “Preconditioning Lanczos approximations to the matrix exponential,” SIAM Journal on Scientiﬁc Computing , vol.

, pp.

–

, November

[

] J. H. Wilkinson, The Algebraic Eigenvalue Problem (Monographs on Numerical Analysis) . USA: Oxford University Press,

st Edition, April

[

] X. Zhu, Z. Ghahramani, and J. D. Laﬀerty, “Semi-supervised learning using Gaussian ﬁelds and harmonic functions,” in Proceedings of the International Conference on Machine Learning (ICML) , pp.

–

,